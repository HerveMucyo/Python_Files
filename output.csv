question,answer
Create a set from a series in pandas,"If you only need to get list of unique values, you can just use unique method.
If you want to have Python's set, then do set(some_series)
In [1]: s = pd.Series([1, 2, 3, 1, 1, 4])

In [2]: s.unique()
Out[2]: array([1, 2, 3, 4])

In [3]: set(s)
Out[3]: {1, 2, 3, 4}

However, if you have DataFrame, just select series out of it ( some_data_frame['<col_name>'] ).
"
Using Kaggle Datasets in Google Colab,"Step-by-step --

Create an API key in Kaggle.
To do this, go to kaggle.com/ and open your user settings page.

Next, scroll down to the API access section and click generate
to download an API key.

This will download a file called kaggle.json to your computer.
You'll use this file in Colab to access Kaggle datasets and 
competitions.
Navigate to https://colab.research.google.com/.
Upload your kaggle.json file using the following snippet in
a code cell:
from google.colab import files
files.upload()

Install the kaggle API using !pip install -q kaggle
Move the kaggle.json file into ~/.kaggle, which is where the
API client expects your token to be located:
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
Now you can access datasets using the client, e.g., !kaggle datasets list.

Here's a complete example notebook of the Colab portion of this process:
https://colab.research.google.com/drive/1DofKEdQYaXmDWBzuResXWWvxhLgDeVyl
This example shows uploading the kaggle.json file, the Kaggle API client, and using the Kaggle client to download a dataset.
"
Setting environment variables in Google Colab,"If you like %magic, you can also use %env to make it a bit shorter.
%env KAGGLE_USERNAME=abcdefgh

If the value is in a variable you can also use 
%env KAGGLE_USERNAME=$username

"
Can&#39;t find kaggle.json file in google colab,"According to kaggle api documentation the location where credentials json is looking for is ~/.kaggle/kaggle.json as google colab environment is Linux based.
In your snippet you try to config path parameter, but it is not used to looking for credential json:

- path: Folder where file(s) will be downloaded, defaults to current working directory

So the full working snippet for google colab environment would be:
!mkdir ~/.kaggle
!touch ~/.kaggle/kaggle.json

api_token = {""username"":""username"",""key"":""api-key""}

import json

with open('/root/.kaggle/kaggle.json', 'w') as file:
    json.dump(api_token, file)

!chmod 600 ~/.kaggle/kaggle.json

And then some api call like
!kaggle datasets download -d datamunge/sign-language-mnist

"
"UnicodeDecodeError: &#39;utf-8&#39; codec can&#39;t decode byte 0x8b in position 1: invalid start byte, while reading csv file in pandas","It's still most likely gzipped data. gzip's magic number is 0x1f 0x8b, which is consistent with the UnicodeDecodeError you get.
You could try decompressing the data on the fly:
with open('destinations.csv', 'rb') as fd:
    gzip_fd = gzip.GzipFile(fileobj=fd)
    destinations = pd.read_csv(gzip_fd)

Or use pandas' built-in gzip support:
destinations = pd.read_csv('destinations.csv', compression='gzip')

"
What is OOF approach in machine learning?,"OOF simply stands for ""Out-of-fold"" and refers to a step in the learning process when using k-fold validation in which the predictions from each set of folds are grouped together into one group of 1000 predictions. These predictions are now ""out-of-the-folds"" and thus error can be calculated on these to get a good measure of how good your model is. 
In terms of learning more about it, there's really not a ton more to it than that, and it certainly isn't its own technique to learning or anything. If you have a follow up question that is small, please leave a comment and I will try and update my answer to include this.
EDIT: While ambling around the inter-webs I stumbled upon this relatively similar question from Cross-Validated (with a slightly more detailed answer), perhaps it will add some intuition if you are still confused.
"
Working with neuralnet in R for the first time: get &quot;requires numeric/complex matrix/vector arguments&quot;,"Before blindly giving the data to the computer,
it is a good idea to look at it:
d <- read.csv(""train.csv"")
str(d)
# 'data.frame': 891 obs. of  12 variables:
#  $ PassengerId: int  1 2 3 4 5 6 7 8 9 10 ...
#  $ Survived   : int  0 1 1 1 0 0 0 0 1 1 ...
#  $ Pclass     : int  3 1 3 1 3 3 1 3 3 2 ...
#  $ Name       : Factor w/ 891 levels ""Abbing, Mr. Anthony"",..: 109 191 358 277 16 559 520 629 417 581 ...
#  $ Sex        : Factor w/ 2 levels ""female"",""male"": 2 1 1 1 2 2 2 2 1 1 ...
#  $ Age        : num  22 38 26 35 35 NA 54 2 27 14 ...
#  $ SibSp      : int  1 1 0 1 0 0 0 3 0 1 ...
#  $ Parch      : int  0 0 0 0 0 0 0 1 2 0 ...
#  $ Ticket     : Factor w/ 681 levels ""110152"",""110413"",..: 524 597 670 50 473 276 86 396 345 133 ...
#  $ Fare       : num  7.25 71.28 7.92 53.1 8.05 ...
#  $ Cabin      : Factor w/ 148 levels """",""A10"",""A14"",..: 1 83 1 57 1 1 131 1 1 1 ...
#  $ Embarked   : Factor w/ 4 levels """",""C"",""Q"",""S"": 4 2 4 4 4 3 4 4 4 2 ...
summary(d)

Some of the variables have too many values to be useful
(at least in your first model): 
you can remove the name, ticket, cabin and passengerId.
You may also want to transform some of the numeric variables (say, class), to factors,
if it is more meaningful.
Since neuralnet only deals with quantitative variables, 
you can convert all the qualitative variables (factors)
to binary (""dummy"") variables, with the model.matrix function -- 
it is one of the very rare situations
in which R does not perform the transformation for you.
m <- model.matrix( 
  ~ Survived + Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, 
  data = d 
)
head(m)
library(neuralnet)
r <- neuralnet( 
  Survived ~ Pclass + Sexmale + Age + SibSp + Parch + Fare + EmbarkedC + EmbarkedQ + EmbarkedS, 
  data=m, hidden=10, threshold=0.01
)

"
Linear model function lm() error: NA/NaN/Inf in foreign function call (arg 1),"I know this thread is really old, but the answers don't seem complete, and I just ran into the same problem.
The problem I was having was because the NA columns also had NaN and Inf.  Remove those and try it again.  Specifically:
col2[which(is.nan(col2))] = NA
col2[which(col2==Inf)] = NA

Hope that helps your 18 month old question!
"
documentation for Kaggle API *within* python?,"I published a blog post that explains most of the common use cases of competition, datasets and kernel interactions.
Here are the steps involved in using the Kaggle API from within Python.
Setting up API Key
Go to your Kaggle account Tab at https://www.kaggle.com/<username>/account
and click ‘Create API Token’.  A file named kaggle.json will be downloaded. Move this file in to ~/.kaggle/ folder in Mac and Linux or to C:\Users<username>.kaggle\  on windows.
Alternatively, you can populate KAGGLE_USERNAME and KAGGLE_KEY environment variables with values from kaggle.json to get the api to authenticate.
Authenticating With API Server
from kaggle.api.kaggle_api_extended import KaggleApi
api = KaggleApi()
api.authenticate()

Downloading Datasets
# Download all files of a dataset
# Signature: dataset_download_files(dataset, path=None, force=False, quiet=True, unzip=False)
api.dataset_download_files('avenn98/world-of-warcraft-demographics')

# download single file
#Signature: dataset_download_file(dataset, file_name, path=None, force=False, quiet=True)
api.dataset_download_file('avenn98/world-of-warcraft-demographics','WoW Demographics.csv')

Downloading Competition Files
# Download all files for a competition
# Signature: competition_download_files(competition, path=None, force=False, quiet=True)
api.competition_download_files('titanic')

# Download single file for a competition
# Signature: competition_download_file(competition, file_name, path=None, force=False, quiet=False)
api.competition_download_file('titanic','gender_submission.csv')

Submitting to competitions
# Signature: competition_submit(file_name, message, competition,quiet=False)
api.competition_submit('gender_submission.csv','API Submission','titanic')

Retrieving Leader Board
# Signature: competition_view_leaderboard(id, **kwargs)
leaderboard = api.competition_view_leaderboard('titanic')

"
Pandas error - invalid value encountered,"I have the same error and have decided that it is a bug. It seems to be caused by the presence of NaN values in a DataFrame in Spyder. I have uninstalled and reinstalled all packages and nothing has effected it. NaN  values are supported and are completely valid in DataFrames especially if they have a DateTime index. 
In the end I have settled for suppressing this warnings as follows. 
import warnings
warnings.simplefilter(action = ""ignore"", category = RuntimeWarning)

"
What does KFold in python exactly do?,"KFold will provide train/test indices to split data in train and test sets. It will split dataset into k consecutive folds (without shuffling by default).Each fold is then used a validation set once while the k - 1 remaining folds form the training set (source).
Let's say, you have some data indices from 1 to 10. If you use n_fold=k, in first iteration you will get i'th (i<=k) fold as test indices and remaining (k-1) folds (without that i'th fold) together as train indices.
An example
import numpy as np
from sklearn.cross_validation import KFold

x = [1,2,3,4,5,6,7,8,9,10,11,12]
kf = KFold(12, n_folds=3)

for train_index, test_index in kf:
    print (train_index, test_index)

Output

Fold 1: [ 4  5  6  7  8  9 10 11] [0 1 2 3]
Fold 2: [ 0  1  2  3  8  9 10 11] [4 5 6 7]
Fold 3: [0 1 2 3 4 5 6 7] [ 8  9 10 11]

Import Update for sklearn 0.20:
KFold object was moved to the sklearn.model_selection module in version 0.20. To import KFold in sklearn 0.20+ use from sklearn.model_selection import KFold. KFold current documentation source
"
Download Kaggle Dataset by using Python,"Basically, if you want to use the Kaggle python API (the solution provided by @minh-triet is for the command line not for python) you have to do the following:
import kaggle

kaggle.api.authenticate()

kaggle.api.dataset_download_files('The_name_of_the_dataset', path='the_path_you_want_to_download_the_files_to', unzip=True)

I hope this helps.
"
How to view the nearest neighbors in R?,"1) You can get the nearest neighbors of a given row like so:
k <- knn(train, test, labels, k = 10, algorithm=""cover_tree"")
indices <- attr(k, ""nn.index"")

Then if you want the indices of the 10 nearest neighbors to row 20 in the training set:
print(indices[20, ])

(You'll get the 10 nearest neighbors because you selected k=10). For example, if you run with only the first 1000 rows of the training and testing set (to make it computationally easier):
train <- read.csv(""train.csv"", header=TRUE)[1:1000, ]
test <- read.csv(""test.csv"", header=TRUE)[1:1000, ]

labels <- train[,1]
train <- train[,-1]

k <- knn(train, test, labels, k = 10, algorithm=""cover_tree"")
indices = attr(k, ""nn.index"")

print(indices[20, ])
# output:
#  [1] 829 539 784 487 293 882 367 268 201 277

Those are the indices within the training set of 1000 that are closest to the 20th row of the test set.
2) It depends what you mean by ""modify"". For starters, you can get the indices of each of the 10 closest labels to each row like this:
closest.labels = apply(indices, 2, function(col) labels[col])

You can then see the labels of the 10 closest points to the 20th training point like this:
closest.labels[20, ]
# [1] 0 0 0 0 0 0 0 0 0 0

This indicates that all 10 of the closest points to row 20 are all in the group labeled 0. knn simply chooses the label by majority vote (with ties broken at random), but you could choose some kind of weighting scheme if you prefer.
ETA: If you're interested in weighting the closer elements more heavily in your voting scheme, note that you can also get the distances to each of the k neighbors like this:
dists = attr(k, ""nn.dist"")
dists[20, ]
# output:
# [1] 1238.777 1243.581 1323.538 1398.060 1503.371 1529.660 1538.128 1609.730
# [9] 1630.910 1667.014

"
kaggle kernel: Your kernel cannot use internet access for this competition,"To turn off internet access in a kaggle notebook or script.

Click settings in the right sidebar.
Next to ""Internet"" click 'on'
Commit

"
Error: &#39;utf8&#39; codec can&#39;t decode byte 0x80 in position 0: invalid start byte,"You are not loading the file correctly. You should use load() instead of load_word2vec_format(). 
The latter is used when you train a model using the C code, and save the model in a binary format. However you are not saving the model in a binary format, and are training it using python. So you can simply use the following code and it should work: 
models = gensim.models.Word2Vec.load('300features_40minwords_10context.txt')

"
General techniques to work with huge amounts of data on a non-super computer,"Prototype--that's the most important thing when working with big data. Sensibly carve it up so that you can load it in memory to access it with an interpreter--e.g., python, R. That's the best way to create and refine your analytics process flow at scale.
In other words, trim your multi-GB-sized data files so that they are small enough to perform command-line analytics.
Here's the workflow i use to do that--surely not the best way to do it, but it is one way, and it works:
I. Use lazy loading methods (hopefully) available in your language of
choice to read in large data files, particularly those exceeding about 1 GB. I
would then recommend processing this data stream according to the
techniques i discuss below, then finally storing this fully
pre-processed data in a Data Mart, or intermediate staging container.
One example using Python to lazy load a large data file:
# 'filename' is the full path name for a data file whose size 
# exceeds the memory on the box it resides. #

import tokenize

data_reader = open(some_filename, 'r')
tokens = tokenize.generate_tokens(reader)
tokens.next()           # returns a single line from the large data file.


II. Whiten and Recast: 

Recast your columns storing categorical
   variables (e.g., Male/Female) as integers (e.g., -1, 1). Maintain
a
   look-up table (the same hash as you used for this conversion
except
   the keys and values are swapped out) to convert these integers
back
   to human-readable string labels as the last step in your analytic
   workflow;
whiten your data--i.e., ""normalize"" the columns that
   hold continuous data. Both of these steps will substantially
reduce
   the size of your data set--without introducing any noise. A
   concomitant benefit from whitening is prevention of analytics
error
   caused by over-weighting.


III. Sampling: Trim your data length-wise.

IV. Dimension Reduction: the orthogonal analogue to sampling. Identify the variables (columns/fields/features) that have no influence or de minimis influence on the dependent variable (a.k.a., the 'outcomes' or response variable) and eliminate them from your working data cube.  
Principal Component Analysis (PCA) is a simple and reliable technique to do this:
import numpy as NP
from scipy import linalg as LA

D = NP.random.randn(8, 5)       # a simulated data set
# calculate the covariance matrix: #
R = NP.corrcoef(D, rowvar=1)
# calculate the eigenvalues of the covariance matrix: #
eigval, eigvec = NP.eig(R)
# sort them in descending order: #
egval = NP.sort(egval)[::-1]
# make a value-proportion table #
cs = NP.cumsum(egval)/NP.sum(egval)
print(""{0}\t{1}"".format('eigenvalue', 'var proportion'))
for i in range(len(egval)) :
    print(""{0:.2f}\t\t{1:.2f}"".format(egval[i], cs[i]))

  eigenvalue    var proportion
    2.22        0.44
    1.81        0.81
    0.67        0.94
    0.23        0.99
    0.06        1.00

So as you can see, the first three eigenvalues account for 94% of the variance observed in original data. Depending on your purpose, you can often trim the original data matrix, D, by removing the last two columns:
D = D[:,:-2]


V. Data Mart Storage: insert a layer between your permanent storage (Data Warehouse) and your analytics process flow. In other words, rely heavily on data marts/data cubes--a 'staging area' that sits between your Data Warehouse and your analytics app layer. This data mart is a much better IO layer for your analytics apps. R's 'data frame' or 'data table' (from the CRAN Package of the same name) are good candidates. I also strongly recommend redis--blazing fast reads, terse semantics, and zero configuration, make it an excellent choice for this use case. redis will easily handle datasets of the size you mentioned in your Question. Using the hash data structure in redis, for instance, you can have the same structure and the same relational flexibility as MySQL or SQLite without the tedious configuration. Another advantage: unlike SQLite, redis is in fact a database server. I am actually a big fan of SQLite, but i believe redis just works better here for the reasons i just gave.
from redis import Redis
r0 = Redis(db=0)
r0.hmset(user_id : ""100143321, {sex : 'M', status : 'registered_user', 
       traffic_source : 'affiliate', page_views_per_session : 17, 
       total_purchases : 28.15})

"
What is difference between eval_metric and feval in xgboost?,"They both do roughly the same thing. 
Eval_metric can take a string (uses their internal functions)  or user defined function
feval only takes a function 
Both are, as you noted, for evaluation purposes.
In the below examples you can see they are used very similarly.
## A simple xgb.train example:
param <- list(max_depth = 2, eta = 1, silent = 1, nthread = 2, 
              objective = ""binary:logistic"", eval_metric = ""auc"")
bst <- xgb.train(param, dtrain, nrounds = 2, watchlist)


## An xgb.train example where custom objective and evaluation metric are used:
logregobj <- function(preds, dtrain) {
   labels <- getinfo(dtrain, ""label"")
   preds <- 1/(1 + exp(-preds))
   grad <- preds - labels
   hess <- preds * (1 - preds)
   return(list(grad = grad, hess = hess))
}
evalerror <- function(preds, dtrain) {
  labels <- getinfo(dtrain, ""label"")
  err <- as.numeric(sum(labels != (preds > 0)))/length(labels)
  return(list(metric = ""error"", value = err))
}

# These functions could be used by passing them either:
#  as 'objective' and 'eval_metric' parameters in the params list:
param <- list(max_depth = 2, eta = 1, silent = 1, nthread = 2, 
              objective = logregobj, eval_metric = evalerror)
bst <- xgb.train(param, dtrain, nrounds = 2, watchlist)

#  or through the ... arguments:
param <- list(max_depth = 2, eta = 1, silent = 1, nthread = 2)
bst <- xgb.train(param, dtrain, nrounds = 2, watchlist,
                 objective = logregobj, eval_metric = evalerror)

#  or as dedicated 'obj' and 'feval' parameters of xgb.train:
bst <- xgb.train(param, dtrain, nrounds = 2, watchlist,
                 obj = logregobj, feval = evalerror)

https://github.com/dmlc/xgboost/blob/72451457120ac9d59573cf7580ccd2ad178ef908/R-package/R/xgb.train.R#L176
"
mount google drive in kaggle notebook,"In fact, the google-colab library does not exist in the Kaggle Kernel. In this way, I use the following procedure to deal with this problem in Kaggle Kernel:

First, extract the ID of your desire file from google drive:

In your browser, navigate to drive.google.com.

Right-click on the file, and click ""Get a shareable link""


Then extract the ID of file from URL:




Next, install gdown PyPI module using conda:
! conda install -y gdown

Finally, download the file using gdown and the intended ID:
!gdown --id <put-the-ID>


For example:
!gdown --id 1-1wAx7b-USG0eQwIBVwVDUl3K1_1ReCt
"
Validation and Testing accuracy widely different,"Reasons for a high generalization gap:

Different distributions: The validation and test set might come from different distributions. Try to verify that they are indeed sampled from the same process in your code.
Number of samples: The size of the validation and / or the test set is too low. This means that the empirical data distributions differ too much, explaining the different reported accuracies. One example would be a dataset consisting of thousands of images, but also thousands of classes. Then, the test set might contain some classes that are not in the validation set (and vice versa). Use cross-validation to check, if the test accuracy is always lower than the validation accuracy, or if they just generally differ a lot in each fold.
Hyperparameter Overfitting: This is also related to the size of the two sets. Did you do hyperparameter tuning? If so, you can check if the accuracy gap existed before you tuned the hyperparameters, as you might have ""overfitted"" the hyperparameters on the validation set.
Loss function vs. accuracy: you reported different accuracies. Did you also check the train, validation and test losses? You train your model on the loss function, so this is the most direct performance measure. If the accuracy is only loosely coupled to your loss function and the test loss is approximately as low as the validation loss, it might explain the accuracy gap.
Bug in the code: if the test and validation set are sampled from the same process and are sufficiently large, they are interchangeable. This means that the test and validation losses must be approximately equal. So, if you checked the four points above, my next best guess would be a bug in the code. For example, you accidentally trained your model on the validation set as well. You might want to train your model on a larger dataset and then check, if the accuracies still diverge.

"
xgboost: AttributeError: &#39;DMatrix&#39; object has no attribute &#39;handle&#39;,"The problem here is related to the initial data: some of values are float or integer and some object. This is why we need to cast them:
from sklearn import preprocessing 
for f in train.columns: 
    if train[f].dtype=='object': 
        lbl = preprocessing.LabelEncoder() 
        lbl.fit(list(train[f].values)) 
        train[f] = lbl.transform(list(train[f].values))

for f in test.columns: 
    if test[f].dtype=='object': 
        lbl = preprocessing.LabelEncoder() 
        lbl.fit(list(test[f].values)) 
        test[f] = lbl.transform(list(test[f].values))

train.fillna((-999), inplace=True) 
test.fillna((-999), inplace=True)

train=np.array(train) 
test=np.array(test) 
train = train.astype(float) 
test = test.astype(float)

"
ValueError: unknown is not supported in sklearn.RFECV,"RFECV checks target/train data to be of one of types  binary, multiclass, multilabel-indicator or multilabel-sequences:

'binary': y contains <= 2 discrete values and is 1d or a column
vector.
'multiclass': y contains more than two discrete values, is not a
sequence of sequences, and is 1d or a column vector.
'mutliclass-multioutput': y is a 2d array that contains more
than two discrete values, is not a sequence of sequences, and both
dimensions are of size > 1.
'multilabel-indicator': y is a label indicator matrix, an array
of two dimensions with at least two columns, and at most 2 unique
values.

while your Y is unknown, that is 

'unknown': y is array-like but none of the above, such as a 3d array, or an array of non-sequence objects.

The reason for that is your target data is string (of form ""0"" and ""1"") and is loaded with read_table as object:
>>> training_data[:, -1].dtype
dtype('O')
>>> type_of_target(training_data[:, -1])
'unknown'

To solve the issue, you can convert to int:
>>> Y = training_data[:, -1].astype(int)
>>> type_of_target(Y)
'binary'

"
subprocess python filenotfounderror: [winerror 2],"This code runs the ls command, which is available on all POSIX-conforming systems.
You are using Microsoft Windows. Microsoft Windows does not conform to POSIX by default. For instance, there is no ls binary. Therefore, subprocess cannot find the file ls, and thus emits a FileNotFoundError.
You can install Microsoft's Bash on Windows, which will give you ls.
However, the pythonic and more portable way to list a directory is not to use subprocess in the first place, but the built-in os.listdir:
import os
print(os.listdir('../input'))

"
&#39;pip install kaggle&#39; works fine - but &#39;kg command not found&#39;,"Shortly after you asked this question, Kaggle released an official API which I recommend you use in place of kaggle-cli. pip install kaggle will install the official API which supports these commands:

kaggle competitions {list, files, download, submit, submissions}
kaggle datasets {list, files, download, create, version, init}
kaggle config {view, set, unset}

Check out the documentation here. After installing, you will also need to generate an API token from your Kaggle user profile ""Account"" tab.
"
C5.0 decision tree - c50 code called exit with value 1,"For anyone interested, the data can be found here: http://www.kaggle.com/c/titanic-gettingStarted/data. I think you need to be registered in order to download it.
Regarding your problem, first of I think you meant to write
new_model <- C5.0(train[,-2],train$Survived)

Next, notice the structure of the Cabin and Embarked Columns. These two factors have an empty character as a level name (check with levels(train$Embarked)). This is the point where C50 falls over. If you modify your data such that
levels(train$Cabin)[1] = ""missing""
levels(train$Embarked)[1] = ""missing""

your algorithm will now run without an error.
"
How to get rid of warning &quot;DeprecationWarning generator &#39;ngrams&#39; raised StopIteration&quot;,"To anyone else who doesn't want or can't suppress the warning.
This is happening because ngrams is raising StopIteration exception to end a generator, and this is deprecated from Python 3.5.
You could get rid of the warning by changing the code where the generator stops, so instead of raising StopIteration you just use Python's keyword return.
More on: PEP 479
"
Choosing the learning_rate using fastai&#39;s learn.lr_find(),"The idea for a learning rate range test as done in lr_find comes from this paper by Leslie Smith: https://arxiv.org/abs/1803.09820  That has a lot of other useful tuning tips; it's worth studying closely.
In lr_find, the learning rate is slowly ramped up (in a log-linear way).  You don't want to pick the point at which loss is lowest; you want to pick the point at which it is dropping fastest per step (=net is learning as fast as possible).  That does happen somewhere around the middle of the downward slope or 1e-2, so the guy who wrote the notebook has it about right.  Anything between 0.5e-2 and 3e-2 has roughly the same slope and would be a reasonable choice; the smaller values would correspond to a bit slower learning (=more epochs needed, also less regularization) but with a bit less risk of reaching a plateau too early.
I'll try to add a bit of intuition about what is happening when loss is the lowest in this test, say learning rate=1e-1.  At this point, the gradient descent algorithm is taking large steps in the direction of the gradient, but loss is not decreasing.  How can this happen?  Well, it would happen if the steps are consistently too large.  Think of trying to get into a well (or canyon) in the loss landscape.  If your step size is larger than the size of the well, you can consistently step over it every time and end up on the other side.
This picture from a nice blog post by Jeremy Jordan shows it visually:

In the picture, it shows the gradient descent climbing out of a well by taking too large steps (maybe lr=1+0 in your test).  I think this rarely happens exactly like that unless lr is truly excessive; more likely, the well is in a relatively flat landscape, and the gradient descent can step over it, not being able to get into the well in the first place.  High-dimensional loss landscapes are hard to visualize, and may be very irregular, but in a sense the lr_find test is looking for the scale of the typical features in the landscape and then picking a learning rate that gives you a step which is similar sized but a bit smaller.
"
Inserting comments into jupyter notebook,"Yep - highlight a cell and click on the ""Cell"" dropdown menu and go to ""Cell Type"" and choose ""Markdown"". Then you can type any markdown text you'd like and it will render as such.  
^^ Also there are shortcuts for changing cell types as well. Highlight a cell and press the esc key to change into ""command"" mode and press m. This changes it to a markdown cell. Press y to change it back to a code cell.
You can also do latex equations using dollar signs ($). There is documentation for this here
"
Object is enumerable but not indexable?,"Python only allows these things if the class has methods for them:

__getitem__ is required for the [] syntax.
__iter__ and __next__1 are required to iterate.

Any class can define one without defining the other. __getattr__ is usually not defined if it would be inefficient.

1 __next__ is required on the class returned by __iter__.
"
Multi label regression in Caffe,"i found it :)
I replaced the SOFTLAYER to EUCLIDEAN_LOSS function and changed the number of outputs. It worked.
layers {
  name: ""loss""
  type: EUCLIDEAN_LOSS
  bottom: ""ip1""
  bottom: ""label""
  top: ""loss""
}

HINGE_LOSS is also another option.
"
"Google colaboratory use kaggle, server version 1.5.6 , client version 1.5.4, failed to upgrade","Ah, this is indeed a strange state.
The fix: !pip install --upgrade --force-reinstall --no-deps kaggle
The underlying problem: we install both py2 and py3 packages, and (for historical reasons) the py2 packages are installed second. kaggle is a wrapper installed by the kaggle python package; since we do py2 second, the py2 wrapper is in /usr/local/bin, and happens to be an older version.
"
R - Image Plot MNIST dataset,"At the moment your im is a matrix of characters. You need to convert it to a matrix of numbers, e.g. by issuing im_numbers <- apply(im, 2, as.numeric).
You can then issue image(1:28, 1:28, im_numbers, col=gray((0:255)/255)).
"
Error while importing Kaggle dataset on Colab,"It suddenly stopped working here as well. Apparently, the kaggle api was not searching the kaggle.json file in the correct place.
Since I was using the kaggle api inside a colab notebook, I was importing the kaggle.json like this:
from googleapiclient.discovery import build
import io, os
from googleapiclient.http import MediaIoBaseDownload
from google.colab import auth

auth.authenticate_user()

drive_service = build('drive', 'v3')
results = drive_service.files().list(
        q=""name = 'kaggle.json'"", fields=""files(id)"").execute()
kaggle_api_key = results.get('files', [])

filename = ""/content/.kaggle/kaggle.json""
os.makedirs(os.path.dirname(filename), exist_ok=True)

request = drive_service.files().get_media(fileId=kaggle_api_key[0]['id'])
fh = io.FileIO(filename, 'wb')
downloader = MediaIoBaseDownload(fh, request)
done = False
while done is False:
    status, done = downloader.next_chunk()
    print(""Download %d%%."" % int(status.progress() * 100))
os.chmod(filename, 600)

It worked just fine. But now, the kaggle api searches the kaggle.json in this location:
~/.kaggle/kaggle.json

So, I just had to move/copy the file I downloaded to the right place:
!mkdir ~/.kaggle
!cp /content/.kaggle/kaggle.json ~/.kaggle/kaggle.json

And it started working again.
"
Kaggle API issue &quot;Could not find kaggle.json. Make sure it&#39;s located in......&quot;,"I had the same problem.
Using Google's GCP notebook and after several attempts, I managed to resolve it as follows:
Generate the API file (JSON) through the Kaggle platform.
Execute:
https://adityashrm21.github.io/Setting-Up-Kaggle/
"
"Python3 CSV writerows, TypeError: &#39;str&#39; does not support the buffer interface","Creating a CSV file is different between Python 2 and Python 3 (as a look into the docs for the csv module would have shown):
Instead of 
predictions_file = open(""myfirstforest.csv"", ""wb"")

you need to use
predictions_file = open(""myfirstforest.csv"", ""w"", newline="""")

(And you should use a context manager to handle the closing of the file for you, in case an error does occur):
with open(""myfirstforest.csv"", ""w"", newline="""") as predictions_file:
    # do stuff
# No need to close the file

"
major memory problems reading in a csv file using numpy,"import pandas, re, numpy as np

def load_file(filename, num_cols, delimiter='\t'):
    data = None
    try:
        data = np.load(filename + '.npy')
    except:
        splitter = re.compile(delimiter)

        def items(infile):
            for line in infile:
                for item in splitter.split(line):
                    yield item

        with open(filename, 'r') as infile:
            data = np.fromiter(items(infile), float64, -1)
            data = data.reshape((-1, num_cols))
            np.save(filename, data)

    return pandas.DataFrame(data)

This reads in the 2.5GB file, and serializes the output matrix. The input file is read in ""lazily"", so no intermediate data-structures are built and minimal memory is used. The initial load takes a long time, but each subsequent load (of the serialized file) is fast. Please let me if you have tips!
"
Pytorch tensor.save() produces huge files for small tensors from MNIST,"As explained in this discussion, torch.save() saves the whole tensor, not just the slice. You need to explicitly copy the data using clone().
Don't worry, at runtime the data is only allocated once unless you explicitly create copies.
As a general advice: If the data easily fits into your memory, just load it at once. For MNIST with 130 MB that's certainly the case.
However, I would still batch the data because it converges faster. Look up the advantages of SGD for more details.
"
Unzip a 7z file in google colab?,"Try this
!7z e train-jpg.tar.7z

See if you get the tar file, then tar -xvf
Also check that train-jpg.tar.7z is in the current directory too.
"
Error in importing Cats-vs-Dogs dataset in Google Colab,"You can add this before loading to set the new URL :
setattr(tfds.image_classification.cats_vs_dogs, '_URL',""https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip"")

"
Pandas scatter_matrix - plot categorical variables,"You need to transform the categorical variables into numbers to plot them.
Example (assuming that the column 'Sex' is holding the gender data, with 'M' for males & 'F' for females)
df['Sex_int'] = np.nan
df.loc[df['Sex'] == 'M', 'Sex_int'] = 0
df.loc[df['Sex'] == 'F', 'Sex_int'] = 1

Now all females are represented by 0 & males by 1. Unknown genders (if there are any) will be ignored.
The rest of your code should process the updated dataframe nicely.
"
convert jpg to greyscale csv using R,"There are some formulas for how to do this at this link. The raster package is one approach. THis basically converts the RGB bands to one black and white band (it makes it smaller in size, which I am guessing what you want.)
library(raster)
color.image <- brick(""yourjpg.jpg"")

# Luminosity method for converting to greyscale
# Find more here http://www.johndcook.com/blog/2009/08/24/algorithms-convert-color-grayscale/
color.values <- getValues(color.image)
bw.values <- color.values[,1]*0.21 + color.values[,1]*0.72 + color.values[,1]*0.07

I think the EBImage package can also help for this problem (not on CRAN, install it through source:
source(""http://bioconductor.org/biocLite.R"")
biocLite(""EBImage"")
library(EBImage)

color.image <- readImage(""yourjpg.jpg"")
bw.image <- channel(color.image,""gray"")
writeImage(bw.image,file=""bw.png"")

"
"What is the difference between xgboost, extratreeclassifier, and randomforrestclasiffier?","Extra-trees(ET) aka. extremely randomized trees is quite similar to random forest (RF). Both methods are bagging methods aggregating some fully grow decision trees. RF will only try to split by e.g. a third of features, but evaluate any possible break point within these features and pick the best. However, ET will only evaluate a random few break points and pick the best of these. ET can bootstrap samples to each tree or use all samples. RF must use bootstrap to work well.
xgboost is an implementation of gradient boosting and can work with decision trees, typical smaller trees. Each tree is trained to correct the residuals of previous trained trees. Gradient boosting can be more difficult to train, but can achieve a lower model bias than RF. For noisy data bagging is likely to be most promising. For low noise and complex data structures boosting is likely to be most promising.
"
Dataset lost in google colab?,"When you upload files in Colab through downloading them using terminal commands or by uploading them in the runtime, the following time you will login they are lost for good. This happens because Colab doesn't (and can't) store all the files that the users upload. You can bypass this problem mounting google drive. In the ""files"" tab you will find a button (next to upload and refresh buttons) ""mount drive"". By clicking this button, a new cell will be written (you can just copy-paste the following code), containing:
from google.colab import drive
drive.mount('/content/gdrive')

This is by far the fastest method you can use to manage datasets in Colab. Just upload everything you need to your google drive storage, and use them as you prefer, like they are manually uploaded to the runtime.
"
Pandas Value Counts With Constraint For More Than One Occurance,"@wen ansered this in the comments. 
df['variety'].value_counts().loc[lambda x : x>1] 

"
Unable to install Python library in Kaggle notebook,"Open the tab bar on the right -> In the Settings drop-down, enable 'Internet'. Now you can install any python library.
"
How to convert the Ipython kernel on kaggle to pdf and download it?,"TL;DR
As of now, downloading the jupyter notebook and then converting it to PDF is the quickest way.

If you still wish to convert the notebook to PDF on kaggle itself, you can do it using command line by following these steps:
Note that the Internet should be connected (check from the right menu).
1. Ensure all necessary libraries are installed.
!pip install nbconvert  # already-installed (most probably)
!apt install pandoc  # already-installed
!apt install texlive-xetex -y  # this'll take a long time

2. Use nbconvert to convert jupyter notebook to PDF
!ls  # will output notebook name e.g. `__notebook_source__.ipynb`

!jupyter nbconvert --execute --to pdf __notebook_source__.ipynb

3. Now, either you can Commit and Run (see Edit below) and get this file on Output tab or simply upload it using SSH or third-party service like transfer.sh (Use file.io, transfer.sh doesn't work anymore).
#!curl --upload-file __notebook_source__.pdf https://transfer.sh/notebook.pdf
!curl -F ""file=@__notebook_source__.pdf"" https://file.io

This will output a url like this:
https://file.io/uYWP0azE2soj Go to the url and get your file.

Note that you can skip installing texlive-xetex if you only want HTML file.

Edit: It turns out Kaggle has changed it GUI a little bit, so, if you're Commiting and Running by clicking on Save Version and you're doing a Quick Save, then you need to select Save output for this version in Advanced Settings in order to save output pdf file. Or you can simply use Save & Run All (Commit), it'll save output pdf as well.
"
How can I import data downloaded from Kaggle to DBFS using Databricks Community Edition?,"spark.read... works with DBFS paths by default, so you have two choices:

use file:/databricks/driver/... to force reading from the local file system - it will work on the community edition because it's single node cluster. It won't work on the distributed cluster

copy files to DBFS using the dbutils.fs.cp command (docs) and read from DBFS:


dbutils.fs.cp(""file:/databricks/driver/WDataFiles_Stage1/Cities.csv"", 
   ""/FileStore/Cities.csv"")
df = spark.read.csv(""/FileStore/Cities.csv"")
....

"
Is there any Command to Download data from particular folder from Kaggle Competition using kaggle API,"Could it be that the error message is true, and that the file is truly not in the dataset's folder?
Another idea is that it has to do with the order (?), because I was able to get your code running when using .sort_values() on the image names' Series:
data = pd.read_csv('driver_imgs_list.csv')
filenames = 'imgs/train/c2/' + data[data['classname'] == 'c2']['img'].sort_values()

for filename in filenames:
    api.competition_download_file('state-farm-distracted-driver-detection', filename)

However, I only let it run for like 10 files. So again it could be that there is a mismatch between the files in the CSV file and the files actually available in the dataset.
"
Using lambda conditional and pandas str.contains to lump strings,"Instead of using a Series.str method, you can use the in operator in your lambda to test for the substring
data['activity'] = data['activity'].apply(lambda x: 'skin diving' if 'skin diving' in x else x)

"
Download file from Kaggle to Google Colab,"Kaggle recommends using their own API instead of wget or rsync.
First, make an API token for Kaggle. On Kaggle's website go to ""My Account"", Scroll to API section and click on ""Create New API Token"" - It will download kaggle.json file on your machine.
Then run the following in Google Colab:
from google.colab import files
files.upload() # Browse for the kaggle.json file that you downloaded

# Make directory named kaggle, copy kaggle.json file there, and change the permissions of the file.
! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json

# You can check if everything's okay by running this command.
! kaggle datasets list

# Download and unzip sign-language-mnist dataset into '/usr/local'
! kaggle datasets download -d datamunge/sign-language-mnist --path '/usr/local' --unzip

Used info from here: https://www.kaggle.com/general/74235
"
Kaggle datasets into jupyter notebook,"I think that the name of the competition is wrong. Try:
from kaggle.api.kaggle_api_extended import KaggleApi

api = KaggleApi('copy and paste kaggle.json content here')
api.authenticate()
files = api.competition_download_files(""two-sigma-financial-news"")

"
cannot import name &#39;PartialState&#39; from &#39;accelerate&#39; when using Huggingface pipeline on Kaggle notebook?,"If you're seeing this error inside Kaggle notebooks, try installing from source,
! pip install -U git+https://github.com/huggingface/transformers.git
! pip install -U git+https://github.com/huggingface/accelerate.git

Then restart the kernel by clicking ""Restart and clear cell outputs"",

Now the imports will work without the error.
"
While converting a PIL image into a tensor why the pixels are changing?,"Q1: transforms.ToTensor() of torchvision normalizes your input image, i.e. puts it in the range [0,1] as it is a very common preprocessing step.
Q2: use torch.tensor(input_image) to convert image into a tensor instead.
"
Kaggle TypeError: slice indices must be integers or None or have an __index__ method,"As @ryankdwyer pointed out, it was an issue in the underlying statsmodels implementation which is no longer existent in the 0.8.0 release.
Since kaggle won't allow you to access the internet from any kernel/script, upgrading the package is not an option. You basically have the following two alternatives:

Use sns.distplot(myseries, bins=50, kde=False). This will of course not print the kde.
Manually patch the statsmodels implementation with the code from version 0.8.0. Admittedly, this is a bit hacky, but you will get the kde plot.

Here is an example (and a proof on kaggle):
import numpy as np

def _revrt(X,m=None):
    """"""
    Inverse of forrt. Equivalent to Munro (1976) REVRT routine.
    """"""
    if m is None:
        m = len(X)
    i = int(m // 2+1)
    y = X[:i] + np.r_[0,X[i:],0]*1j
    return np.fft.irfft(y)*m

from statsmodels.nonparametric import kdetools

# replace the implementation with new method.
kdetools.revrt = _revrt

# import seaborn AFTER replacing the method. 
import seaborn as sns

# draw the distplot with the kde function
sns.distplot(myseries, bins=50, kde=True)

Why does it work? Well, it relates to the way Python loads modules. From the Python docs:

5.3.1. The module cache
The first place checked during import search is sys.modules. This mapping serves as a cache of all modules that have been previously imported, including the intermediate paths. So if foo.bar.baz was previously imported, sys.modules will contain entries for foo, foo.bar, and foo.bar.baz. Each key will have as its value the corresponding module object.

Therefore, the from statsmodels.nonparametric import kdetools is inside this module cache. The next time seaborn acquires it, the cached version will be returned by the Python module loader. Since this cached version is the module that we have adapted, our patch of the revrt function is used. By the way, this practice is very handy when writing unit tests and is called mocking. 
"
Tensorflow &quot;decode_png&quot; keeps printing &quot;Cleanup called...&quot;,"Updating my TensorFlow installation to version 2.8 fixed the issue for me.
"
SKLearn - Principal Component Analysis leads to horrible results in knn predictions,"When you are processing the test data, you used fit_transform(X_test) which actually recomputes another PCA transformation on the test data. You should be using transform(X_test), so that the test data undergoes the same transformation as the training data.
The portion of code will look something like (thanks ogrisel for the whiten tip):
estimator = PCA(n_components=350, whiten=True)
X_train_pca = estimator.fit_transform(X_train)
X_test_pca = estimator.transform(X_test)

Try and see if it helps?
"
Is there any alternative way to download kaggle competition data in Colab?,"I'm sorry I'm only seeing this 9 months later, but the solution I found was simple. So simple I got annoyed. Hopefully, someone else is spared the wasted hours.
You need to accept the rules of the competition first!
While logged in to Kaggle, go to the competition page and look at the menu tabs. You should see something like this:

Select the 'Rules' tab and click on 'I Understand and Accept':

You should be good to go now.
"
Group by a column and sort by another column in R,"There's actually no need to group and summarise, since you are just looking for distinct / unique entries. A dplyr option is therefore:
select(movies, -movie) %>% 
  distinct() %>% 
  arrange(desc(director_rating))
#  director director_rating
#1    Dir 3            3000
#2    Dir 2            2000
#3    Dir 1            1000

Or in case you like to keep other columns:
distinct(movies, director, .keep_all = TRUE) %>%   # for dplyr >= 0.5.0
  arrange(desc(director_rating))
#    movie director director_rating
#1 Movie 4    Dir 3            3000
#2 Movie 2    Dir 2            2000
#3 Movie 1    Dir 1            1000

"
"How to Vectorize this R code Using Plyr, Apply, or Similar?","Here is a solution in base that is a little more concise:
md5s<-sapply(filelist,digest,file=TRUE,algo=""md5"")
split(filelist,md5s)

"
OSError: libmkl_intel_lp64.so.1: cannot open shared object file: No such file or directory,"import os 
os.environ['LD_LIBRARY_PATH']='/usr/local/lib'

!echo $LD_LIBRARY_PATH
!sudo ln -s /usr/local/lib/libmkl_intel_lp64.so /usr/local/lib/libmkl_intel_lp64.so.1
!sudo ln -s /usr/local/lib/libmkl_intel_thread.so /usr/local/lib/libmkl_intel_thread.so.1
!sudo ln -s /usr/local/lib/libmkl_core.so /usr/local/lib/libmkl_core.so.1

!ldconfig
!ldd /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch.so

worked for me. We will also try to fix the problem internally.
"
how to order seaborn pointplot,"In the code call  to grid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep'), the x category is the Pclass and the hue category is the Sex. Hence you need to add 
order = [1,2,3], hue_order=[""male"", ""female""]

Complete example (where I took the titanic that ships with seaborn - what wordplay!):
import seaborn as sns
import matplotlib.pyplot as plt

df = sns.load_dataset(""titanic"")

grid = sns.FacetGrid(df, row='embarked', size=2.2, aspect=1.6)
grid.map(sns.pointplot, 'pclass', 'survived', 'sex', palette='deep', 
             order=[1,2,3], hue_order=[""female"",""male""])
grid.add_legend()

plt.show()


Note that while hue_order is definitely required, you may leave out the order. While this will throw a warning, the correct order is garantied by the fact that those values are numerical and are hence automatically sorted.
"
R: Kaggle Titanic Dataset Random Forest NAs introduced by coercion,"You need to convert your char columns into factors. Factors are treated as integers internally whereas character fields are not. See the following small demonstration:
Data:
df <- data.frame(y = sample(0:1, 26, rep=T), x1=runif(26), x2=letters, stringsAsFactors=F)

df$y <- as.factor(df$y)

> str(df)
'data.frame':   26 obs. of  3 variables:
 $ y : Factor w/ 2 levels ""0"",""1"": 1 1 1 1 1 1 1 2 2 1 ...
 $ x1: num  0.457 0.296 0.517 0.478 0.764 ...
 $ x2: chr  ""a"" ""b"" ""c"" ""d"" ...

Now if I run my randomForest function:
> randomForest(y ~ x1 + x2, data=df)
Error in randomForest.default(m, y, ...) : 
  NA/NaN/Inf in foreign function call (arg 1)
In addition: Warning message:
In data.matrix(x) : NAs introduced by coercion

I get the same error you did.
Whereas if I convert the char column into factor:
df$x2 <- as.factor(df$x2)

> randomForest(y ~ x1 + x2, data=df)

Call:
 randomForest(formula = y ~ x1 + x2, data = df) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 1

        OOB estimate of  error rate: 61.54%
Confusion matrix:
  0  1 class.error
0 0 16           1
1 0 10           0

It works great!
"
How to sum values of one column based on other columns in pandas?,".groupby and .sum() for the home team and then do the same for the away team and add the two together:
df_new = df.groupby('home_team')['home_score'].sum() + df.groupby('away_team')['away_score'].sum()

output:
England     12
Scotland    34
Wales        1

More detailed explanation (per comment):

You need to only .groupby one column home_team. In your answer, you were grouping by ['home_team', 'home_score'] Your goal (no pun intended) is to get the .sum() of the home_score -- so you should NOT .groupby() it. As you can see ['home_score'] is after the part where I use .groupby, so that I can get the .sum() of it. That gets you set for the home teams.
Then, you do the same for the away_team.
At that point python / pandas is smart enough that since the results of the home_team and away_team groups have the same values for countries, you can simply add them together...

"
Resolving PyDev Unresolved imports regarding numpy &amp; sklearn,"Maybe you have 2 python interpreters installed on your system and the one used by PyDev is not the one where numpy is installed.
Type which python in a terminal to know the path of the python installed in your PATH where numpy is installed and then go to the PyDev preference menu to know which python is configured for PyDev.
"
Python .loc confusion,"It set column Sex to 1 if condition is True only, another values are untouched:
titanic[""Sex""] == ""male""

Sample:
titanic = pd.DataFrame({'Sex':['male','female', 'male']})
print (titanic)
      Sex
0    male
1  female
2    male

print (titanic[""Sex""] == ""male"")
0     True
1    False
2     True
Name: Sex, dtype: bool

titanic.loc[titanic[""Sex""] == ""male"", ""Sex""] = 0
print (titanic)

0       0
1  female
2       0

It is very similar by boolean indexing with loc - it select only values of column Sex by condition:
print (titanic.loc[titanic[""Sex""] == ""male"", ""Sex""])
0    male
2    male
Name: Sex, dtype: object

But I think here better is use map if only male and female values need convert to some another values:
titanic = pd.DataFrame({'Sex':['male','female', 'male']})
titanic[""Sex""] = titanic[""Sex""].map({'male':0, 'female':1})
print (titanic)
   Sex
0    0
1    1
2    0

EDIT:
Primary loc is used for set new value by index and columns:
titanic = pd.DataFrame({'Sex':['male','female', 'male']}, index=['a','b','c'])
print (titanic)
      Sex
a    male
b  female
c    male

titanic.loc[""a"", ""Sex""] = 0
print (titanic)
      Sex
a       0
b  female
c    male

titanic.loc[[""a"", ""b""], ""Sex""] = 0
print (titanic)
    Sex
a     0
b     0
c  male

"
How to prevent Azure ML Studio from converting a feature column to DateTime while importing a dataset,"I have tried with you sample data and here is my quick and dirty solution:
1) Add any symbol (I've added the '#') in front of each date
2) Load it to AML Studio (it is now considered as a string feature)
3) Add a Python/R component to remove the '#' symbol and explicitly convert the column to string (as.string(columnname) or str(columnname))
Hope this helps
"
"Error in eval(expr, envir, enclos) : could not find function &quot;eval&quot;","Take a look at the latest ggplot2 code on github. ggproto now replaces proto among other changes.
The code below should work fine.
 GeomRasterDigit <- ggproto(ggplot2:::GeomRaster, expr={
 draw_groups <- function(., data, scales, coordinates, digits, ...) {
 bounds <- coord_transform(coordinates, data.frame(x = c(-Inf, Inf), y = c(
 - Inf, Inf)), scales)
 x_rng <- range(bounds$x, na.rm = TRUE) 
 y_rng <- range(bounds$y, na.rm = TRUE)
 rasterGrob(as.raster(rowToMatrix(digits[data$rows,])), x_rng[1], y_rng[1], 
 diff(x_rng), diff(y_rng),default.units = ""native"", just =c(""left"",""bottom""),
 interpolate = FALSE)
 }
 })

There is a vignette about ggproto that is a good read.
"
AttributeError: &#39;Simple_Imputer&#39; object has no attribute &#39;fill_value_categorical&#39;&#39; in PyCaret,"@eddygeek's answer is correct. I faced the same error, and forcing the installation of scikit-learn 0.23.2 did the trick.
pip install scikit-learn==0.23.2 --force-reinstall

"
Import Kaggle csv from download url to pandas DataFrame,"You are creating a stream and passing it directly to pandas. I think you need to pass a file like object to pandas. Take a look at this answer for a possible solution (using post and not get in the request though).
Also i think the login url with redirect that you use is not working as it is. I know i suggested that here. But i ended up not using is because the post request call did not handle the redirect (i suspect).
The code i ended up using in my project was this:
def from_kaggle(data_sets, competition):
    """"""Fetches data from Kaggle

    Parameters
    ----------
    data_sets : (array)
        list of dataset filenames on kaggle. (e.g. train.csv.zip)

    competition : (string)
        name of kaggle competition as it appears in url
        (e.g. 'rossmann-store-sales')

    """"""
    kaggle_dataset_url = ""https://www.kaggle.com/c/{}/download/"".format(competition)

    KAGGLE_INFO = {'UserName': config.kaggle_username,
                   'Password': config.kaggle_password}

    for data_set in data_sets:
        data_url = path.join(kaggle_dataset_url, data_set)
        data_output = path.join(config.raw_data_dir, data_set)
        # Attempts to download the CSV file. Gets rejected because we are not logged in.
        r = requests.get(data_url)
        # Login to Kaggle and retrieve the data.
        r = requests.post(r.url, data=KAGGLE_INFO, stream=True)
        # Writes the data to a local file one chunk at a time.
        with open(data_output, 'wb') as f:
            # Reads 512KB at a time into memory
            for chunk in r.iter_content(chunk_size=(512 * 1024)):
                if chunk: # filter out keep-alive new chunks
                    f.write(chunk)

Example use: 
sets = ['train.csv.zip',
        'test.csv.zip',
        'store.csv.zip',
        'sample_submission.csv.zip',]
from_kaggle(sets, 'rossmann-store-sales')

You might need to unzip the files.
def _unzip_folder(destination):
    """"""Unzip without regards to the folder structure.

    Parameters
    ----------
    destination : (str)
        Local path and filename where file is should be stored.
    """"""
    with zipfile.ZipFile(destination, ""r"") as z:
        z.extractall(config.raw_data_dir)

So i never really directly loaded it into the DataFrame, but rather stored it to disk first. But you could modify it to use a temp directory and just delete the files after you read them.
"
How to convert a Kaggle R notebook to pdf or html?,"Updated answer (I just realized you were looking for not just PDF, but PDF OR HTML):
You should be able to just click File>Save Version and have your notebook render, so I suspect that some of your settings are wrong if you're not able to do that.

Go through this checklist to make sure it's setup correctly. In the editor, make sure your settings are as follows:


Editor menu bar>File>Editor Type: select Script
Editor menu bar>File>Language: select 'RMarkdown'


Make sure your YAML includes the output format. It should look like below, with the critical line being output: html_notebook

---
title: ""Untitled""
author: ""Garrett""
date: ""July 10, 2014""
output: html_notebook
---


Click on the menu bar>File>Save Version. This will render your report and save the output.

In the window that pops-up showing that your workbook has being created, click on the three dots and select 'Open in Viewer' when it says it was Successful in creating it.

In the 'Report' view for this report, right-click on something from your report and select 'View Frame Source'. This will open up the HTML source code for your report. (I'm using Chrome on windows - not sure if other browsers have same options).

With the source window open, remove view-source: from the start of the URL and you'll see just the HTML output of your notebook. Right click the page to print to PDF or save to HTML.


Note: There are some settings in the HTML/javascript that prevent scrolling of the window, but that's not an area I'm familiar with so can't help with that
"
ValueError: Trying to create optimizer slot variable under the scope for tf.distribute.Strategy,"The error looks a bit like the one described here:
https://github.com/tensorflow/tensorflow/issues/32561
Looking at your error trace, it (edited) is complaining:

ValueError: Trying to create optimizer slot variable under the scope
for tf.distribute.Strategy, which is different from the scope used for
the original variable TPUMirroredVariable

so it looks like something you call in your train() method might need to be within the tpu_strategy scope.
"
Unable to load model trained on Pytotch version 1.6.0 in Pytorch version 1.5.1,"In torch.__version__ == 1.6.0 :
torch.save(model_.state_dict(), 'best_model.pth.tar', use_new_zipfile_serialization=False)
then in torch.__version__ == 1.5.1:
torch.load('best_model.pth.tar',map_location='cpu')
"
How to execute Kaggle Api commands on windows system?,"Assuming the Kaggle API has been successfully installed using pip and the python install location along with the location of the Scripts\ folder have been added into the PATH; the execution of kaggle directly within Windows command prompt (CMD) should be able.
In order to ensure Python and the folder Scripts\ have been added into the PATH execute the command WHERE python3 succeeding WHERE kaggle.
If any of the two commands above produce an equivalent output of INFO: Could not find files for the given pattern(s) manually modify the PATH using the directions in Excursus: Setting environment variables to add both python install location and location of the Scripts\ folder.
"
Getting the message &#39;Cleanup called...&#39; repeatedly while training a model on kaggle. How can we get rid of this? (CNN model using Keras),"I found this on Kaggle and it has worked for me.
Just paste the code below and it should hopefully work for you too.
from IPython.display import clear_output

!pip install -q tensorflow==2.4.1

clear_output()

"
kaggle could not download resnet50 pretrained model,"Internet setting comes off as default on kaggle. If you turn on it, you can download the pre-trained models.
On the right side of the kernel, you will see the settings you can enable the Internet there.

"
AttributeError: module &#39;IPython.utils.traitlets&#39; has no attribute &#39;Unicode&#39;,"The warning message is saying that traitlets is now a top level package, and not still under the IPython.utils package.
You can fix the error by editing the data_table.py file ( /opt/conda/lib/python3.7/site-packages/google/colab/data_table.py). Change the line importing the traitlets package from
from IPython.utils import traitlets as _traitlets
to
import traitlets as _traitlets
I have no idea why this error arose, but this fixed it for me anyway.
"
kaggle kernels: urllib.request.urlopen not working for any url,"The reason this isn't working for you is because Kaggle Kernels currently don't currently have internet access. As a result, there's not a way for you to make API calls that require a network connection from within kernels.
Edit August 2018: Just FYI, we have now added internet access to Kernels. :) You can enable it in the left-hand side bar from within the editor.  
"
Uncaught Error: Script error for &quot;plotly&quot; http://requirejs.org/docs/errors.html#scripterror,"I had the same issue.
There seems to be an issue with the source from which Plotly is loaded. Changing the notebook mode might help.
from plotly.offline import plot, iplot, init_notebook_mode
init_notebook_mode(connected=True)

Taken from here:
https://www.kaggle.com/product-feedback/138599
"
upload files from kaggle to google drive,"I was facing the same problem and was able download files from Kaggle to Colab then move to Google Drive. For example, if the current directory is /kaggle/working and the file to move is processed_file.zip then,
From Kaggle
from IPython.display import FileLink
FileLink(r'processed_file.zip')

This will generate a link,
https://....kaggle.net/...../processed_file.zip

From Colab
!wget ""https://....kaggle.net/...../processed_file.zip""

Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

Copy File to Google Drive
!cp ""/content/processed_file.zip"" ""/content/drive/My Drive/workspace""

"
PyCaret methods on GPU/TPU,"Only some models can run on GPU, and they must be properly installed to use GPU. For example, for xgboost, you must install it with pip and have CUDA 10+ installed (or install a GPU xgboost version from anaconda, etc). Here is the list of estimators that can use GPU and their requirements: https://pycaret.readthedocs.io/en/latest/installation.html?highlight=gpu#pycaret-on-gpu
As Yatin said, you need to use use_gpu=True in setup(). Or you can specify it when creating an individual model, like xgboost_gpu = create_model('xgboost', fold=3, tree_method='gpu_hist', gpu_id=0).
For installing CUDA, I like using Anaconda since it makes it easy, like conda install -c anaconda cudatoolkit. It looks like for the non-boosted methods, you need to install cuML for GPU use.
Oh, and looks like pycaret can't use tune-sklearn with GPU (in the warnings here at the bottom of the tune_model doc section).
"
How to get kaggle competition data via command line on virtual machine?,"Install CurlWget chrome extension.
start downloading your kaggle data-set. CurlWget will give you full wget command. paste this command to terminal with sudo.
Job is done.
"
pandas: pandas.DataFrame.describe returns information on only one column,"By default, describe only works on numeric dtype columns. Add a keyword-argument include='all'. From the documentation:

If include is the string ‘all’, the output column-set will match the
  input one.

To clarify, the default arguments to describe are include=None, exclude=None. The behavior that results is:

None to both (default). The result will include only numeric-typed
  columns or, if none are, only categorical columns.

Also, from the Notes section:

The output DataFrame index depends on the requested dtypes:
For numeric dtypes, it will include: count, mean, std, min, max, and
  lower, 50, and upper percentiles.
For object dtypes (e.g. timestamps or strings), the index will include
  the count, unique, most common, and frequency of the most common.
  Timestamps also include the first and last items.

"
"Dot preceding parentheses, .( ), in data.table",".() is a data.table convenience function, acting as a terse alias for list(). Complicating matters just a little bit (mostly for those, like you, trying to figure out what the heck that . does!) is the fact that it's only interpreted as such within the scope of a call to  [.data.table().
Here, from ?data.table:
 DT = data.table(x=rep(c(""a"",""b"",""c""),each=3), y=c(1,3,6), v=1:9)
 setkey(DT,x,y)             # 2-column key

 DT[""a""]                    # join to 1st column of key
 DT[.(""a"")]                 # same, .() is an alias for list()
 DT[list(""a"")]              # same

 ## But note that *this* doesn't work (my addition --- not in ?data.table)
 .(""a"")

See also the vignette Introduction to data.table:

data.table also allows wrapping columns with .() instead of list(). It is an alias to list(); they both mean the same. Feel free to use whichever you prefer

"
Is LightGBM available for Mac M1?,"As of this writing, no official release of lightgbm (the Python package for LightGBM) supports the M1 Macs (which us ARM chips).
osx-arm64 builds of lightgbm are supported by the lightgbm conda-forge feedstock, so you can install lightgbm on an M1 Mac using conda.
conda install \
   --yes \
   -c conda-forge \
   'lightgbm>=3.3.3'

Progress towards officially supporting M1 Mac builds of LightGBM can be tracked in microsoft/LightGBM#5269 and microsoft/LightGBM#5328.
"
creating barplots using for loop using pandas/matplotlib,"You can use matplotlib to create separate axes and then plot each histogram on a different axis. Here is some sample code:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Create some dummy data
data = pd.DataFrame()
zones = [""east"",""west"",""south"",""central""]

data['zone']=np.hstack([[zone]*np.random.randint(10,20) for zone in zones])
data['OS Usage']=np.random.random(data.shape[0])

# Now create a figure
fig, axes = plt.subplots(1,4, figsize=(12,3))

# Now plot each zone on a particular axis
for i, zone in enumerate(zones):
    data.loc[data.zone==zone].hist(column='OS Usage',
                                   bins=np.linspace(0,1,10),
                                   ax=axes[i],
                                   sharey=True)
    axes[i].set_title('OS Usage in {0}'.format(zone))
    axes[i].set_xlabel('Value')
    axes[i].set_ylabel('Count')

fig.tight_layout()
fig.show()

Which produces the following figure:

Notes:

You can use enumerate to loop through the zones and simultaneously generate an index to specify the axis used.
The tight_layout() will ensure all the labels, title, etc. fit nicely in the figure.
By specifying the axes you have much more control over them.

Hope this helps!
"
Python:Getting text from html using Beautifulsoup,"If you aren't going to try browser automation through selenium as @Ali suggested, you would have to parse the javascript containing the desired information. You can do this in different ways. Here is a working code that locates the script by a regular expression pattern, then extracts the profile object, loads it with json into a Python dictionary and prints out the desired ranking:
import re
import json

from bs4 import BeautifulSoup
import requests


response = requests.get(""https://www.kaggle.com/titericz"")
soup = BeautifulSoup(response.content, ""html.parser"")

pattern = re.compile(r""profile: ({.*}),"", re.MULTILINE | re.DOTALL)
script = soup.find(""script"", text=pattern)

profile_text = pattern.search(script.text).group(1)
profile = json.loads(profile_text)

print profile[""ranking""], profile[""rankingText""]

Prints:
1 1st

"
What does &quot;The indices parameter is deprecated and will be removed (assumed True) in 0.17&quot; mean?,"It means that where you use the indices keyword argument to call cross_validation.KFold that future versions will not support it:
cv = cross_validation.KFold(len(train), n_folds=5, indices=False)

According to the error message you will have the effect of indices=True for 0.17. The message states that they will be removing the keyword argument, likely they don't ignore unused keyword arguments, therefore, likely you will get a TypeError exception in 0.17 if you continue to try to pass in indices.
"
Pandas read_csv does not load a comma separated CSV properly,"Attention!
The main issue was downloading the data. If you run a problem of loading and processing the Kaggle Titanic Dataset, you may re-download the CSV from here and re-run your program.

You can pass delimiter=',':
df = pd.read_csv(""Desktop/data/train.csv"", delimiter=',')
print(df.head())

   PassengerId  Survived  Pclass  \
0            1         0       3   
1            2         1       1   
2            3         1       3   
3            4         1       1   
4            5         0       3   

                                                Name     Sex   Age  SibSp  \
0                            Braund, Mr. Owen Harris    male  22.0      1   
1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   
2                             Heikkinen, Miss. Laina  female  26.0      0   
3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   
4                           Allen, Mr. William Henry    male  35.0      0   

   Parch            Ticket     Fare Cabin Embarked  
0      0         A/5 21171   7.2500   NaN        S  
1      0          PC 17599  71.2833   C85        C  
2      0  STON/O2. 3101282   7.9250   NaN        S  
3      0            113803  53.1000  C123        S  
4      0            373450   8.0500   NaN        S  


print(df.columns)

Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',
       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],
      dtype='object')

Next, you can create a mapping of sorts:
mapping = {'male' : 0, 'female' : 1}

And you'll call pd.Series.replace:
df.Sex = df.Sex.replace(mapping)
print(df.Sex)

0    0
1    1
2    1
3    1
4    0
Name: Sex, dtype: int64

"
Overriding my company&#39;s proprietary pypi repository for a specific package (kaggle),"You can install directly using a url:
pip install kaggle -i https://pypi.python.org/simple

"
"TypeError: add(): argument &#39;other&#39; (position 1) must be Tensor, not numpy.ndarray","Ok, the error seems be for the latest pytorch-1.0.0, when degrade pytorch to pytorch-0.4.1, the code seems work (passed the error lines at this point). Still have no idea to make the code work with pytorch-1.0.0
"
Pandas head command does not give the expected results,"By default, pandas displays a summary form of the output if it has too many columns to be displayed in a readable way.  You can force it to display the actual data by doing print train_data.head().to_string(), but the output may be difficult to read because you have so many columns.
"
Kaggle - Complete Leaderboard Download,"You can try the Meta Kaggle dataset. It has files with team membership data and solutions submitted by team and competition.
P.S. Parsing competition web pages is indeed hard - I've spent hours trying to get info that way.
"
I am looking for a simple way to extract python code from a notebook on kaggle?,"Follow these steps:

Visit the kernel you wish to retrieve the code of.
Fork the kernel which will take you to an editor mode.
From the editor mode, download the Ipython notebook.
Run following command to convert the notebook into a python script and enjoy the code. !jupyter nbconvert --to script config_template.ipynb

For clear information on point 4, have a look on this webpage https://stackoverflow.com/a/19779226/10734525 
"
ValueError while implementing the train_test_split,"Although the error arose from the line you pointed out, the actual problem is in this line:
    train_x, val_x, train_y, val_x = train_test_split(x, y,random_state = 0)

Notice that you have two val_x in there. The second val_x should be val_y. What happened was, you set val_x, which should be a 2-D array of inputs, to what should have been y values which are 1-D arrays of predictions - thereby getting that ValueError saying you input a 1-D array where a 2-D array was expected. 
"
Can I use double question mark to lookup documentation in Kaggle kernals Jupyter Notebook?,"I had no problems doing that, but look you need to select a function, not the package itself.

"
how to install kaggle_datasets?,"The kaggle_datasets module can be used in Google Colab and Kaggle Notebook. It's not a part of pip.
If you want to load the dataset on a Kaggle Notebook, follow these steps.
(1) Click on ""Add or upload data""

(2) Search for the dataset that you want

(3) Expand the ""Data box"" and load the dataset from the specified path

If you want to train this AI locally, manually download the photo and Monet datasets from Monet CycleGAN Tutorial Data. As you follow the Notebook, remember to change str(GCS_PATH + '/monet_tfrec/*.tfrec') and str(GCS_PATH + '/photo_tfrec/*.tfrec') to the paths where you download the datasets. Don't import kaggle_datasets because you already have the datasets ready-to-go in your local disk.
"
Upload data to google bucket with kaggle api and use it in colab,"You could try this solution for your first issue. Not sure if wget is possible with the data set you need, but this suggests it's possible. But this isn't via the Kaggle API.
The second question, how to use data without copying it to the notebook, is you can actually mount the bucket as a disk to your instance. Then you could access the data directly.
So putting them together you could have the bucket mounted locally, and then move the data into it. Then you can access it in the notebook.
"
Noisy validation loss in Keras when using fit_generator,"I would look, in that order:

bug in validation_generator implementation (incl. steps - does it go through all pics reserved for validation?)
in validation_generator, do not use augmentation (reason: an augmentation might be bad, not learnable, and at train, it does achieve a good score only by hard-coding relationships which are not generalizable)
change train/val split to 50/50
calculate, via a custom callback, the validation loss at the end of the epoch (use the same function, but calling it with a callback produces different (more accurate, at certain, non-standard models) results)

If nothing of the above gives a more smooth validation loss curve, then my next assumption would be that this is the way it is, and I might need to work on the model architecture
"
"Error in train.default(x, y, weights = w, ...) : final tuning parameters could not be determined","The following should work:
model1 <- train(as.factor(Cover_Type) ~ Elevation + Aspect + Slope + Horizontal_Distance_To_Hydrology,
                          data = data.train,
                          method = ""rf"", tuneGrid = data.frame(mtry = 3))

Its always better to specify the tuneGrid parameter which is a data frame with possible tuning values. Look at ?randomForest and ?train for more information. rf has only one tuning parameter mtry, which controls the number of features selected for each tree.
You can also run modelLookup to get a list of tuning parameters for each model 
> modelLookup(""rf"")
#  model parameter                         label forReg forClass probModel
#1    rf      mtry #Randomly Selected Predictors   TRUE     TRUE      TRUE

"
Python Machine Learning/Data Science Project Structure,"We've started a cookiecutter-data-science project designed for Python data scientists that might be of interest to you, check it out here. Structure is explained here.
Would love feedback if you have it! Feel free to respond here, open PRs or file issues.

In response to your issue about re-using code by importing .py files into notebooks, the most effective way that our team has found is to append to the system path. This may make some people cringe, but it seems like the cleanest way of importing code into a notebook without lots of module boilerplate and a pip -e install.
One tip is to use the %autoreload and %aimport magics with the above. Here's an example:
# Load the ""autoreload"" extension
%load_ext autoreload

# always reload modules marked with ""%aimport""
%autoreload 1

import os
import sys

# add the 'src' directory as one where we can import modules
src_dir = os.path.join(os.getcwd(), os.pardir, 'src')
sys.path.append(src_dir)

# import my method from the source code
%aimport preprocess.build_features

The above code comes from section 3.5 in this notebook for some context.
"
pandas srt.lower() not working on dataframe column,"str.lower() does not modify the existing column.  It just returns a new Series with the lowercase transform applied.  If you want to overwrite the original column, you need to assign the result back to it:
df['sex'] = df.sex.str.lower()

"
Cannot access internet on Kaggle Notebook,"
Make sure your Kaggle account is phone verified by clicking ""Get phone verified"" and following the steps



After phone verification, the full settings menu should be visible. Toggle the ""Internet"" switch.



Done

"
Downloading files in Kaggle gives - &#39;failed: Temporary failure in name resolution&#39; error,"That was seen before with a kaggle kernel

For anyone in the future who has the same issue in a Kaggle kernel, you need to turn on internet and gpu.
On the right side of the kernels window, there should be a settings section:



You’re gonna want GPU turned on and internet connection turned on.

"
Import file with pandas to Jupyter Notebook running on iPad with the app carnets,"I’m the author of the app. The issue is related to iOS limitations on file access. 
Carnets can access all files in the App directory. Since you have issues, I guess the notebook is not in the App directory, but in another App. By opening the notebook, you granted Carnets access to the notebook,  but not to other files in the directory. 
The solution is to grant Carnets access to the directory that contains both the notebook and the dataset: at the file open screen, navigate to the  directory immediately above this one, then click “Select” (top right corner), then click on the directory, then click “Open”. 
You will access a screen showing the content of that directory. Navigate to your notebook, and it should be able to access the dataset. 
"
How to replace NaN values where the other columns meet a certain criteria?,"
I am trying to replace the unknown age of male, 1st class passengers
  with the average age of male, 1st class passengers.

You can split the problem into 2 steps. First calculate the average age of male, 1st class passengers:
mask = (df['Pclass'] == 1) & (df['Sex'] == 'male')
avg_filler = df.loc[mask, 'Age'].mean()

Then update values satisfying your criteria:
df.loc[df['Age'].isnull() & mask, 'Age'] = avg_filler

"
&quot;Numpy not Available&quot; After installing Pytorch XLA,"At least in Google Colab I was able to solve this issue by running (after installing xla):
!pip install -U numpy

Not completely sure it will help in any context
"
Downloading Kaggle zip files in R,"library(RCurl)

#Set your browsing links 
loginurl = ""https://www.kaggle.com/account/login""
dataurl  = ""https://www.kaggle.com/c/titanic/download/train.csv""

#Set user account data and agent
pars=list(
  UserName=""suiwenfeng@live.cn"",
  Password=""-----""
)
agent=""Mozilla/5.0"" #or whatever 

#Set RCurl pars
curl = getCurlHandle()
curlSetOpt(cookiejar=""cookies.txt"",  useragent = agent, followlocation = TRUE, curl=curl)
#Also if you do not need to read the cookies. 
#curlSetOpt(  cookiejar="""", useragent = agent, followlocation = TRUE, curl=curl)

#Post login form
welcome=postForm(loginurl, .params = pars, curl=curl)

bdown=function(url, file, curl){
  f = CFILE(file, mode=""wb"")
  curlPerform(url = url, writedata = f@ref, noprogress=FALSE, curl = curl)
  close(f)
}

ret = bdown(dataurl, ""c:\\test.csv"",curl)

rm(curl)
gc()

FYI : use RCurl like a web client.
"
Your notebook tried to allocate more memory than is available. It has restarted,"Yes, kaggle has a memory limit: Its 8GB per kernel, or 20 Min running. It takes a lot of server juice to host such a thing.
There are various solutions for this problem, notably loading and processing the dataset in chunks.
There is this as well. But I have no experience with it.
And of course you can use another cloud platform such as AWS, GCP, Oracle, etc..
"
R xgboost importance plot with many features,"Check out the top_n argument to xgb.plot.importance. It does exactly what you want.
# Plot only top 5 most important variables.
print(xgb.plot.importance(importance_matrix = importance, top_n = 5))

Edit: only on development version of xgboost. Alternative method is to do this:
print(xgb.plot.importance(importance_matrix = importance[1:5]))

"
Simplifying categorical variables with python/pandas,"is that what you want?
In [181]: x
Out[181]:
      val
en  15011
zh    101
fr     99
de     53
es     53
ko     43
ru     21
it     20
ja     19
pt     14
sv     11
no      6
da      5
nl      4
el      2
pl      2
tr      2
cs      1
fi      1
is      1
hu      1

In [182]: x.groupby(x.index == 'en').sum()
Out[182]:
         val
False    459
True   15011

"
Ggvis bar chart - Choosing colors,"You're looking for ?scale_nominal.  Scaling is different than with ggplot2 - there are only three scales.
library(ggvis)

set.seed(0)
dat <- data.frame(Survived = factor(sample(0:1, 50, rep=T)))

dat %>% ggvis(~Survived, fill=~Survived) %>%
  layer_bars() %>%
  scale_nominal(""fill"", range = c(""red"", ""green""))

"
"TypeError: Tensor is unhashable. Instead, use tensor.ref()","As the error message says, you cannot use a tensor inside a Set directly, since it is not hashable. Try using a tf.lookup.StaticHashTable:
keys_tensor = tf.constant([1, 2, 3])
vals_tensor = tf.constant([1, 2, 3])
table = tf.lookup.StaticHashTable(
    tf.lookup.KeyValueTensorInitializer(keys_tensor, vals_tensor),
    default_value=-5)

print(table.lookup(tf.constant(1)))
print(table.lookup(tf.constant(5)))

tf.Tensor(1, shape=(), dtype=int32)
tf.Tensor(-5, shape=(), dtype=int32)

Alternatively, you could also use tf.where:
def check_value(value):
  frozen_set = tf.constant([1, 2, 3])
  return tf.where(tf.reduce_any(tf.equal(value, frozen_set), axis=0, keepdims=True), value, tf.constant(-5))

print(check_value(tf.constant(1)))
print(check_value(tf.constant(2)))
print(check_value(tf.constant(4)))

tf.Tensor([1], shape=(1,), dtype=int32)
tf.Tensor([2], shape=(1,), dtype=int32)
tf.Tensor([-5], shape=(1,), dtype=int32)

"
"In Kaggle, where can I store common code to be imported into multiple Notebooks?","Figured out how to do it:

Create a file like common_code.py to hold the code you want to import in Kaggle
Create a Kaggle DataSet like CommonCode and upload common_code.py to it
In the Notebook where you want to import the common code add the DataSet CommonCode
At the top of your Notebook, add the following code:

import sys
sys.path.append( ""/kaggle/input/CommonCode"" )

Then, at any subsequent point in the Notebook you can say
from common_code import *

While this shows an example of a single import-able file, you can use it for any number of files, and you can update the DataSet for additions or revisions.
"
"How to download data set into Colab? Stuck with a problem, it says &quot;401 - Unauthorized&quot;?","Do generate new API Token from your kaggle account ,it may have expired in past.
And put it into any of your google drive folder ,in google colab set your current working directory as that folder, and then visit the kaggle ,copy API command and with ""!+ API command"" without quotes in google colab you will be able to download it.
"
Permission denied while downloading dataset from Kaggle API,"When I wanted to download a dataset, I got this error. (I think it's related to your question)
Traceback (most recent call last):
  File ""/home/matin/.local/bin/kaggle"", line 5, in <module>
    from kaggle.cli import main
  File ""/home/matin/.local/lib/python3.9/site-packages/kaggle/__init__.py"", line 19, in <module>
    from kaggle.api.kaggle_api_extended import KaggleApi
  File ""/home/matin/.local/lib/python3.9/site-packages/kaggle/api/__init__.py"", line 22, in <module>
    from kaggle.api.kaggle_api_extended import KaggleApi
  File ""/home/matin/.local/lib/python3.9/site-packages/kaggle/api/kaggle_api_extended.py"", line 84, in <module>
    class KaggleApi(KaggleApi):
  File ""/home/matin/.local/lib/python3.9/site-packages/kaggle/api/kaggle_api_extended.py"", line 102, in KaggleApi
    os.makedirs(config_dir)
  File ""/home/matin/miniconda3/lib/python3.9/os.py"", line 225, in makedirs
    mkdir(name, mode)
PermissionError: [Errno 13] Permission denied: '/kaggle.json'

Solution
sudo mkdir /.kaggle
sudo cp kaggle.json /.kaggle
sudo chmod 600 /.kaggle/kaggle.json
sudo chown `whoami`: /.kaggle/kaggle.json

export KAGGLE_CONFIG_DIR='/.kaggle/'

Explanation
In KaggleApi source code, it's trying to find a directory from the environment.
    config_dir = os.environ.get('KAGGLE_CONFIG_DIR') or os.path.join(
        expanduser('~'), '.kaggle')
    if not os.path.exists(config_dir):
        os.makedirs(config_dir)

So if you add KAGGLE_CONFIG_DIR to your environment, it doesn't try to make a directory!
If you got ValueError: Error: Missing username in configuration., you should just change the owner by using chown command.
"
Normalizing variable using /= throws ufunc error,"Take a look at Augmented Assignments from the python docs: https://docs.python.org/3/reference/simple_stmts.html#index-14
... 
An augmented assignment evaluates the target (which, unlike normal assignment statements, cannot be an unpacking) and the expression list, performs the binary operation specific to the type of assignment on the two operands, and assigns the result to the original target. The target is only evaluated once.
An augmented assignment expression like x += 1 can be rewritten as x = x + 1 to achieve a similar, but not exactly equal effect. In the augmented version, x is only evaluated once. Also, when possible, the actual operation is performed in-place, meaning that rather than creating a new object and assigning that to the target, the old object is modified instead.
Unlike normal assignments, augmented assignments evaluate the left-hand side before evaluating the right-hand side. For example, a[i] += f(x) first looks-up a[i], then it evaluates f(x) and performs the addition, and lastly, it writes the result back to a[i].
... 
So you're partially correct, it's a float vs int thing, combined with whether the operation is performed in-place. 
x/=255.0 doesn't work because the operation is performed in-place. Let's look at your error message:
TypeError: ufunc 'true_divide' output (typecode 'd') could not be coerced to provided output parameter (typecode 'B') according to the casting rule ''same_kind''
Looking at the table below (https://docs.python.org/2/library/array.html), the type codes it's referencing are ""int"" and ""float"". x/=255.0 attempts to coerce an int (x) into a float (result of dividing x by 255.0). This is an unsafe cast, and you get your error message.

But x=x/255.0 is fine, because operation isn't performed in-place. On the right hand side we get the result of x/255.0, and we simply assign this value to x, as you would assign any float to any variable.
"
How to read file from Kaggle in Jupyter Notebook in Microsoft Azure?,"Kaggle has already provided extensive documentation for their command line API here, which has been built using Python and the source can be found here so reverse engineering it is very straight forward in order to use Kaggle API pythonically.
Assuming you've already exported the username and key as environment variables
import os
os.environ['KAGGLE_USERNAME'] = '<kaggle-user-name>'
os.environ['KAGGLE_KEY'] = '<kaggle-key>'
os.environ['KAGGLE_PROXY'] = '<proxy-address>' ## skip this step if you are not working behind a firewall

or
you've successfully downloaded kaggle.json from the API section in your Kaggle Account page and copied this JSON to ~/.kaggle/ i.e. the Kaggle configuration directory in your system.
Then, you can use the following code in your Jupyter notebook to load this dataset to a pandas dataframe:

Import libraries

import kaggle as kg
import pandas as pd



Download the dataset locally

kg.api.authenticate()
kg.api.dataset_download_files(dataset=""START-UMD/gtd"", path='gt.zip', unzip=True)


Read the downloaded dataset

df = pd.read_csv('gt.zip/globalterrorismdb_0718dist.csv', encoding='ISO-8859-1')

"
How to use data in a docker container?,"Note that I also assume you are using linux containers. This works in all platforms, but on windows you need to tell your docker process that that you are dealing with linux containers. (It's a dropdown in the tray)
It takes a bit of work to understand docker and the only way to understand it is to get your hands dirty. I recommend starting with making an image of an existing project. Make a Dockerfile and play with docker build . etc.
To cover the docker basics (fast version) first.

In order to run something in docker we first need to build and image
An image is a collection of files
You can add files to an image by making a Dockerfile
Using the FROM keyword on the first line you extend and image
by adding new files to it creating a new image
When staring a container we need to tell what image it should use
and all the files in the image is copied into the containers storage

The simplest way to get files inside a container:

Crate your own image using a Dockerfile and copy in the files
Map a directory on your computer/server into the container
You can also use docker cp, to copy files from and two a container,
but that's not very practical in the long run.

(docker-compose automates a lot of these things for you, but you should probably also play around with the docker command to understand how things work. A compose file is basically a format that stores arguments to the docker command so you don't have to write commands that are multiple lines long)
A ""simple"" way to configure multiple projects in docker in local development.
In your project directory, add a docker-dev folder (or whatever you want to call it) that contains an environment file and a compose file. The compose file is responsible for telling docker how it should run your projects. You can of course make a compose file for each project, but this way you can run them easily together.
projects/
    docker-dev/
        .env
        docker-compose.yml
    project_a/
        Dockerfile
        # .. all your project files
    project_b/
        Dockerfile
        # .. all your project files

The values in .env is sent as variables to the compose file. We simply add the full path to the project directory for now.
PROJECT_ROOT=/path/to/your/project/dir

The compose file will describe each of your project as a ""service"". We are using compose version 2 here.
version: '2'
services:
  project_a:
    # Assuming this is a Django project and we override command
    build: ${PROJECT_ROOT}/project_a
    command: python manage.py runserver 0.0.0.0:8000
    volumes:
      # Map the local source inside the container
      - ${PROJECT_ROOT}/project_a:/srv/project_a/
    ports:
      # Map port 8000 in the container to your computer at port 8000
      - ""8000:8000""
  project_a:
    # Assuming this is a Django project and we override command
    build: ${PROJECT_ROOT}/project_b
    volumes:
      # Map the local source inside the container
      - ${PROJECT_ROOT}/project_b:/srv/project_b/

This will tell docker how to build and run the two projects. We are also mapping the source on your computer into the container so you can work on the project locally and see instant updates in the container.
Now we need to create a Dockerfile for each out our projects, or docker will not know how to build the image for the project.
Example of a Dockerfile:
FROM python:3.6

COPY requirements.txt /requirements.txt

RUN pip install requirements.txt
# Copy the project into the image
# We don't need that now because we are mapping it from the host
# COPY . /srv/project_a

# If we need to expose a network port, make sure we specify that
EXPOSE 8000

# Set the current working directory
WORKDIR /srv/project_a

# Assuming we run django here
CMD python manage.py runserver 0.0.0.0:8000

Now we enter the docker-dev directory and try things out. Try to build a single project at a time.
docker-compose build project_a
docker-compose build project_b

To start the project in background mode.
docker-compose up -d project_a

Jumping inside a running container
docker-compose exec project_a bash

Just run the container in the forground:
docker-compose run project_a

There is a lot of ground to cover, but hopefully this can be useful.
In my case I run a ton of web servers of different kinds. This gets really frustrating if you don't set up a proxy in docker so you can reach each container using a virtual host. You can for example use jwilder-nginx (https://hub.docker.com/r/jwilder/nginx-proxy/) to solve this in a super-easy way. You can edit your own host file and make fake name entires for each container (just add a .dev suffix so you don't override real dns names)
The jwilder-nginx container will automagically send you to a specific container based on a virtualhost name you decide. Then you no longer need to map ports to your local computer except for the nginx container that maps to port 80.
"
How to use Rs neuralnet package in a Kaggle competition about Titanic,"Try using this to predict instead:
res = compute(r, m2[,c(""Pclass"", ""Sexmale"", ""Age"", ""SibSp"")])

That worked for me and you should get some output.
What appears to have happend: model.matrix creates additional columns ((Intercept)) which isn't part of the data which was used to build the neural net, as such in the compute function it doesn't know what do with it. So the solution is to select explicitly the columns needed to use in the compute function. This is because neuralnet tries to do some kind of matrix multiplication, but the matrix is of the wrong size.

For how many neurons, or optimizing hyper-parameters, you could use Cross-validation and all those other methods. If using a different package (nnet) is fine then you can use the caret package to determine the optimal parameters for you. It would look like this:
library(caret)
nnet.model <- train(Survived ~ Pclass + Sex + Age + SibSp, 
                    data=train, method=""nnet"")
plot(nnet.model)
res2 = predict(nnet.model, newdata=test)

with the plot of the hyperparameters being this:


You can measure performance using the confusionMatrix in the caret package:
library(neuralnet)
library(caret)
library(dplyr)
train <- read.csv(""train.csv"")
m <- model.matrix(  ~ Survived + Pclass + Sex + Age + SibSp, data =train )

r <- neuralnet( Survived ~ Pclass + Sexmale + Age + SibSp, 
                data=m, rep=20)

res = neuralnet::compute(r, m[,c(""Pclass"", ""Sexmale"", ""Age"", ""SibSp"")])
pred_train = round(res$net.result)

# filter only with the ones with a survival prediction, not all records
# were predicted for some reason;
pred_rowid <- as.numeric(row.names(pred_train))
train_survived <- train %>% filter(row_number(Survived) %in% pred_rowid) %>% select(Survived)
confusionMatrix(as.factor(train_survived$Survived), as.factor(pred_train))

Output:
Confusion Matrix and Statistics

          Reference
Prediction   0   1
         0 308 128
         1 164 114

               Accuracy : 0.5910364             
                 95% CI : (0.5539594, 0.6273581)
    No Information Rate : 0.6610644             
    P-Value [Acc > NIR] : 0.99995895            

                  Kappa : 0.119293              
 Mcnemar's Test P-Value : 0.04053844            

            Sensitivity : 0.6525424             
            Specificity : 0.4710744             
         Pos Pred Value : 0.7064220             
         Neg Pred Value : 0.4100719             
             Prevalence : 0.6610644             
         Detection Rate : 0.4313725             
   Detection Prevalence : 0.6106443             
      Balanced Accuracy : 0.5618084             

       'Positive' Class : 0    

"
Cannot use &#39;kaggle.competitions&#39; in Kaggle kernel,"I believe it is because your notebook is not attached to the dataset.
See this forum post for details on how to do so: https://www.kaggle.com/c/nfl-big-data-bowl-2020/discussion/113684.

the easiest is to create it from within the competition: click the ""Notebooks"" tab on any page in the competition — or just go here: https://www.kaggle.com/c/nfl-big-data-bowl-2020/notebooks — and click on ""New Notebook"" then the notebook should be set up properly.

"
Error trying to run Rmd file on Kaggle Kernel,"You can write a markdown (.md) natively with Kaggle kernels. Check out this example. However I don't think Kaggle kernel will render the same performance as RStudio.  
"
Trying to understand why a nested for loop is called only the first time,"In Python 3, zip() returns an iterator.  You can only iterate through the elements of an iterator once.  To replicate the Python 2 behavior, use list(zip(...)) in place of zip(...).
"
Deploy R markdown document on shinyapps.io,"Issue solved with R Studio version
Version 0.99.451 – © 2009-2015 RStudio, Inc.

!Note: In the newest version the deploy button has been replaced with Publish
This has not been adapted in the official shiny documentation yet. The screenshot below is the view you get once a first publish (formerly called deploy) action has been taken.

"
"Can the Kinect SDK be run with saved Depth/RGB videos, instead of a live Kinect?","It is not a feature within the SDK itself, however you can use something like the Kinect Toolbox OSS project (http://kinecttoolbox.codeplex.com/) which provides Skeleton record and replace functionality (so you don't need to stand in front of your Kinect each time). You do however still need a Kinect plugged in to your machine to use the runtime. 
"
Resolving Dependency Conflicts in Kaggle Notebook: Incompatibility Issues with Installed Packages ( lightning-flash and torchvision ),"You can try using conda installations in kaggle. Example:
!conda install pytorch::torchvision

You can try installing spesific versions of the libraries that are known to be compatible(you can google search for them) Example:
!pip install torchvision==0.15.0

You can try reinstalling the packages or reinstalling a spesific version like this:
!pip uninstall torchvision

then
!pip install torchvision==0.15.0

Important sidenote:
Checking what libraries and versions are installed in your kaggle enviroment.
You can use
!pip freeze

Example:

"
OSError: Could not find kaggle.json. Make sure it&#39;s located in C:\Users\Lior\.kaggle. Or use the environment method,"
Sample photo for finding the accounts page
"
How to use custom dataset generators with TPU?,"Have you tried defining shape of an output in from_generator() function?
"
Simple code running on google colab but not working on kaggle,"This code is running fine on Kaggle. It takes some time but will complete.
Check again
output:
dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])
"
Kaggle kernel is not using GPU,"GPUs are only helpful if you are using code that takes advantage of GPU-accelerated libraries (e.g. TensorFlow, PyTorch, etc.) mostly for training deep learning models. scikit-learn, which you're using here, doesn't support GPUs (and it probably won't). Unless you want to use a different framework, I'd recommend turning the GPU off.
"
Style a kable table knitted to pdf,"I don't know whether you tried already, but the kableExtra package offers a lot more features than kable. You could simply pipe your kable call into row specifications with
%>% row_spec (0, bold = T)
I couldn't find an option for cell height there. In case you'd generally want to change it you could either pass it as an option into the YAML header or change the default.tex file.
Regards
"
Trying Kaggle Titanic with keras .. getting loss and valid_loss -0.0000,"Old post, but answering anyway in case someone else attempts Titanic with Keras.
Your network may have too many parameters and too little regularization (e.g. dropout).
Call model.summary() right before the model.compile and it will show you how many parameters your network has. Just between your two Dense layers you should have 512 X 512 = 262,144 paramters. That's a lot for 762 examples.
Also you may want to use a sigmoid activation on the last layer and binary_cross entropy loss as you only have two output classes.
"
"Pandas warning &#39;rows is deprecated, use index instead&#39;","From pandas.pivot_table in Pandas 1.2.4. the rows is not a parameter.
Therefore, changing rows to index should solve your problem
fare_means = df.pivot_table('Fare', index='Pclass', aggfunc='mean')

"
Try to make fine-tuning of model gpt like,"Try this :
   gradient_accumulation_steps=2,

"
OneHotEncoder gives ValueError : Input contains NaN ; even though my DataFrame doesn&#39;t contain any NaN as indicated by df.isna(),"I figured it out, a ColumnTransformer will concatenate the transformations instead of passing them along to the next transformer in line.
So any transformations done in fill_pipeline, won't be noticed by OneHotEncoder since it is still working with the untransformed dataset.
So I had to put the one hot encoding into the fill_pipeline instead of the ColumnTransformer.
full_pipeline = ColumnTransformer([
    (""fill"",fill_pipeline,list(strat_train_set)),
    (""emb_cat"",OneHotEncoder(),['Sex']),
    (""cat"",OneHotEncoder(),['Embarked']),
])

"
Unable to solve error In h2o happening in kaggle kernels,"
Failed to connect to localhost port 54321: Connection refused

This is an issue caused by how Kaggle is running H2O in their kernels (which are probably Docker images).  The H2O R client is not able to connect to the local H2O server running at localhost:54321.
Something you can try is to start the H2O cluster on a different port. So instead of running h2o.init() do something like h2o.init(port=44444).  If they are allowing many people to start H2O clusters on the same machine/port, that may cause some issues.  If you are already connected to the H2O cluster in your session, then first run h2o.shutdown(prompt = FALSE) before re-starting H2O on a different port.
I also suggest that you contact a Kaggle admin to see if they can help debug the issue.  We've seen issues like this before with Kaggle kernels.
"
Features and label size crashes (tflearn),"Try restarting the kernel and clear output  and to run it again .
I too faced this issue and this solution worked for me .
"
Python seaborn graphics,"If you are using IPython Notebook, simply use the %pylab inline command before you make your plot. If not, try plt.show() after you make your plot.
"
Is overfitting always a bad thing?,"First of all understand what over-fitting is.
you can see over fit when the training score is increasing (or error is decreasing) while your testing set score is decreasing (or error is increasing)
Over fit is when your training model is too precise and doesn't generalize on the problem you try to solve. In other words its too FIT for the training, and the training alone, so that it cannot solve/predict a different data set.
In your example it seems like both the errors of the train and the test are decreasing, which means you are not over fitting.
Over fitting is always a bad thing.
As for your current problem. If you want to run multiple cross validations, or manually split your data for many training and many testing sets you can do the following:

Split the data for training and testing (50%, 50%) or (70%, 30%), what ever you think is right for you
Then, randomly sample with X% your training data, and it'll be the training set.
Randomly sample the testing data with Y% and it'll be your testing set
I suggest X = Y = 75%. And the above split to be 70% training and 30% testing.

As for your questions:

It is just an indicator for over fit.
You are not over fitting by your example
Score will varies on different data sets
Same answer as 3

Adding a picture to describe over fitting: 
There is a point (10) in complexity where keep training will decrease the training error but will INCREASE the testing error.
"
do.call in R - Kaggle starter script,"Actually do.call is not needed. It could have (and should have) been written in a simpler fashion like this:
psum2 <- function(..., na.rm = FALSE) rowSums(cbind(...), na.rm = na.rm)

psum2(BOD, BOD)
# [1] 18.6 24.6 44.0 40.0 41.2 53.6

psum(BOD, BOD) # same
# [1] 18.6 24.6 44.0 40.0 41.2 53.6

Note: In general we use do.call when we don't know how many arguments are going to be passed to a function so we want to pass the function a list of them instead.  The following:
L <- list(arg1, arg2, arg3)
do.call(f, L)

is same as:
f(arg1, arg2, arg3)

but in the first case we can dynamically create L so that it could have any number of arguments whereas the second case is harded coded to three arguments.
For example this code which can be varied by varying n (where n can be 1, 2, 3, ...):
n <- 3
L <- lapply(1:n, function(i) i * BOD) # create list of n components

rowSums(do.call(cbind, L))
[1]  55.8  73.8 132.0 120.0 123.6 160.8

vs. this code which is hard coded to use 3 arguments to cbind:
rowSums(cbind(BOD, 2*BOD, 3*BOD)) # hard coded
[1]  55.8  73.8 132.0 120.0 123.6 160.8

"
Get object size from read_csv in pandas,"It is expected, because DataFrame.size working different, it counts all values of DataFrame.
data = pd.DataFrame({
        'A':list('abcdef'),
         'B':[4,5,4,5,5,4],
         'Survived':[7,8,9,4,2,3],

})

print (data)
   A  B  Survived
0  a  4         7
1  b  5         8
2  c  4         9
3  d  5         4
4  e  5         2
5  f  4         3

#3 columns x 6 rows = 18
print (data.size)
18

y = data[""Survived""]
print (y)
0    7
1    8
2    9
3    4
4    2
5    3
Name: Survived, dtype: int64

#nuber of values in Series/column
print (y.size)
6

If want number of rows and columns:
print (data.shape)
(6, 3)

Or number of rows:
print (len(data))
6

"
List comprehensions where entry occurs twice in a row,"in your example, len(meals) is 2, so len(meals)-2 is 0, so the range is empty.
you only need to subtract 1.
try this:
def menu_is_boring(meals):
    return any([meals[l] == meals[l + 1] for l in range(0, len(meals) - 1)])


print(menu_is_boring([""Spam"", ""Spam""]))

Output:

True

it's worth noting that python's range is not inclusive of the stop param (i.e., list(range(1,5)) is [1, 2, 3, 4])
"
python pandas upper() not work for string columns,"It's because your columns Cabin and Embarked contain NaN values which have dtype np.float. You could check it with casting type for your apply:
In [355]: train.Cabin.apply(lambda x: type(x))[:10]
Out[355]:
0    <class 'float'>
1      <class 'str'>
2    <class 'float'>
3      <class 'str'>
4    <class 'float'>
5    <class 'float'>
6      <class 'str'>
7    <class 'float'>
8    <class 'float'>
9    <class 'float'>
Name: Cabin, dtype: object

So you could use str.upper which handle NaN by default. 
Or you could fill your NaN values to empty string '' with fillna which has upper method and then use your `lambda function:
In [363]: train.Cabin.fillna('').apply(lambda x: x.upper)[:5]
Out[363]:
0
1     C85
2
3    C123
4
Name: Cabin, dtype: object

In [365]: train.Cabin.str.upper()[:5]
Out[365]:
0     NaN
1     C85
2     NaN
3    C123
4     NaN
Name: Cabin, dtype: object

Or if you'd like to save NaN as sting you could fillna with NaN string:
In [369]: train.Cabin.fillna('NaN').apply(lambda x: x.upper())[:5]
Out[369]:
0     NAN
1     C85
2     NAN
3    C123
4     NAN
Name: Cabin, dtype: object

"
Converting a column in R to type datetime: as.POSIXct returns NA,"Your input formatting codes are not correct.  Should be ""%m/%d/%Y %H:%M:%S"" instead of ""%M/%D%/%Y% %H:%M%S"".  Everything else is fine:
library(dplyr)

df <- read.table(header = TRUE, text = '
 Date.Time          Lat     Lon         Base
""4/1/2014 0:11:00""    40.7690 -73.9549    B02512
""4/1/2014 0:17:00""    40.7267 -74.0345    B02512
""4/1/2014 0:21:00""    40.7316 -73.9873    B02512
""4/30/2014 23:31:00""  40.7443 -73.9889    B02764
""4/30/2014 23:32:00""  40.6756 -73.9405    B02764
""4/30/2014 23:48:00""  40.6880 -73.9608    B02764')

df2 <- mutate(df, Date.Time = as.POSIXct(Date.Time, 
                                        format = ""%m/%d/%Y %H:%M:%S""))

str(df2)
# 'data.frame': 6 obs. of  4 variables:
#   $ Date.Time: POSIXct, format: ""2014-04-01 00:11:00"" ""2014-04-01 00:17:00"" ""2014-04-01 00:21:00"" ...
# $ Lat      : num  40.8 40.7 40.7 40.7 40.7 ...
# $ Lon      : num  -74 -74 -74 -74 -73.9 ...
# $ Base     : chr  ""B02512"" ""B02512"" ""B02512"" ""B02764"" ...

"
Unable to write y/n to proceed in the installation of a library in kaggle,"I looked upon Anaconda website FAQ and there is an answer specific to this. You can check the article in here
Be sure to provide -y to specify yes to the install prompt as you can not submit input to the commands when running.
In my example:
!conda install -c conda-forge gmaps -y

"
Read dataset from Kaggle,"I found a solution based on the answer posted here. Someone posted the link in the comment but I don't see the comment any more. Thank you Good Samaritan!
library(httr)
dataset <- httr::GET(""https://www.kaggle.com/api/v1/competitions/data/download/10445/train.csv"", 
                 httr::authenticate(username, authkey, type = ""basic""))

temp <- tempfile()
download.file(dataset$url,temp)
data <- read.csv(unz(temp, ""train.csv""))
unlink(temp)

"
Download Kaggle output file,"This does somewhat similar task, upload files from kaggle to google drive.
You can use below snippet to generate a file download link for h5, zip, tar.gz and more, then download using wget, browser, download manager etc.
The download link will be shown in output of the cell. You may need to enable internet in kaggle notebook settings (should still work when disabled).
from IPython.display import FileLink
FileLink(r'outputname.tar.gz')

from IPython.display import FileLink
FileLink(r'outputname.h5')

"
Kaggle - stopwords download - download false,"This error occurs if your kernel does not have internet turned on. You can turn the internet on in your settings
Solution 1 - Turn on kernel internet

On the right side of your notebook, open Settings
Switch Internet to 'On'. You may need to do additional verification for your account


Solution 2 - Directly add the nltk_data
An easy way to resolve this without turning on internet on your kernel is to do the following:

Select 'Add Data' in the top right of the notebook
Search for nltk_data, click 'Add' button
Update your code to include the following:

import nltk
from nltk.corpus import stopwords

"
Find average based upon two conditions; create column from these averages,"Use groupby.transform:
df['AVE MONTHLY TEMP']=df.groupby(['ID','MONTH'])['TEMP'].transform('mean')
print(df)

Output
    ID  MONTH  TEMP  AVE MONTHLY TEMP
0    1      1     0                 5
1    1      1    10                 5
2    2      1    50                55
3    2      1    60                55
4    3      1    80                85
5    3      1    90                85
6    1      2     0                 5
7    1      2    10                 5
8    2      2    50                55
9    2      2    60                55
10   3      2    80                85
11   3      2    90                85

"
"IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices&quot;","counter = counter + 1.
should be
counter = counter + 1 (note the dot) or counter += 1. 
The dot makes counter a float (since 1. is equivalent to 1.0) and floats can not be used as indexes.
"
How to produce a Kaggle submission CSV file with specific entries?,"How about this:
submission = pd.DataFrame({ 'PassengerId': test_data.passengerid.values, 'Survived': test_predictions })
submission.to_csv(""my_submission.csv"", index=False)

"
Python reshaping a np 4D array into a 2D array. Kaggle Image data,"I think it's the way you're using transpose. What should be passed as argument should be a list of positions, in your case integers between 0 and 3.
I guess you're trying to reverse the indexes of npXt, so maybe (3,2,1,0) should be put instead of (28709,48,48,1) as argument of the transpose function.
"
Kaggle Titanic with tflearn neural network,"Using softmax as an activation layer in the output ensures that the sum of the outputs across all nodes in that layer is 1. Since you only have a single node, and the output has to sum to 1, it will always output 1 by definition. 
You should never use softmax as your activation for a binary classification task. A better option is the logistic function, which I think tensorflow calls sigmoid.
So instead of
net = tflearn.fully_connected(net, 1, activation='softmax')

try
net = tflearn.fully_connected(net, 1, activation='sigmoid')

"
The system cannot find the file specified for check_output,"add shell=True will be fined
from subprocess 

import check_output print(check_output([""dir"", ""D:\\C_Drive\\Desktop\\Kaggle""],shell=True).decode(""utf-8""))

inspired by this post
Windows can't find the file on subprocess.call()
"
uncompress a big .gz file,"This file decompresses to a 22GB .csv file. You can't process it all at once in R on your 6GB machine because R needs to read everything into memory. It would be best to process it in an RDBMS like postgresql. If you are intent on using R you could process it in chunks, reading a manageable number of rows at a time: read a chunk, process it, and then overwrite with the next chunk. For this data.table::fread would be better than the standard read.table.
Oh, and don't decompress in R, just run gunzip from the command line and then process the csv. If you're on Windows you can use winzip or 7zip.
"
Plotly FigureWidgets doesn&#39;t get loaded in kaggle&#39;s notebook,"Finally, I was able to find a solution for this bug with an help from APollner. I had to ask in kaggle discussion. Here is the solution. Hope this helps future coders!
"
Can&#39;t subset column even if the column exists using SELECT from DPLYR package,"(Sorry, I can't leave a comment since I'm a newbie to StackOverflow.)
Is it possible that the column name is actually CRIMINAL CASES (with a space) or CRIMINAL\nCASES?
Could you try CRIMINAL\nCASES wrapped in backticks (`)?
The following worked for me without issues:
# creating test tibble
a <- 
  tribble(
  ~""CRIMINAL\nCASES"", ~""random"",
  ""hi"", ""bye""
)

a %>%
  select(`CRIMINAL\nCASES`)

"
how to download single file from kaggle website?,"the official instructions does have the answer

get the kaggle.json from Account tab under your name
place under your Windows/username in windows dir.
in jupyter notebook - run command kaggle competitions files home-credit-default-risk to list files
to download 1 file run kaggle competitions download home-credit-default-risk -f application_train.csv.7z'

"
Kaggle notebook is not able to read the dataset,"Solution:-
import os
print(os.listdir(""../input""))

It shows you what files you have available, which you can then copy-and-paste to the appropriate path.
"
Possibility to save uploaded data in Google Colab for reopening,"You can leverage the storage capacity of GoogleDrive. Colab allows you to have this data stored on your Drive and access it from colab notbook as follows:
from google.colab import drive
import matplotlib.image as mpimg 
import matplotlib.pyplot as plt
import pandas as pd

drive.mount('/content/gdrive')
img = mpimg.imread(r'/content/gdrive/My Drive/top.bmp')  # Reading image files
df = pd.read_csv('/content/gdrive/My Drive/myData.csv')  # Loading CSV

When it mounts, it would ask you to visit a particular url to grant permission for accessing drive. Just paste the token returned. Needs to be done only once.
The best thing about colab is you can also run shell cmds from code, all you need to do is to prefix the commands with a ! (bang). Useful when you need to unzip etc.
import os
os.chdir('gdrive/My Drive/data')  #change dir
!ls
!unzip -q iris_data.zip 
df3 = pd.read_csv('/content/gdrive/My Drive/data/iris_data.csv')

Note: Since you have specified that the data is about 30GB, this may not be useful if you are on the free tier provided by Google (as it gives only 15GB per account) you may have to look elsewhere.
You can also visit this particular question for more solutions on Kaggle integration with Google Colab.
"
Origin must be supplied Error in random date generation in R,"Try with origin as guided by the error message:
as.Date(runif(45764, as.Date(""1942-02-01""), as.Date(""2023-10-01"")), 
        origin = ""1970-01-01"")

as.Date(<numeric>) used to require origin, while it was changed in R 4.3 and origin now defaults to ""1970-01-01"", Kaggle is still using R 4.0.5 . Also note that origin has no effect on as.Date(<character>)
When you are working with different R and R package versions, it often makes sense to check the documentation on that exact system, i.e. in this case check ?as.Date.numeric in Kaggle instead of relying on the same on your local system or Google results.
"
upload .nii files to kaggle,"I found the answer, the problem was that .nii files are actually compressed, they were .nii.gz so kaggle thought they were separate directories and not the actual files, so it tried to decompress them but found nothing inside. I solved it by reading them locally and creating a new dataset with .pt files for the images and read them as such.
"
"Cannot download Saved AI Models in Kaggle working dir, and models were lost","This feedback may be useful to new Kaggle notebook users like myself;

To allow internet access in Kaggle notebook (it is disabled by default ) , you must enable via the very small / easy to miss 'arrow ' button bottom left hand side of notebook, this reveals  a sliding menu with option to enable Internet access.

Saving and downloading models and model weights:
The models will not appear in your 'Output' in Notebook interface / overview.
You need to  save to



'/kaggle/working/'

directory, which is accessible again via sliding menu (see above)
These files will not persist by default, you must allow 'File' Persistence via sliding menu (see above), otherwise you may well lose your data like I did.

"
How to read a large file as Pandas dataframe?,"I have had similar problem. With some research, I came to know about Vaex.
You can read about its performance here and here.
Essentially this is what you can try to do:

Read the csv file using Vaex and convert it to a hdf5 file (file format most optimised for Vaex)
vaex_df = vaex.from_csv('../input/subtype-nt/meth_subtype_normal_tumor.csv', convert=True, chunk_size=5_000)


Open the hdf5 file using Vaex. Vaex will do the memory-mapping and thus will not load data into memory.
vaex_df = vaex.open('../input/subtype-nt/meth_subtype_normal_tumor.csv.hdf5')



Now you can perform operations on your Vaex dataframe just like you would be doing with Pandas. It will be blazingly fast and you will certainly notice huge performance gains (lower CPU and memory usage).
You can also try to read your csv file directly into Vaex dataframe without converting it to hdf5. I had read somewhere that Vaex works fastest with hdf5 files therefore I suggested the above approach.
vaex_df = vaex.from_csv('../input/subtype-nt/meth_subtype_normal_tumor.csv.hdf5', chunk_size=5_000)

"
ModuleNotFoundError: No module named &#39;onnxruntime&#39;,"
the error occurs because ""import"" cannot find onnxruntime in any of the paths, check where import is searching and see if onnxruntime is in there.
check what path pip install installs to, that way in the future you won't have the same problem! :)

"
How to load just one chosen file of a way too large Kaggle dataset from Kaggle into Colab,"You could write a script that downloads only certain files or the files one after the other:
import os

os.environ['KAGGLE_USERNAME'] = ""YOUR_USERNAME_HERE""
os.environ['KAGGLE_KEY'] = ""YOUR_TOKEN_HERE""

!kaggle datasets files allen-institute-for-ai/CORD-19-research-challenge

!kaggle datasets download allen-institute-for-ai/CORD-19-research-challenge -f metadata.csv

"
Numpy doesn&#39;t provide an attribute,"I've just back up statmodels library to the version 0.11.0:
pip uninstall statsmodels -y
pip install statsmodels==0.11.0
It seems like default version 0.11.1 has a bug
"
Connection error with &#39;!pip install torchxrayvision&#39; on kaggle,"Just turn the on switch for internet access in your notebook settings and try again;)
"
"Pandas :TypeError: float() argument must be a string or a number, not &#39;pandas._libs.interval.Interval&#39;","Your X_train, y_train, or both, seem to have entries that are not float numbers.
At some point in the code, try using
X_train = X_train.astype(float)
y_train = y_train.astype(float)
X_test = X_test.astype(float)
y_test = y_test.astype(float)

Either this will work and the error will go away, or one of the conversions will fail, at which point you will need to decide how (or if) you want to encode the data as a float.
"
Kaggle Opencv restarts Notebook,"You cannot use cv2.imshow in a Kaggle notebook.  It requires a Qt backend which the Kaggle notebook is not set up for which is why your notebook is crashing.  In addition, cv2.imshow opens up a separate window which of course the notebook environment is also not set up for.  Therefore, you unfortunately cannot use OpenCV windowing or interactive functionality in the notebook.  Since Matplotlib is working for you, you need to use that.
"
TfIdfVectorizer not tokenizing properly,"The regex pattern gets ignored if you pass a custom tokenizer. This is not mentioned in the documentation, but you can see it clearly in the source code here:
https://github.com/scikit-learn/scikit-learn/blob/9e5819aa413ce907134ee5704abba43ad8a61827/sklearn/feature_extraction/text.py#L333
def build_tokenizer(self):
    """"""Return a function that splits a string into a sequence of tokens.
    Returns
    -------
    tokenizer: callable
          A function to split a string into a sequence of tokens.
    """"""
    if self.tokenizer is not None:
        return self.tokenizer
    token_pattern = re.compile(self.token_pattern)
    return token_pattern.findall

If self.tokenizer is not None, you will not do anything with the token pattern.
Solving this is straightforward, just put the regex token pattern in your custom tokenizer, and use this to select tokens.
"
Pandas does not change categorical data [sex] to numerical values [0/1],"Try:
sex = train_dataset['Sex'].replace(['female','male'],[0,1])
print(sex)

It looks like your syntax is off. See the replace function
Output:

"
ggplot geom_bar() fill not coloring bars on plot,"This is doing the trick for me:
ggplot(data = passengers) + 
   geom_bar(mapping = aes(x=Survived, fill = as.character(Pclass)))


"
What is the difference between 0:: and 0: when filtering a numpy array?,"There is no difference, both methods will hook into __getitem__ in the same way.
>>> class Thing(object):
...     def __getitem__(self, item):
...         print(repr(item))
...
>>> t = Thing()
>>> t[0:, 4]
(slice(0, None, None), 4)
>>> t[0::, 4]
(slice(0, None, None), 4)

"
Sci-kit learn pipeline returns indexError: too many indices for array,"It turns out that the Pandas dataframe is the wrong shape.
estimator.fit(train.values, train_labels[0].values)

works, although I also had to drop the penalty term.
"
Why use cross validation?,"Cross-validation is a necessary step in model construction. If cross-validation gives you poor results, there is no sense in even trying it on live data. Your set on which you are training and validating is also live data, isn't it? So, the results should be similar. Without validating your model you don't have any insight into its performance whatsoever. Models which give 100% accuracy on training set could give random results on validation set. 
Let me re-iterate, cross-validation is not a replacement for live data test, it is a part of model construction process.
"
unable to apply learned model to test data in R,"I'm posting an answer to correct a couple points that seem to have gotten confused. There really is no predict-function as such. That is what is meant where the help page says ""predict"" is a ""generic function"". Sometimes generic functions do have a fun.default method, but in the case of predict.*, there is no default method. So dispatch is on the basis of the class of the first argument. There will be separate help pages for each method and the help page for ""predict"" lists several. Package authors need to write their own predict methods for new classes.
Logistic regression predates the machine learning paradigm, so expecting it to ""predict classes"" is somewhat unrealistic. Even the fact that you can get a ""response"" prediction is a gift over what the software would have provided 30 years ago when some of us were taking our regression classes. One needs to understand that probabilities are generally not 0 or 1 but rather something in between. If the user wants to set a threshold and determine how many cases exceed the threshold then that is an analyst decision and the analysts need to make any transformations to categories they deem worthwhile.
Executing: predict(fit, train$Sex) would be expected to give a result that was as long as there were values from the training set, so I'm guessing that you perhaps meant to try predict(fit, test$Sex) and were disappointed. If that's the case then it should have been: predict(fit, list(Sex=test$Sex) ). R needs the argument to be a value that can be coerced to a dataframe, so a named list of values is a minimum requirement for predict-ors.
If predict.glm gets a malformed argument to the second argument, newdata, it falls back on the original data argument and uses the linear predictors that are retained in the model object.
"
How to connect two dataframes with one line Code,"Your statement can be written more succinctly using Named Aggregations:
tmp_df = (
    train_df.groupby(""Parch"")
    .agg(**{""Count"": (""Survived"", ""count""), ""Surviving rate"": (""Survived"", ""mean"")})
    .reset_index()
)

"
"Input 0 of layer dense is incompatible with the layer: expected axis -1 of input shape to have value 784 but received input with shape (None, 14)","Looking at your teacher's python code, the error is telling you that the Dense layer is expecting in input data with 784 features but got a series of data with only 14 features.
By looking at your python code, I'm assuming that you're using the predefined dataset MNIST in the teacher's code which, if I remember correctly, the input data in x_train has a shape of (784,x) where x is an integer number that could be 3,4, etc.
Now to solve this problem, you have to understand how you have to pass the data into your model. Looking again at your teacher's code we have this:
from keras.layers import Dense
from keras.models import Sequential

model=Sequential()
model.add(Dense(32,activation=""relu"",input_shape=(7,)))
model.add(Dense(2,activation=""softmax""))

So we have a Sequential model that has two layers:

The first Dense node, that works also as the Input Layer for the model, is made of 32 units with the Rectified Linear Unit as the activation function and, the most important thing, we know that the Input shape of this model is written as a tuple of (7,). So what is this value (7,)? Basically you're telling to the model that your input data has 7 features so each single data must have 7 values within it.
The second Dense node is simply the output layer which gives in output a probability answer between two values. This is less important since your problem is passing the input data.

Now to understand how the data is handled in Keras/Tensorflow you have to think the data represented as a matrix where in the rows you have the values of the data while the columns indicates the meaning of the values. Kinda like Excel/CSV files where there is a header which explains the meaning of values in each column.
If you ever used NumPy, more specifically dimensional arrays, basically the shape of the data is the most important thing you should check before giving the data to your model because the shape tells to you how the data is handled and in how many dimensions is the data represented. If your data has, for example, a shape of (784,14) the data is represented as a bidimensional (2D) matrix where the first value (784) are the rows of the matrix while the second value (14) are the columns.
So to answer your question, the python program is expecting in input a series of data of 784 features, which to me is translated to 784 columns, but instead you have given in input a series of data with only 14 features. So to solve this problem you have to check the shape of your data, which you can simply do with x_train.shape and see in output what you get. Next you have to pass correctly the data to the model and then your program will work for sure. One way could be reshaping the data.
For a better understanding of how Tensorflow handles the shape of the data look at this guide.
Another thing I suggest to you is to write your question in a more appropriate way. My suggestion is to write a question in the following way:

Title of the problem.
Description of the problem.
Show the error you get.
Write what have you tried, add also references to other similar questions from yours.
ALWAYS show at least one piece of code so people can analyze it. More pieces of codes are also acceptable.
Optionally, write what is the expected result you want.

If you write the questions in this way surely people will understand better your problem.
I hope this helps understand better your problem.
"
UnimplementedError: File system scheme &#39;[local]&#39; not implemented,"As suggested by @Allen Wang, the solution is to use train_paths instead of train to pass images.
This is what I have changes to make it work
    train_image=tf.data.Dataset.from_tensor_slices((train_paths,train.iloc[:,1::] ))
    train_dataset=train_image.map(decode_image, num_parallel_calls=AUTO).repeat().shuffle(512).batch(BATCH_SIZE).prefetch(AUTO)
    test_image=tf.data.Dataset.from_tensor_slices((test_paths))
    test_dataset=test_image.map(decode_image, num_parallel_calls=AUTO).batch(BATCH_SIZE)

"
"How can my Python Tensorflow program print what device { CPU, GPU, TPU } it is using?","There are many ways to answer your question. The simplest is to check what devices are available to you:
with tf.Session() as sess:
  devices = sess.list_devices()

If you want to know some more details about each of the devices you can run tf.test.gpu_device_name to get the name of GPU device (or any other allocated).
"
How to save csv file in python with datatype of one particular columns has to be int32?,"when you read the csv, you can set the argument to int32
data = pd.read_csv(..., dtype={'Id': np.int32})

"
unable to see bubbles in bubble map of plotly/python,"The complete scatter is not visible because of the default setting for the locatioinmode parameter of the scatter_geo function. It can be changed to country name when the locations are country names. The modified code is as follows:
import pandas as pd
import numpy as np
import plotly.express as px
import pycountry

df = pd.read_csv('ramen-ratings.csv')
country_count = df['Country'].value_counts()
country_count = country_count.reset_index().rename(columns={'index':'Country', 'Country':'Count'})
fig = px.scatter_geo(country_count, locations='Country', locationmode='country names', size='Count')
fig.show()

The graph obtained is as follows:

The complete documentation can be accessed here: https://plotly.com/python-api-reference/generated/plotly.express.scatter_geo.html
"
How to find memory usage of Kaggle notebook?,"Use this code to track your memory usage of the notebook:
import os, psutil  

def cpu_stats():
    pid = os.getpid()
    py = psutil.Process(pid)
    memory_use = py.memory_info()[0] / 2. ** 30
    return 'memory GB:' + str(np.round(memory_use, 2))

"
Downloading Kaggle files in Python,"See the documentation here: https://technowhisp.com/kaggle-api-python-documentation/

5.2 Listing dataset files
api.dataset_list_files('avenn98/world-of-warcraft-demographics').files

"
SyntaxError while trying to perform RobustScaler on Pandas Dataframe,"I managed to get it to work. Not sure how Pythonic this solution is, but it got me back on track:
df_train[list(df_train.select_dtypes(exclude=['object']).columns)] = RobustScaler().fit_transform(df_train[list(df_train.select_dtypes(exclude=['object']).columns)])

"
xgboost error predictions R,"If you look at this page (https://rdrr.io/cran/xgboost/src/R/xgb.Booster.R), you will see that some R users are likely to get the following error message: ""Feature names stored in object and newdata are different!"". 
Here is the code from this page related to the error message:
predict.xgb.Booster <- function(object, newdata, missing = NA, outputmargin = FALSE, ntreelimit = NULL,predleaf = FALSE, predcontrib = FALSE, approxcontrib = FALSE, predinteraction = FALSE,reshape = FALSE, ...)

object <- xgb.Booster.complete(object, saveraw = FALSE)
      if (!inherits(newdata, ""xgb.DMatrix""))
        newdata <- xgb.DMatrix(newdata, missing = missing)
      if (!is.null(object[[""feature_names""]]) &&
          !is.null(colnames(newdata)) &&
          !identical(object[[""feature_names""]], colnames(newdata)))
        stop(""Feature names stored in `object` and `newdata` are different!"")

identical(object[[""feature_names""]], colnames(newdata)) => If the column names of object (i.e. your model based on your training set) are not identical to the column names of newdata (i.e. your test set), you will get the error message.
For more details:
train_matrix <- xgb.DMatrix(as.matrix(training %>% select(-target)), label = training$target, missing = NaN)
object <- xgb.train(data=train_matrix, params=..., nthread=2, nrounds=..., prediction = T)
newdata <- xgb.DMatrix(as.matrix(test %>% select(-target)), missing = NaN)

While setting by yourself object and newdata with your data thanks to the code above, you can probably fix this issue by looking at the differences between object[[""feature_names""]] and colnames(newdata). Probably some columns that don't appear in the same order or something.
"
Order Dataframe Index based on a second Dataframe,"You can also sort values by CodeNumber on each dataframe by calling .sort_values(by = 'CodeNumber') before setting them as index:
d3 = {'CodeNumber': [1234, 1235, 111, 101], 'Date': [20150808, 20141201, 20180119, 20120720], 'Weight': [26, 32, 41, 24]}
d4 = {'CodeNumber': [1235, 1234, 101, 111], 'Date': [20160808, 20151201, 20180219, 20130720], 'Weight': [28, 25, 47, 3]}

data_SKU3 = pd.DataFrame(data=d3).sort_values(by = 'CodeNumber')
data_SKU4 = pd.DataFrame(data=d4).sort_values(by = 'CodeNumber')

data_SKU3.set_index('CodeNumber', inplace = True)
data_SKU4.set_index('CodeNumber', inplace = True)

"
Adding Linux ODBC drivers for SQL Server to a Kaggle/Python docker image,"I'm not sure I have exactly the answer you're looking before because you make reference to the Microsoft ODBC driver- however I will share what I've done to get UnixODBC installed (which I then use with the PyODBC module to talk to an MSSQL database).
Here is a summary of the commands I've used (caveat: untested in this format as they're cut and paste from a production Dockerfile that I can't share in it's entirety-):
Assuming you are inside a Debian/Ubuntu-based Linux Docker container
$ apt-get update
$ apt-get install python2.7 python-pip unixodbc unixodbc-dev freetds-bin freetds-dev tdsodbc nano

# nano is a text editor; Ctrl + O to write out, Ctrl + X to exit

$ nano /etc/odbcinst.ini

[FreeTDS]
Description = FreeTDS Driver for MSSQL
Driver = /usr/lib/x86_64-linux-gnu/odbc/libtdsodbc.so
Setup = /usr/lib/x86_64-linux-gnu/odbc/libtdsS.so

$ nano /etc/freetds/freetds.conf

[global]
#   $Id: freetds.conf,v 1.12 2007-12-25 06:02:36 jklowden Exp $
#
# This file is installed by FreeTDS if no file by the same
# name is found in the installation directory.
#
# For information about the layout of this file and its settings,
# see the freetds.conf manpage ""man freetds.conf"".

# Global settings are overridden by those in a database
# server specific section
[global]
# TDS protocol version
tds version = 8.0

# Whether to write a TDSDUMP file for diagnostic purposes
# (setting this to /tmp is insecure on a multi-user system)
;dump file = /tmp/freetds.log
;debug flags = 0xffff

# Command and connection timeouts
;timeout = 10
;connect timeout = 10

# If you get out-of-memory errors, it may mean that your client
# is trying to allocate a huge buffer for a TEXT field.
# Try setting 'text size' to a more reasonable limit
text size = 64512

# If you experience TLS handshake errors and are using openssl,
# try adjusting the cipher list (don't surround in double or single quotes)
# openssl ciphers = HIGH:!SSLv2:!aNULL:-DH

$ pip install pyodbc

$ nano test_database.py

import pyodbc

host = 'some.host.org'
port = 1433
username = 'some_username'
password = 'some_password'

conn_str = """"""
            DRIVER={FreeTDS};
            TDS_VERSION=8.0;
            SERVER=%s;
            Port=%i;
            DATABASE=%s;
            UID=%s;
            PWD=%s;
        """""" % (
    host, port, database, username, password
)

conn_str = conn_str.replace('\n', '').replace(' ', '')

connection = pyodbc.connect(conn_str, timeout=self._timeout)

connection.autocommit = True

cursor = connection.cursor()

cursor.execute('SELECT * FROM some_table;')

rows = cursor.fetchall()

for row in rows:
    print row

cursor.close()

connection.close()

(the code block above is scrollable)
I must caveat again this is all cut and paste from our codebase and I haven't tested it as it appears right now.
If you don't have access to an MSSQL database for testing, you can spin one up in as a Docker container (SQL Server Express Edition) with the following command:
docker run -d --name mssql-server -p 1433:1433 -e 'ACCEPT_EULA=Y' -e 'SA_PASSWORD=some_password' -e 'MSSQL_PID=Express' microsoft/mssql-server-linux:latest

Also please note as part of the packages installed above you'll have the ""tsql"" command line tool which can be used to double check your database stuff outside of Python.
Best of luck!
"
Kaggle output path has invalid characters,"After importing the notebook block by block I found the problem.
In order to have an interactive index I used Markdown like so :
In the index :
- [Title...](#id-section0)

Further down :
<div id='id-section0'/>
# Title...

It worked fine on my local machine but not when Kaggle was rendering HTML. After deleting those headers, it worked fine
"
Pandas apply in 2 columns and substitute them in one line,"You can simply use apply for the selected columns with astype(str) i.e
submission_df[['question1','question2']]=submission_df[['question1','question2']].astype(str).apply(lambda row: [nltk.word_tokenize(row['question1']),nltk.word_tokenize(row['question2'])], axis=1)

Example :
import nltk
df = pd.DataFrame({""A"":[""Nice to meet you "",""Nice to meet you "",""Nice to meet you "",8,9,10],""B"":[7,6,7,""Nice to meet you "",""Nice to meet you "",""Nice to meet you ""]})
df[['A','B']] = df[['A','B']].astype(str).apply(lambda row: [nltk.word_tokenize(row['A']),nltk.word_tokenize(row['B'])], axis=1)

Output:

                          A                      B
0  [Nice, to, meet, you]                    [7]
1  [Nice, to, meet, you]                    [6]
2  [Nice, to, meet, you]                    [7]
3                    [8]  [Nice, to, meet, you]
4                    [9]  [Nice, to, meet, you]
5                   [10]  [Nice, to, meet, you]

"
Queue.dequeue hanging in Tensorflow input pipeline,"The standard MNIST images have 28 x 28 = 784 pixels. Assuming that each column in the CSV file is a different pixel, plus one column for the label, the following line in your code:
for column in range(749):

…should be:
for column in range(785):

"
Sklearn TruncatedSVD() ValueError: n_components must be &lt; n_features,"It is highly likely that the shape of your data matrix is wrong: It seems to have only one column. That needs to be fixed. Use a debugger to figure out what goes into the fit method of the TruncatedSVD, or unravel the pipeline and do the steps by hand.
As for the error message, if it is due to a matrix with one column, this makes sense: You can only have maximally as many components as features. Since you are using TruncatedSVD it additionally assumes that you don't want the full feature space, hence the strict inequality.
"
Tensorflow multi-class ML model issues,"If you just want something up and running quickly for Kaggle competition, I would suggest you trying out examples in TFLearn first. There's embedding_ops for one-hot, examples for early stopping, custom decay, and more importantly, the multi-class classification/regression you are encountering. Once you are more familiar with TensorFlow it would be fairly easy for you to insert TensorFlow code to build a custom model you want (there are also examples for this).  
"
use caffe to train Lenet with CSV data,"I assume you have one hdf5 data file 'data/mnist_train_h5.hd5'. 

As you can see from the error message you got, ""HDF5Data"" layer does not support data transformation. Specifically, you cannot scale the data by the layer.
Thus, any transformations you wish to have, you must apply them yourself during the creation of 'data/mnist_train_h5.hd5'.
""HDF5Data"" layer does not accept data_param, but rather hdf5_data_param with a parameter source specifying a list of hd5 binary files. In your case you should prepare an extra text file 'data/mnist_train_h5.txt' with a single line:


data/mnist_train_h5.hd5

This text file will tell caffe to read 'data/mnist_train_h5.hd5'.
The resulting layer should look like:
layer {
  name: ""mnist""
  type: ""HDF5Data""
  top: ""data""
  top: ""label""
  hdf5_data_param {
    source: ""data/mnist_train_h5.txt""
    batch_size: 64
  }
  include {
    phase: TRAIN
  }
}

"
Stanford NLP Parser gives different result (sentiment) for same statement used in Kaggle Movie review,"I assume you are using Bag of Words and the comma and the dot are one of your features (a column in your X matrix). 
+-------------------------+-----------+-----------+----+
|    Document/Features    | Genuinely | unnerving | .  |
+-------------------------+-----------+-----------+----+
|  Genuinely unnerving .  |         1 |         1 | 1  |
|  Genuinely unnerving    |         1 |         1 | 0  |
+-------------------------+-----------+-----------+----+

An ideal algorithm should learn wether these features are relevant or not. For example in the case of Logistic Regression your algorithm would assign a really small weight to the corresponding column, and therefore a 1 or a 0 in that column won't change the outcome of the prediction. So you would have something like:
""Genuinely unnerving ."" -> 0.5*1 + -2.3*1 + 0.000001*1 -> Negative
""Genuinely unnerving  "" -> 0.5*1 + -2.3*1 + 0.000001*0 -> Also negative

In your case it looks like they are having some small effect. Is this really a problem? You have found some special cases were it seems wrong but by looking at the data the algorithm found that sentences with a dot are more negative that sentences without one. May be you should trust that statistically speaking, a dot can change the meaning of a sentence.
It can also happen that you have bad training data or a bad overfitting model. If you really think something is wrong, then you can impose this knowledge on the model by representing the sentences so that they are indistinguishable, for example by ignoring some punctuation.
I think it would be wrong to leave all punctuations out at once, for example a ! could be representative of very positive sentiment when accompanied by the word yes, if you strip it from your sentence you would be hiding the model valuable information. But may be it is just the opposite and ! is negative in most cases, so it gets a high negative weight after training, which confuses the model when predicting yes!!! like sentences. In this case you could represent your sentence as bigrams, so that the model can separately weight the effect of a single ! and a (yes, !) combination.
So in resume, you should try different models and ways to represent your data and see what works.
"
how to upload a folder on Kaggle Notebook,"Just drag and drop the folder like this

source
"
NotFittedError - Titanic Project Kaggle,"As @Blackgaurd pointed out just change model.fit = (X, y) to model.fit(X, y)
Your current code overwrites the fit method of your Random Forest Classifier.
Full code of yours with correction:
from sklearn.ensemble import RandomForestClassifier

y = train_data[""Survived""]

features = [""Pclass"", ""Sex"", ""SibSp"", ""Parch""]
X = pd.get_dummies(train_data[features])
X_test = pd.get_dummies(test_data[features])

model = RandomForestClassifier(n_estimators = 100, max_depth = 5, random_state = 1)
model.fit(X, y) # <- line of code fixed
predictions = model.predict(X_test)

output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})
output.to_csv('submission.csv', index = False)
print('Your submission was successfully saved!')

"
why import cugraph not working in kaggle?,"For future reader's reference, the full answer is detailed here:
https://github.com/rapidsai/cugraph/issues/1972#issuecomment-982925312
User needs to specify RPAIDS version, python version and CUDA toolkit Version.  The working conda install command for Kaggle, as of writing, is:
!conda install -y -c rapidsai -c nvidia -c conda-forge cugraph=21.08 python=3.7 cudatoolkit=11.0

"
"Error in parse(text = x, srcfile = src): &lt;text&gt;:13:6: unexpected symbol 12: 13: This R ^","Click 'Save Version' (top right of screen)
That will run (i.e. 'knit') your RMarkdown code (it may take a moment). A link to the knitted document pop up in the bottom left of screen. It's that easy!

"
"trainer.train() in Kaggle: StdinNotImplementedError: getpass was called, but this frontend does not support input requests","You may want to try adding report_to=""tensorboard"" or any other reasonable string array in your TrainingArguments
https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments
If you have multiple logger that you want to use report_to=""all"" (the default value)
try os.environ[""WANDB_DISABLED""] = ""true"" such that wandb is always disabled.
see: https://huggingface.co/transformers/main_classes/trainer.html#transformers.TFTrainer.setup_wandb
"
How to add a column with string values of a dictionary in Python dataframe,"Does this help?
country_dct = {}
for key, countries in eu_regions.items():
    
    for country in countries:
        country_dct[country] = key


eu_vaccine_df['region'] = eu_vaccine_df['country'].map(country_dct)

eu_vaccine_df.head()

"
Error installing tensorflow-io kaggle notebook,"I think it's only a warning. I got the same somedays back when I did
!pip install tensorflow-io
It got installed successfully and I was able to train also.
"
Kaggle TPU: failed to connect to all addresses,"You have to create your model and optimizer within strategy scope:
with strategy.scope():
  model = create_model()
  model.compile(optimizer='adam',
                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                metrics=['sparse_categorical_accuracy'])

"
Download a Kaggle Dataset,"Maybe this post helps: https://www.kaggle.com/general/51898
it links to this script:
# Info on how to get your api key (kaggle.json) here: https://github.com/Kaggle/kaggle-api#api-credentials
!pip install kaggle
api_token = {""username"":""USERNAME"",""key"":""API_KEY""}
import json
import zipfile
import os
with open('/content/.kaggle/kaggle.json', 'w') as file:
    json.dump(api_token, file)
!chmod 600 /content/.kaggle/kaggle.json
!kaggle config path -p /content
!kaggle competitions download -c jigsaw-toxic-comment-classification-challenge
os.chdir('/content/competitions/jigsaw-toxic-comment-classification-challenge')
for file in os.listdir():
    zip_ref = zipfile.ZipFile(file, 'r')
    zip_ref.extractall()
    zip_ref.close()

from: https://gist.github.com/jayspeidell/d10b84b8d3da52df723beacc5b15cb27
"
How to run Tensorflow&#39;s Keras model.fit() function on GPU in Kaggle Notebook?,"I finally got it to work. There was some unknown bug in tensorflow. It is working properly in tf-nightly build.
"
Kaggle: How to download all files from working directory,"Have a look at Kaggle's API docs describing how to get a kernel's output.
You're probably looking to use something like this command:
kaggle kernels output mrisdal/exploring-survival-on-the-titanic -p /path/to/dest.
"
"In a Kaggle kernel while having selected the GPU option when checking torch.cuda.is_available(), it says is not available","Start the kernel and turn-on the GPU in settings option, on the right-hand side control panel. Then run following commands.
    [1]:import torch
    [2]:torch.cuda.device_count()
    Out[2]:1

    [3]:torch.cuda.is_available()
    Out[3]:True

"
pd.DataFrame from an array in 2 different ways,"submission2=pd.DataFrame(data=Ynew, index=col, columns=labels)

"
How to calculate Normalized Gini Coefficient in tensorflow,"Here's a tensorflow version (uses tf.nn.top_k instead of np.lexsort for sorting).
def gini_tf(actual, pred):
  assert (len(actual) == len(pred))
  n = int(actual.get_shape()[-1])
  indices = tf.reverse(tf.nn.top_k(pred, k=n)[1], axis=0)
  a_s = tf.gather(actual, indices)
  a_c = tf.cumsum(a_s)
  giniSum = tf.reduce_sum(a_c) / tf.reduce_sum(a_s)
  giniSum -= (n + 1) / 2.
  return giniSum / n

gini_normalized doesn't change. By the way, looks like your version ignores cmpcol and sortcol arguments.
"
neural network from scratch in python using sigmoid activation,"In Python range generates a tuple of numbers from x to y with range(x, y). If you generate something like range(10) then it is the same as (0, 1, 2, 3, 4, 5, 6, 7, 8, 9). Lists in Python need an integer index such as list[0] or list[4], not list[0, 4], however, there is a built-in thing in Python that allows access from index x to index y in a list here is the syntax: list[0:4]. This will return every value from 0 to 3 in the list. Such as if a list is list = [0,10,3,4,12,5,3] then list[0:4] will return [0,10,3,4]. 
Try taking a look at list data structures in Python on the Python Docs. As well as Understanding Generators in Python.
I think what your looking for is something like this: delta3 = [[z-1 for z in delta3[x:y]] for x in range(m1)]. This list comprehension uses two generations both, [x-1 for x in l], which subtracts one from every element in the list, and [l[x:y] for x in range(m)], which generates a list of lists with values through x to y in a range of m. Though I'm not sure I understand what your end goal is, fully.
"
Pandas unable to access a json file in my Documents directory,"Make sure that you have read permission on that file. You can just check it out by viewing the properties of that file. Other reason may be, due to other process holding write lock on that file. You can check it out by using Process-Explorer tool.
"
Use different colors in scatterplot for Iris dataset,"Seaborn provides an interface to data that is organized in DataFrames. If you want to use seaborn it would make sense to keep your data in a DataFrame, possibly adding the column you want to plot. 
import matplotlib.pyplot as plt
import seaborn as sns
iris = sns.load_dataset(""iris"")
iris[""ID""] = iris.index
iris[""ratio""] = iris[""sepal_length""]/iris[""sepal_width""]

sns.lmplot(x=""ID"", y=""ratio"", data=iris, hue=""species"", fit_reg=False, legend=False)

plt.legend()
plt.show()


The same can be achieved with a usual matplotlib scatter plot like so:
import matplotlib.pyplot as plt
import seaborn as sns
iris = sns.load_dataset(""iris"")

ratio = iris[""sepal_length""]/iris[""sepal_width""]

for name, group in iris.groupby(""species""):
    plt.scatter(group.index, ratio[group.index], label=name)

plt.legend()
plt.show()

"
Why does adding features to linear regression decrease accuracy?,"If you're talking about training error for a linear regression classifier, then adding features will always decrease your error unless you have a bug.  Like you say, it's a convex problem and the global solution can never be worse as you can just set the weight to zero.
If you're talking about test error however, then overfitting is going to be the big issue with adding features, and is certainly something you would observe.  
"
Can sigmoid function graph be linear?,"No, it can't be linear. I don't have your complete code, but try
 x = np.linspace(-3, 3)
 y = sigmoid(x)
 plt.plot(x, y)

to see the shape
"
Python Pandas in colab:UnicodeDecodeError: &#39;utf-8&#39; codec can&#39;t decode byte 0xd3 in position 0: invalid continuation byteUnicodeDecodeError:,"Try encoding option in read_csv like below.
read_csv('file', encoding = ""utf-8"")

or
read_csv('file', encoding = ""ISO-8859-1"")

"
What is &#39;mean&#39; f1 score in machine learning?,"The F1 score is a measure for the test accuracy of a binary classification task. In multi-label classification tasks, each document has a F1 score. Therefore, the mean F1 Score is: 

Where N is the row's size of the train set
"
How to scrape infinite scroll page of Kaggle dataset in Python?,"If you inspect the browser, you can see that everytime you scroll down an AJAX request is being made in the networks tab.
The request is being made to:  
https://www.kaggle.com/datasets.json?sortBy=hottest&group=all&page=2

Which returns the results in json format. You can continue incirmenting page untill you reach max results. The json file has key u'totalDatasetListItems': 770 and returns 20 results per search, so you can use that info to develop a loop.
This example is for python3 and shows how to get concurrent requests running with this sort of pagination ssytem.
import scrapy
import json
from w3lib.url import add_or_replace_parameter
class MySpider(scrapy.Spider):
    name = 'myspider'
    start_urls = ['https://www.kaggle.com/datasets.json?sortBy=hottest&group=all&page=1']

    def parse(self, response):
        data = json.loads(response.body) 
        total_results = data['totalDatasetListItems']
        page = 1
        # figure out how many pages are there and loop through them.
        for i in range(20, total_results, 20):  # step 20 since we have 20 results per page
            url = add_or_replace_parameter(response.url, 'page', page)
            yield scrapy.Request(url, self.parse_page)

        # don't forget to parse first page as well!
        yield from self.parse_page(self, response)

    def parse_page(self, response):
        data = json.loads(response.body) 
        # parse page data here
        for item in data['datasetListItems']:
            item = dict()
            yield item

"
Error: unsupported operand type(s) for /: &#39;str&#39; and &#39;long&#39;,"Based on the answer of @batmac. Adding kind=""count"" to the command, solved the issue. 
Example:
import pandas as pd 
import seaborn as sns

data = pd.read_csv('train.csv')
sns.factorplot('Sex', data=data, kind=""count"")

"
Evaluating estimator performance in scikit-learn,"This probably means that there is a significant discrepancy between the distribution of the final evaluation data and the development set.
It would be interesting to measure the over-fitting of your decision trees though: what is the difference between the training score clf.score(X_train, y_train) and the testing score clf.score(X_test, y_test) on your split?
Also pure decision trees should be considered a toy classifier. They have very poor generalization properties (and can overfit a lot). You should really try ExtraTreesClassifier with increasing numbers for n_estimators. Start with n_estimators=10, then 50, 100, 500, 1000 if the dataset is small enough.
"
how to convert all the column headers in a data frame (written in python pandas ) to uppercase,"I recommend to use this code:
df.columns = df.columns.str.upper()

Read more about this method here:
https://pandas.pydata.org/docs/reference/api/pandas.Series.str.upper.html
"
Multi-label classification implementation,"The loss function to be used is indeed the binary_crossentropy with a sigmoid activation.
The categorical_crossentropy is not suitable for multi-label problems, because in case of the multi-label problems, the labels are not mutually exclusive. Repeat the last sentence: the labels are not mutually exclusive.
This means that the presence of a label in the form [1,0,1,0,0,0] is correct. The categorical_crossentropy and softmax will always tend to favour one specific class, but this is not the case; just like you saw, a comment can be both toxic and obscene.
Now imagine photos with cats and dogs inside them. What happens if we have 2 dogs and 2 cats inside a photo? Is it a dog picture or a cat picture? It is actually a ""both"" picture! We definitely need a way to specify that multiple labels are pertained/related to a photo/label.
The rationale for using the binary_crossentropy and sigmoid for multi-label classification resides in the mathematical properties, in that each output needs to be treated as in independent Bernoulli distribution.
Therefore, the only correct solution is BCE + 'sigmoid'.
"
Downloading an entire git repo to google colab,"Try %cd instead of !cd. Commands executed with ! execute in a subshell, so you won't actually change the working directory for subsequent shell commands if you execute cd using !.
Here's an example:

"
kaggle kernel can&#39;t install python package,"The problem is I not enable the kaggle kernel internet.
so the detail solution is:
kernel settings > internet > on
and it works!
"
Is it a good practice to reduce a dataset to have a better PCA decomposition,"It makes sense, definitely.
The technique you are using is commonly known as Random Undersampling, and in ML it is useful in general when you are dealing with imbalanced data problems (such as the one you are describing). You can see more about it this Wikipedia page.
There are, of course, many other methods to dealt with class imbalance, but the beauty of this one is that it is quite simple and, sometimes, really effective. 
"
Cleaning and preparind data for Spark,"Rather than RDDs, you should consider using DataSets both for performance and ease of use--particularly if you are new to Scala. Taking the DataSet approach, you can do this:
val titanicDs = sparkSession.read
    .option(""header"", true)
    .csv(""titanic.csv"")
    .na
    .drop
    .withColumn(""TicketSplit"", split($""Ticket"", "" ""))
    .withColumn(""Ticket"", when(size($""TicketSplit"") === ""2"", $""TicketSplit"".getItem(1)).otherwise($""TicketSplit"".getItem(0)))
    .drop(""TicketSplit"")

There is a lot going on here:

Set the header option to true so Spark realizes the first row is a header imposing structure on the data and uses those column names in the DataFrame.
The na method returns a DataFrameNaFunctions object that is very helpful for working with missing data. In this case, the combination of na.drop eliminates all rows containing any data that are null.
I add a new temporary column called TicketSplit where I use the wonderful functions library to split the raw Ticket data on the space character into an array of either length 1 (if there is only a number) or 2 (if there is text followed by a space and number). 
I use when and otherwise from the functions library to modify the raw Ticket column according to the size of the array in the TicketSplit column. Whatever the size of the array in the TicketSplit column, ultimately only the number is preserved by either getting the first element of a 1-element array at index 0 or the second element of a 2-element array at index 1.
Drop that TicketSplit column because it has served its purpose.
Enjoy a nice cold drink.

"
If condition not working with Titanic Kaggle dataset,"That happens because for every iteration of your loop, you are setting the entire 'person' column to be equal whatever is relevant; it then happens that for the final iteration, the second clause is the one that comes into play.
You will probably want to use something like DataFrame.apply. For example, in your case, you could do something like
In [1]: import pandas as pd
   ...: 
   ...: df = pd.DataFrame()
   ...: df['Sex'] = ['Male', 'Female', 'Male']
   ...: df['Age'] = [15, 20, 50]
   ...: df
   ...: 
Out[1]: 
      Sex  Age
0    Male   15
1  Female   20
2    Male   50

In [2]: df['Person'] = df.apply(lambda x: 'Child' if x['Age'] < 16 else x['Sex'], axis=1)

In [3]: df
Out[3]: 
      Sex  Age  Person
0    Male   15   Child
1  Female   20  Female
2    Male   50    Male

In general, you very rarely end up needing to manually loop through your Series/DataFrames.
Edit: Note also that for large DataFrames, the above code is greatly outperformed by @piRSquared's solution:
In [41]: n = 10**5

In [42]: df = pd.DataFrame()

In [43]: df['Sex'] = np.random.choice(['Male', 'Female'], size=n)

In [44]: df['Age'] = np.random.randint(1, 100, size=n)

In [46]: df.head(10)
Out[46]: 
      Sex  Age
0  Female   15
1  Female   91
2  Female   50
3  Female   11
4  Female   59
5  Female   40
6  Female   50
7    Male   28
8    Male   13
9  Female   27

In [47]: %timeit np.where(df.Age.values < 16, 'Child', df.Sex.values)
100 loops, best of 3: 3.06 ms per loop

In [48]: %timeit df.apply(lambda x: 'Child' if x['Age'] < 16 else x['Sex'], axis=1)
1 loop, best of 3: 5.73 s per loop

"
"line 596, in call_cpp_shape_fn raise ValueError(err.message) ValueError: Negative dimension size caused by subtracting 3 from 1","Your problem lies here:
input_shape=(1, img_rows, img_cols)

This input shape is in theano format, in Tensorflow the channels dimension goes at the end. You need to change the input shape to:
input_shape=(img_rows, img_cols, 1)

And also make sure that the input data (train and validation) are in the same format, with the channels dimension at the end of the shape tuple.
"
About fork notebook on Kaggle,"""Forking"" a notebook means making a copy of it as it currently is. When you make edits on a forked copy, they're saved in that copy but don't affect the notebook you forked from. The term comes from version control. 
As a note, you can edit your own notebooks, but you can only fork notebooks you aren't a collaborator on. In those cases, the ""Fork Notebook"" button is blue. 
Hope that helps! :)
"
Graphlab : replacing values in Sframe and filtering,"You can use alternative code as below:
Just create a new column 'Pclass_' in the original Sframe,then you can do:
df['Pclass_'] = [1 if item == 38 else 2 if item == 30 else 3 if item == 26 else 4 for item in df['Age']]

You can use any kind of (if-else-if) conditions in the list.
"
Plotly in Juypter Notebook On Kaggle,"Your code looks like OK, here you are your code result.

First have you install plotly?
pip install plotly

Second try like this;
import plotly.plotly as py
import plotly.graph_objs as go
from plotly import tools
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot
init_notebook_mode(connected=True) 
# Graph
iplot([{""x"": [1, 2, 3], ""y"": [3, 1, 6]}])

"
How to fill null value in object attributes in feature engineering?,"First, it depends if your model can manages NA (like xgboost).
Second, are the dropouts explanatory of a behavior (like a depressed man is more likely to skip a task)
There is a whole literature about this questions. The main ways to do are:

Just drop the rows
Fill the missing data with replacements (the median, the most seen value...)
Fill the missing data and add some error to it

So here, you can either leave it NA and use xgboost, drop the uncomplete rows or put the most frequent value between male and female
A few recommendations if you wan to go further :

Try to understand why the datas are missing
Perform sensitivity analysis of the solution you chose

"
Rename several unnamed columns in a pandas dataframe,"The problem isn't really that you need to rename the columns.
What do the first few rows of the .csv file that you're importing look at, because you're not importing it properly. Pandas isn't recognising that JobTitle and Year are meant to be column headers. Pandas read_csv() is very flexible with what it will let you do.
If you import the data properly, you won't need to reindex, or relabel. 
"
"Pandas &#39;read_csv&#39; giving an error, in one specific directory only","Write:
import matplotlib.pyplot as plt

By reimporting as pd you overwrite import pandas as pd
"
What does this mean? function()!(),"This function returns a logical vector indicating if the left operand is not a match for any element in the right operand. 
%in% is  a shortcut for the match function and this function is a further shortcut. Instead of writing
!(a%in%b)

we can define this function as such and then write:
a%!in%b

Definition for !:
?Logic

! indicates logical
  negation (NOT).

"
Lookup from a pandas dataframe given a multiindex,"Here is a way that is almost the same:
>>> f.groupby([""A"", ""B""])[""Y""].apply(list).ix[idx]
A  B
0  0       NaN
   1       [1]
1  0       [0]
   1    [1, 0]
dtype: object

The only difference is that this gives NaN instead of an empty list in the case where there is no match.  Unfortunately you cannot use fillna to replace the NaN with an empty list due to this issue.  However, you can drop it with dropna, and in many cases you won't really need the empty item for the cases with no match anyway.
"
Matplotlib xy line graph legend,"How about doing this?
x = np.linspace(0, 3*np.pi, 500)
fig, ax = plt.subplots()
a = ax.plot(x, np.sin(x**2), label = 'square')
b = ax.plot(x, np.sin(x**0.5), label = 'square root')
handles, labels = ax.get_legend_handles_labels()
ax.legend(handles, labels)

To get this:

"
ValueError: negative dimensions are not allowed in scikit linear regression CV model with sparse matrices,"It looks like this problem occurs without using sklearn. Its in scipy.sparse matrix multiplication. There is this issue on a scipy-users board: sparse matrix multiplication problem. The crux of the problem is that scipy uses a 32-bit int for non-zero indices during sparse matrix multiplication. That's the marked line at the bottom of the traceback above. That can overflow if there are too many non-zero elements. That overflow causes the variable nnz to become negative. Then the code at the last arrow creates an empty array of size nnz, resulting in a ValueError due to a negative dimension.  
You can generate the tail end of the traceback above without sklearn as follows:
import scipy.sparse as ss
X = ss.rand(75000, 42000, format='csr', density=0.01)
X * X.T

For this problem, the input is probably quite sparse, but RidgeCV looks like its multiplying X and X.T in the last part of the traceback within sklearn. That product might not be sparse enough.
"
How can I use Multiple GPU&#39;s During Model training on Kaggle,"To run on multiple GPU:s you must adopt the training to run distributed training.
Pytorch has documentation regarding the area here:
https://pytorch.org/tutorials/distributed/home.html
From there you can find what case fits you the best.
I also found another source which goes into the distributed training area:
https://saturncloud.io/blog/how-to-use-multiple-gpus-in-pytorch/
There they talk about three different methods, where the first (Data parallelism) is the most common for simpler and smaller models, as it is the easiest to adapt to.
"
ERROR: Cannot install tflite-model-maker (The conflict is caused by other modules),"This is a python version issue. numba==0.53 only exists for python versions <3.9, so you will need to change the python verison in your kaggle notebook by executing these commands in your notebook:
!conda create -n newCondaEnvironment -c conda-forge python=3.9 -y

!sudo rm /opt/conda/bin/python3.10
!sudo ln -sf /opt/conda/envs/newCondaEnvironment/bin/python3 /opt/conda/bin/python3.10


!sudo rm /opt/conda/bin/python
!sudo ln -s /opt/conda/envs/newCondaEnvironment/bin/python3 /opt/conda/bin/python

"
!source venv/bin/activate not working in a Kaggle notebook any ideas?,"Each ! command is executed in a subshell which exits immediately.
For example, !cd doesn't work, we have to use %cd. (docs)
So !source activate & !pip freeze are executed in different env.
Try !source activate && pip freeze.
Not very related, there's another pitfall: venv is hard to relocate.
The path is hard-coded in the activate file.
And it fails silently when the path doesn't exist.
"
Save Notebooks to GitHub,"Saving (a Jupyter notebook from) Kaggle to Github will only work if you have first created at least one repository in Github.

TLDR
Conversely, even if you have ""linked"" Kaggle to Github, but have not yet created any repository in the latter, the corresponding ""Save"" button in Kaggle will simply be grayed-out:
As of now (and subject to change on Kaggle's side), Kaggle itself won't inform you about why the save button is grayed out, i.e., the reason being that there is no repository to save to, but rather will only have the ""save"" button grayed out - without an explanation as to why.
"
Import dataset from Kaggle to Databricks with Kaggle&#39;s API,"The main problem here is that you're trying to reference file on DBFS directly /Filestore/..., but Kaggle API doesn't know anything about that filesystem because it uses Python's local file API.  You have two choices:

Refer to DBFS path directly by prepending the /dbfs to file names. Like, /dbfs/FileStore/tables/Kaggle_token/kaggle-2.json instead of /FileStore/tables/Kaggle_token/kaggle-2.json

Use DBUtils commands to copy files to local disks or upload downloaded data to DBFS. For example, to copy file to local disk you can use following (not the file: prefix):


import os
conf_dir = ""/tmp/kaggle-conf""
os.mkdir(conf_dir)
dbutils.fs.cp('/FileStore/tables/Kaggle_token/kaggle-2.json', 
  f'file:{conf_dir}/kaggle.json')
os.environ['KAGGLE_CONFIG_DIR'] = conf_dir

"
How to use Kaggle datasets in BigQuery?,"It is currently not possible to query a public kaggle dataset directly in the BigQuery UI console. You'll have to download the dataset and store it as a native dataset to be able to query it directly.
Alternatively, Kaggle data can be stored in external sources like Drive or Google Cloud Storage and can be queried directly on the BQ console.
So to summarize,
You can query a BQ dataset on kaggle kernel, however public kaggle datasets cannot be queried directly on BQ console.
"
How can I get the requirements.txt file on Colab or Kaggle notebook?,"In Google colab, to get an initial colab environment (set of modules that already installed by Google colab. You can use it without any further import) , you can try this :
# to get packages initially installed in colab or in other word, the colab environment
!pip3 freeze > requirements.txt  

For the code above, the requirements.txt file will appear in the content section of your Google colab (will be in the path named : ‘/content/‘)
Or, in case that what you want is the imported module, not a colab environment. What you can do is :
# get list of all imported modules 
import sys
print(sys.modules.keys())

Moreover, for the requirements.txt in kaggle, you can take a quick look at the comment below your question section. The reference link provided by Ali Ent was very clear
Hope this help !
"
kaggle dataset download 403 Forbidden,"This is a novice mistake but others may have the same issue as it is a bit confusing.
I thought the page that have Data tab is the page where I could download the dataset and get API command.

Copied the <owner>/<dataset> which is abdz82/yolov1 and run download command.
$ kaggle datasets download -d abdz82/yolov1
403 - Forbidden


Actually I needed to click the dataset name PascalVOC_YOLO which took me to the actual page to download.

kaggle datasets download -d aladdinpersson/pascalvoc-yolo

"
Loaded model gives either 0 or 1 with Keras,"It is probably due to different keras versions.
To get the probabilities try:
model.predict_proba(input_image)

After a certain keras version (I think 2.6), predict and predict_proba return probabilities but for previous versions predict returns 0 or 1
"
how to reduce memory usage in kaggle for python code,"Don't use a list, just write line after line.
itertools.combinations create an iterator that allows you to iterate over each value without having to create a list and store each value in memory.
You can use the csv module to write each combination as a line.
If you don't want an empty line between each combination, don't forget to use the newline='' in open: https://stackoverflow.com/a/3348664/6251742.
import csv
import itertools

deck = ['AD', '2D', '3D', '4D', '5D', '6D', '7D', '8D', '9D', '10D', 'JD', 'QD', 'KD',
        'AC', '2C', '3C', '4C', '5C', '6C', '7C', '8C', '9C', '10C', 'JC', 'QC', 'KC',
        'AH', '2H', '3H', '4H', '5H', '6H', '7H', '8H', '9H', '10H', 'JH', 'QH', 'KH',
        'AS', '2S', '3S', '4S', '5S', '6S', '7S', '8S', '9S', '10S', 'JS', 'QS', 'KS']

combinations = itertools.combinations(deck, 9)

with open('combinations.csv', 'w', newline='') as file:
    writer = csv.writer(file, delimiter=',')
    for combination in combinations:
        writer.writerow(combination)

Result after some time:
AD,2D,3D,4D,5D,6D,7D,8D,9D
AD,2D,3D,4D,5D,6D,7D,8D,10D
AD,2D,3D,4D,5D,6D,7D,8D,JD
AD,2D,3D,4D,5D,6D,7D,8D,QD
AD,2D,3D,4D,5D,6D,7D,8D,KD
...  # 3679075395 more lines, 98.3 GB

"
How to created stacked bar plot with spesific value?,"If you want a grouped bar plot, you should use the px.bar command, not px.histogram. To have stacked bars you need to add a new column with a dummy group (or meaningful if you have several countries):
px.bar(dfg.assign(country='India'), x='country', color='City', y = 'Amount')

Output:

To get the country from the original City column:
df[['City', 'Country']] = df['City'].str.split(', ', n=1, expand=True)

dfg = (df.groupby(['City', 'Country']).sum().sort_values(by='Amount', ascending = False)
         .groupby('Country').head(4).reset_index()
       )

px.bar(dfg, x='Country', color='City', y = 'Amount')

"
How to prevent Kaggle re-downloading model files each time session is ended and restarted?,"Maybe this is the setting you've been looking for. Under Notebook Options, there's a setting labelled PERSISTENCE. You can select Variable and Files from the drop down menu to get persistent storage for both your variables and files.
"
module &#39;tensorflow.keras.applications&#39; has no attribute &#39;convnext&#39;,"Attribution convnext come to TensorFlow in v2.11, as you said update is necessary. If using pip simple,


pip install tensorflow --upgrade


, is fine if you don't have lot of dependencies.
"
"Can&#39;t remove the comma from price list using df[&#39;size_sq.ft&#39;].str.strip(&quot;,&quot;) or .replace method",".str.strip() only operates on leading and trailing characters, if the comma is anywhere else than in first or last position in the string, it won't work. What you are looking for is .str.replace("","", """") which will replace any comma by an empty string.
"
"In a kaggle notebook, how to show all hidden code cells (Show hidden code) at once?","On the notebook page, go to the Chrome Console tab (F12) and paste the code below.
Invisible cells have the class ""sc-jSMfEi imrMBR"".
The code identifies each one and clicks on them to make them visible.
list_hiddencode = document.getElementsByClassName(""sc-jSMfEi imrMBR"");
for (var i =0; i < list_hiddencode.length; i++){
   list_hiddencode[i].click();
}

"
Logit model output is incorrect,"Without your testing/training data or code to get the Kaggle data into the same format, it's hard to know for sure.  However, using the small snippet of data at the start of your code, it is pretty clear that a GLM on those data is predicting hit=TRUE.  Note that the predicted probabilities (pr in the data below) are approximately 1 for the hits and 0 for the non-hits.
library(dplyr)
#> 
#> Attaching package: 'dplyr'
#> The following objects are masked from 'package:stats':
#> 
#>     filter, lag
#> The following objects are masked from 'package:base':
#> 
#>     intersect, setdiff, setequal, union
dat <- structure(list(bpm = c(105L, 170L, 120L, 87L, 129L, 125L), 
               nrgy = c(72L, 
                        71L, 42L, 38L, 71L, 94L), dnce = c(72L, 74L, 75L, 72L, 58L, 
                                                           74L), dB = c(-7L, -4L, -8L, -8L, -8L, -1L), hit = c(TRUE, 
                                                                                                               TRUE, TRUE, FALSE, TRUE, FALSE)), row.names = c(8L, 80L, 
                                                                                                                                                               15L, 361L, 42L, 185L), class = ""data.frame"")


g <- glm(hit ~ bpm + nrgy + dnce + dB, data=dat, family=binomial)
#> Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
broom::augment(g) %>% 
  select(hit, .fitted) %>%
  mutate(pr = plogis(.fitted))
#> # A tibble: 6 × 3
#>   hit   .fitted       pr
#>   <lgl>   <dbl>    <dbl>
#> 1 TRUE     25.7 1.00e+ 0
#> 2 TRUE     39.1 1   e+ 0
#> 3 TRUE     23.4 1.00e+ 0
#> 4 FALSE   -23.7 4.95e-11
#> 5 TRUE     24.2 1.00e+ 0
#> 6 FALSE   -25.9 5.87e-12

Created on 2022-04-18 by the reprex package (v2.0.1)
"
When applying &#39;WHERE&#39; for timestamp directly and applying &#39;EXTRACT&#39; showed different results (Codes Attached),"You're comparing constant dates to timestamps. Constant dates are actually timestamps  looking like 2022-04-07 00:00:00.
So when you want to get all records in a date range January to June you need:
WHERE trip_start_timestamp >= '2017-01-01' 
  AND trip_start_timestamp <  '2017-07-01'  

In other words you want everything on or after midnight on the first day of the range and everything up to but not including midnight on the day after the last day. In mathematical notation you want the dates in the range [2017-01-01, 2017-07-01). The beginning of the range is closed and the end is open.
Your code like this gives a correct result.
WHERE EXTRACT(YEAR from trip_start_timestamp) = 2017 
AND EXTRACT(MONTH from trip_start_timestamp) BETWEEN 1 and 6

But it can't exploit an index on your trip_start_timestamp column, so it won't be efficient in production.
"
"I visualized it as a heatmap to see how the features are correlated, but only one specific feature is not correlated. Why is this happening?","I think there is an initial column and an initial column, so there seems to be a typo, so please check the code.
"
List image paths of multiple formats in a Kaggle Dataset,"This can be done using either os or glob modules in Python. I would suggest using glob as it facilitates more functionality w.r.t filenames in various scenarios.
SAMPLE CODE:

import glob
from tqdm import tdqm

# The required file extensions
fetch_formats = ['png', 'jpg', 'jpeg']

# Declare an empty list for storing the file names
img_list = list()

# State the directory of interest
path = working_dir + ""images/**/*.""

# Fetch each type of file from the given directory
for ff in tqdm(fetch_formats, desc=""Fetching the filenames""):
    img_list.extend(list(glob.glob(path+ff)))

print(f""\nTotal number of images: {len(img_list)}"")

NOTE:

Usage of tqdm is for generating the progress bar and can be avoided
The *.png* would imply any filename ending with .png`
The dir\**\*.png would imply any sub-directory inside dir which contains files whose names end with .png

Check out the official documentation for more information
"
Removing single quotes in R,"Your input has ""fancy quotes"", not standard quotes. This should get rid of all fancy single and double quotes and all non-fancy single quotes:
gsub(""['‘’”“]"", """", df$Line)

"
Creating a boxplot in MatPlotLib using Titanic Dataset,"There are a lot of missing values in the Age column of the Titanic data set. Either remove those rows, or fill the values with a default before creating a boxplot.
plt.boxplot(data['Age'].fillna(0.0))


"
How do I create a prefetch dataset from a folder of images?,"import pathlib 
import tensorflow as tf
import numpy as np

@tf.autograph.experimental.do_not_convert
def read_image(path):
   image_string = tf.io.read_file(path)
   image = DataUtils.decode_image(image_string,(image_size))
   return image
    
AUTO = tf.data.experimental.AUTOTUNE

paths = np.array([x for x in pathlib.Path(IMAGE_PATHS_DIR).rglob('*.jpg')])
dataset = tf.data.Dataset.from_tensor_slices((paths.astype(str)))
dataset = dataset.map(self.read_image)
dataset = dataset.shuffle(2048)
dataset = dataset.prefetch(AUTOTUNE)

"
Install pywin32 package in google colab or kaggle notebook environment,"Unfortunately you can't install it in linux python, pywin32 is a package of extension modules for accessing Windows C and COM APIs in Windows python:

Python extensions for Microsoft Windows Provides access to much of the Win32 API, the ability to create and use COM objects, and the Pythonwin environment.

Google Colab

Kaggle

"
Data cardinality is ambiguous. Make sure all arrays contain the same number of samples,"Thank you @Tim Roberts and @hlcodes01. For the benefit of community providing solution here
import os
import caer
import canaro
import numpy as np
import cv2 as cv
import gc
import matplotlib.pyplot as plt
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import LearningRateScheduler

IMG_SIZE = (80,80)
channels = 1
char_path = r'../input/the-simpsons-characters-dataset/simpsons_dataset'

char_dict = {}
for char in os.listdir(char_path):
    char_dict[char] = len(os.listdir(os.path.join(char_path,char)))

#sort
char_dict = caer.sort_dict(char_dict, descending=True)

characters = []
count = 0
for i in char_dict:
    characters.append(i[0])
    count += 1
    if count >= 10:
        break

#training data
train = caer.preprocess_from_dir(char_path, characters, channels=channels, IMG_SIZE=IMG_SIZE, isShuffle=True)

plt.figure(figsize=(30,30))
plt.imshow(train[0][0], cmap='gray')
plt.show()

featureSet, labels = caer.sep_train(train, IMG_SIZE=IMG_SIZE)

# normalize feature sets
featureSet = caer.normalize(featureSet)
labels = to_categorical(labels, len(characters))

x_train, x_val, y_train, y_val = caer.train_val_split(featureSet, labels, val_ratio = 0.2)

del train
del featureSet
del labels
gc.collect()

BATCH_SIZE = 32
EPOCHS = 10

# image data generator
x_train = np.array(x_train)
y_train = np.array(y_train)
x_val2  = np.array(x_val)
y_val2  = np.array(y_val)


datagen = canaro.generators.imageDataGenerator()
train_gen = datagen.flow(x_train, y_train, batch_size=BATCH_SIZE)

# creating the model
model = canaro.models.createSimpsonsModel(IMG_SIZE=IMG_SIZE, channels=channels, output_dim=len(characters), loss='binary_crossentropy', decay=1e-6, learning_rate=0.001, momentum=0.9, nesterov=True)

callbacks_list = [LearningRateScheduler(canaro.lr_schedule)]

training = model.fit(train_gen,
                     steps_per_epoch=len(x_train)//BATCH_SIZE,
                     epochs=EPOCHS,
                     validation_data=(x_val2, y_val2),
                     validation_steps=len(y_val2)//BATCH_SIZE,
                     callbacks=callbacks_list)

"
Assigning labels to train data in CNN,"Firstly read the .txt file and make path list and labels:
with open('train.txt', 'r') as f:
     path_label = f.readlines() # returns a list

# seperate paths and labels
paths = [i.strip().split()[0] for i in path_label] 
labels = [i.strip().split()[1] for i in path_label]

Then you can read and fed the data to your model.
"
Matplotlib bar chart: how to change names on axis x,"I am writing an answer since I cannot write a comment due to the low reputation.
Given your code, it creates an expected output with matplotlib version 3.3.4.
Result image
from matplotlib import pyplot as plt
import pandas as pd


if __name__ == '__main__':
    s = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]
    p_s = [0.05, 0.15, 0.20, 0.30, 0.20, 0.10]
    p_s_x = [0.06005163309361129, 0.4378503494734475,0.3489460783665687,0.1404287057633398,0.012362455732360653,0.00036077757067209113]
    
    df_to_plot = pd.DataFrame(data={""P(S)"": p_s,
                                    ""P(S|X)"": p_s_x,
                                    ""S"": s})
    
    df_to_plot.plot.bar(y=['P(S)', 'P(S|X)'],
                    x='S',
                    alpha=0.7,
                    color=['red', 'green'],
                    figsize=(8,5))
    plt.show()

"
How can I find the count of the most frequent and least frequent using Pandas?,"This should work:
df.groupby('cast')['show_id'].count().nlargest()

This will return the count for each group, sorted by count in descending order:
cast              count
Alan Marriott       100
Jandino Asporaa      78
...
Peter                 1

"
How do I prevent corrupted record in Kaggle TFRecord file?,"I found the problem.  The program that was writing the TFRecords was specifying that the writer GZIP its output, but the reader didn't know that.  By making the following change in creating the reader
raw_dataset = tf.data.TFRecordDataset( output_path, compression_type = 'GZIP' )

I am now able to read the records.
"
Kaggle - Tweet Sentiment Extraction - What will be length of word or phrase that supports the sentiment,"The most common way this is done is by having your model predict a start index and an end index (of the sequence of tokens you want to extract).
Poking through the discussion threads, this was the architecture of the winning entry for that competition: https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/159477
Notice in the first section ""Heartkilla"" they are predicting two things, y-start and y-end. Further down they mention they filter out predictions where y-start is greater than y-end.
"
Why does my Kaggle notebook look like JSON,"Oh, OK, I just clicked on [Edit] and now the notebook looks like a notebook.
"
FileNotFoundError: [Errno 2] No such file or directory: &#39;/kaggle/input/CORD-19-research-challenge/comm_use_subset/comm_use_subset/pdf_json&#39; in kaggle,"The above error is because of the wrong file path. To get the correct file path (structure of the directories), run the below code in the Kaggle notebook:
count = 0
for root, folders, filenames in os.walk('/kaggle/input'):
   print(root, folders)

You will get the below result:
Screenshot of the Result
Change the file of the json file paths as
'/kaggle/input/CORD-19-research-challenge/document_parses/pdf_json/'
as shown in the result above. Now re-run the code. It will work.
"
python traceback keyerror code for training model on kaggle text dataset,"If you just want to multi class classification, below code is enough.
# no loop with categories
SVC_pipeline.fit(X_train, train.category)

You can subscript pandas dataframe only by column name(id, title, abstract, categories).
But you use a value of category column. So key error happened.
If you want to filter the dataframe by value, use below code.
train[train[‘category’] == category]

"
Kaggle notebook struggles to download RESNET34 FastAI,"I ran into the same problem, and asked about it in the FastAI Discord. It turns out that you have to ""enable internet"" for your Kernel in Kaggle, so it can be Downloaded.
In your Kaggle notebook, at the right-top you have the icon: |< (on the right-side of the blue ""Save Version"").
Click that icon, and then under ""Settings"", there is a ""Internet"" slider. You'll need to verify your account with a phone number.
"
How can I fix the issue to reshape process in image derived from x_train in Python?,"This dataset is likely not intended to be used with CNN, because the data encoded into the columns has no spatial relation to each other, like in images. Considering that this dataset was downloaded 1 (one) time, probably by you, and nobody has created any notebooks or deemed it worth a discussion, I'd recommend to move to another dataset, which has other people working on it, so you can ask questions there (on Kaggle) and get help.
"
ImageDataBunch.from_df positional indexers are out-of-bounds,"I faced this error while creating a DataBunch when my dataframe/CSV did not have a class label explicitly defined.
I created a dummy column which stored 1's for all my rows in the dataframe and it seemed to work. Also please be sure to store your independent variable in the second column and the label(dummy variable in this case) in the first column.
I believe this error happens if there's just one column in the Pandas DataFrame.
Thanks.
Code:
df = pd.DataFrame(lines, columns=[""dummy_value"", ""text""])
df.to_csv(""./train.csv"")
data_lm = TextLMDataBunch.from_csv(path, ""train.csv"", min_freq=1)

Note: This is my first attempt at answering a StackOverflow question. Hope it helped!
"
How to use R and python in a Kaggle Notebook?,"One can notice that a Kaggle Kernel is using behind an anaconda environment. For example, 
/opt/conda/bin/python3.7

Also, it is necessary to have R installed on this conda environment. Thus, we can use the subprocess library to run the following script for installing R 
import subprocess
subprocess.run('conda install -c conda-forge r-base', shell=True)

and the corresponding rpy2 package 
!pip install rpy2

I have provided a notebook on Kaggle with a complete explanation. I'll appreciate your comments. 
"
where does this parenthesis come from?,"The problem is how you created the columns of data_enc. You passed a list which contains an Index object. Because of this nesting, pandas decided to create a broken MultiIndex. (It's broken because it's a MultiIndex with only a single level, so it really shouldn't exist)
Example:
df = pd.DataFrame(columns=list('abc'))

# Placing the Index in a list incorrectly leads to a MultiIndex
pd.DataFrame(columns=[df.columns+'_suffix']).columns
#MultiIndex([('a_suffix',),
#            ('b_suffix',),
#            ('c_suffix',)],)

# Instead get rid of the list, just add the suffix:
pd.DataFrame(columns=df.columns+'_suffix').columns
#Index(['a_suffix', 'b_suffix', 'c_suffix'], dtype='object')

"
type object &#39;DataFrame&#39; has no attribute &#39;sparse&#39;,"Try this:
_dataframe = pd.SparseDataFrame(_vectorized)
"
Variables are assigned only locally inside a for loop,"When you are in the loop dataset is a copy of the DataFrame in combine, so when you change dataset you aren't changing the actual DataFrame in combine, just the copy. To change the actual  DataFrame in the list try something like this:
for ii in range(len(combine)):
    combine[ii] = combine[ii].drop(['Ticket', 'Cabin'], axis=1)

Now you are changing the variable in the list and not just the copy. 
"
"Performing an aggregate function (e.g., mean) across rows produces NaN","You likely specify the wrong axis. Try:
df_viz['mean_monthly_saleprice']=df_viz.mean(axis=1)


As to why your original code returned na, df_viz.mean(axis=0) produces the means by column. The result is a series with the column names as labels:
SalePrice2006    <a number>
SalePrice2007    <a number>
SalePrice2008    <a number>
SalePrice2009    <a number>
SalePrice2010    <a number>

You then try to combine that series with the df_viz dataframe, which is labeled by MoSold. No label matched between the two indexes. Hence your result was na.
Moral of the story: index matters hugely in a dataframe. Pay good attention to them.
"
How to load Kaggle datasets into Intel DevCloud Jupyter Notebook,"Steps:

Open a terminal in dev cloud (+ on top left corner)

Type following in the terminal
pip install kaggle

To use Kaggle API, create an account in kaggle official page(https://www.kaggle.com). Then, go to ""My Account"" tab of user's profile and select ""create new API token"". This will download the kaggle.json file in your system.

Upload .json file to devcloud

type the command to copy .json file to the kaggle root folder
cp -r ""kaggle.json"" ""/home/u51xxx/.kaggle""

To check the list of datasets available
kaggle datasets list -s bengaliai-cv19



(-s refers search, followed by the name of the dataset)

To download the dataset type:
kaggle datasets download (filename obtained in the list)


"
Using Torchvision ImageFolder with Test Set,"Usually, in the context of training networks, the ""test"" set is actually a ""validation"" set: its a subset of the labeled examples that the model does not train on, but only being evaluated. This validation set is used for tuning meta parameters (e.g., number of epochs, learning rate, batch size etc.).
Therefore, despite the fact that the validation (""test"") set is not used for actual SGD training, you do have its labels and they are used to estimate the generalization error of the trained model.
Since you usually do have the labels for this set, you can read it using ImageFolder class same as the training set.
However, if you have a test set that has no labels at all, you can still use the ImageFolder class to handle the set. All you need is to create a dummy subfolder to represent a ""label"" for the set: ImageFolder assumes the images are stored in subfolders based on their labels. 
"
remove factor level from dataframe,"After removing the lines with empty values for Embarked, refactorize:
df <- df[df$Embarked!="""",]
df$Embarked <- factor(df$Embarked)
barplot(table(df$Embarked), xlab=""Port of Embarkment"", 
        ylab=""Frequency"", main=""Histograma de la variable \n Embarked"")


"
Can&#39;t see full Error log in kaggle Console,"Hmm, it might be easiest to flip it to a notebook (in the editor, File -> Kernel Type -> Notebook), run the cell, and then flip it back to a script (in the editor, File -> Kernel Type -> Script) when you're ready.
"
Where are the files generated through code in a kaggle kernel stored?,"https://www.kaggle.com/getting-started/58426
Pretty simple and posted from Kaggle forum.
"
How to work with a kaggle dataset in a zip file?,"You have the images already unzipped in the directory ../home/train_images 
Run this in your kernel:
from os import listdir
listdir('../input/train_images/')


Use ImageDataGenerator.flow_from_directory() to use the images in the directory with your generator. 
Check Keras docs: https://keras.io/preprocessing/image/#imagedatagenerator-methods
"
Sort Pandas columns by number of unique groups,"You can call nunique directly and index your initial DataFrame using the result:
u = df.nunique().sort_values().index
df[u]


  MSZoning Street LotConfig
0       RL   Pave    Inside
1       RL   Pave       FR2
2       RL   Grvl    Corner


df.nunique() will return a Series of unique values per column.
>>> df.nunique()
LotConfig    3
Street       2
MSZoning     1
dtype: int64


However, on your small example, your approach is actually faster than mine, albeit a bit more verbose.  I would test this on your actual DataFrame, since your method isn't incorrect, and if it improves performance, I would go with that instead.  I believe my method should be faster on a large frame since it avoids calling nunique many times.
"
How to install tensorflow-probability in kaggle kernel for R language,"The key problem is that the preinstalled r-tensorflow virtual environment is not in a default location, which prevents the install_tensorflow() method from editing it. To resolve this, one first must set the WORKON_HOME environment variable that Reticulate uses to identify the root of the virtualenv environments.  I was able to get a proper installation along the following lines:
# set virtualenv root to where 'r-tensorflow' env is located
Sys.setenv(WORKON_HOME=""/root/.virtualenvs"")

# install greta
install.packages(""greta"")

# install tfp
tensorflow::install_tensorflow(envname=""r-tensorflow"", extra_packages=c(""tensorflow-probability==0.3.0""))

# check that TFP is installed in the env
dir(""/root/.virtualenvs/r-tensorflow/lib/python2.7/site-packages"")
## ...
## [56] ""tensorflow""                            
## [57] ""tensorflow_probability""                
## [58] ""tensorflow_probability-0.3.0.dist-info""
## [59] ""tensorflow-1.10.0.dist-info""
## ...

Along these lines, I made a public Kaggle kernel available that runs the default Greta example.
The above code results in installing Greta v0.3.0, TF 1.10.0, and TFP 0.3.0, which is the correct version matching. I was also able to install the latest versions using
# set virtualenv root to where 'r-tensorflow' env is located
Sys.setenv(WORKON_HOME=""/root/.virtualenvs"")

# install latest greta
devtools::install_github(""greta-dev/greta"")

# install tfp
tensorflow::install_tensorflow(envname=""r-tensorflow"", version=""1.13.1"", extra_packages=c(""tensorflow-probability==0.6.0""))

which also gets library(greta) to launch without complaint. However, it crashed during sampling, with a complaint about the assertthat package being corrupted. Note that assertthat gets updated as part of the Greta install from GitHub, which is why I ended up using the CRAN version.
Hopefully in the future Kaggle just includes TFP and one won't have to deal with this mess.
"
Upload Kaggle Dataset into Github,"This is not recommended however there is access from Kaggle Kernel to the terminal by using ! before the linux command. 
Note: Please ensure the internet for the kernel is enabled
example : 
!ls ../input
!git

"
How do I deal with an Ifelse condition that is not giving me an error but is also not giving me any change in my output?,"Try logical indexing.
inx <- train$titles == ""Dr""
train$titles[inx & train$sex == ""male""] <- ""Mr""
train$titles[inx & train$sex == ""female""] <- ""Mrs""

Also, like user Dan Y said in a comment to the question, repeated here because sometimes comments are deleted,

Use ifelse instead of if because the former is vectorized.

A ifelse solution still using inx as defined above could be
train$titles[inx] <- ifelse(train$sex[inx] == ""male"", ""Mr"", ""Mrs"")

I am using inx to avoid a longer code line. You can put the definition of inx in the indices of the ifelse if you prefer.
"
Inconsistent numbers of samples error from Python,"The error is because you are taking labels from filtered data and taking x from unfiltered data
Change the following line
x = train_data[columns_of_interest]

to
x = filtered_titanic_data[columns_of_interest]

"
&#39;Response&#39; object has no attribute &#39;soup&#39; in kaggle cli,"I'd recommend using the officially supported Kaggle API instead. Once you've got it set up (you can find step-by-step instructions here), you can download all the files in the competition like so:
kaggle competitions download -c instacart-market-basket-analysis

Note that, just like with the unofficial Kaggle CLI, you will have to accept the competition rules before you can download the data for that competition. 
Hope that helps!
"
Can we use Logistic Regression to predict numerical(continuous) variable i.e Revenue of the Restaurant,"Yes... You can.!!
Prediction using Logistic Regression can be done for numerical variables. The data you have right now contains all independent variables, and the outcome will be a dichotomous (dependent variable, having value TRUE/1 or FALSE/0).
You can then use it to determine the log odds ratio to find a probability(range 0-1).
For a reference you can have look at this.
-------------------UPDATE-------------------------------
Let me give u an example of my last yr's wok.. we had to predict if a student can qualify in campus placement or not, given history data of 3 yrs of test results and their final success or failure. (NOTE : This is dichotomous, will talk about this later.)
Sample data was, student's marks in academics, and aptitude test held at college, and their status as placed or not.
But in your case, you have to predict the revenue (WHICH IS non-dichotomous). So what to do?? It seems that my case was simple, right??
Nope..!!
We were not asked just to predict if the student will qualify or not, we were to predict the chances of individual student getting placed, which is not at all a dichotomous. Looks like your scenario right?
So, what you can do is, first classify the data as for what input variables, what is the final output variable (that will help in revenue calculation).
For eg: Use data to find out if the restaurant will go in profit or loss, then relate it with some algorithms to find out the approx revenue prediction.
I'm not sure if there are already such algorithms (identical to your need) exists or not, but I'm sure you can do much better by putting more efforts on research an analysis on this topic.
TIP: NEVER think in such way that ""Will Logistic Regression ONLY solve my problem?"" Rather expand it to, ""What Logistic can do better if used with some other technique.?""
"
Deep Learning fit error (the list of Numpy arrays that you are passing to your model is not the size the model expected.),"You should simply transform your Y to a numpy array with shape (24500, 2):
Y = np.ndarray(Y)

"
Keras model.pop() not working,"I believe this is a problem with the implimentation of layers.pop() in Keras when using the tensorflow backend. For now here is a work-around removing the last layer by name:
name_last_layer = str(model1.layers[-1])

model2 = Sequential()
for layer in model1.layers:
    if str(layer) != name_last_layer:
        model2.add(layer)

Where model1 is your original model and model2 is the same model without the last layer. In this example I've made model2 a Sequential model but you can ofcourse change this. 
"
Python: [Errno 2] No such file or directory: ...site-packages/testpath-0.3.1.dist-info/top_level.txt,"I used pip3 to install the kaggle-cli (pip3 install kaggle-cli) and I got the same error. Then I uninstalled it and used pip install kaggle-cli, then it worked.
"
best way for substring pandas data frame,"Use str.split, and then extract the second item from the resultant list.
In [37]: df['Name'].head()
Out[37]: 
0                              Braund, Mr. Owen Harris
1    Cumings, Mrs. John Bradley (Florence Briggs Th...
2                               Heikkinen, Miss. Laina
3         Futrelle, Mrs. Jacques Heath (Lily May Peel)
4                             Allen, Mr. William Henry
Name: Name, dtype: object

An observation here is that names follow this format: Last Name, Salutation Given Name. We'll split on spaces and extract the Salutation from the split lists using df.apply:
In [38]: df['Title'] = df['Name'].str.split(' ').apply(lambda x: x[1])

In [39]: df['Title'].head()
Out[39]: 
0      Mr.
1     Mrs.
2    Miss.
3     Mrs.
4      Mr.
Name: Title, dtype: object

"
Pandas Hashtable KeyError,"The following lines
X = df['MaterialDescription']
clean_review = description_to_words(X[3] )

give in python description_to_words(df['MaterialDescription'][3] )
You have to locate your index through:
clean_review = description_to_words(df.iloc[3]['MaterialDescription'] )

"
Spark: How to split an instance into Postivie/Negative Samples according to two columns,"You can indeed use explode on the result of a UDF that would produce a series of ""events"" - 1 for a click event and 0 for a non-clicked impression event:
// We create a UDF which expects two columns (imps and clicks) as input, 
// and returns an array of ""is clicked"" (0 or 1) integers
val toClickedEvents = udf[Array[Int], Int, Int] {
  case (imps, clicks) => {
    // First, we map the number of imps (e.g. 3) into a sequence
    // of ""imps"" indices starting from zero; Each one would later
    // represent a single impression ""event""
    val impsIndices = (0 until imps)

    // we map each impression ""event"", represented by its index, 
    // into a 1 or a 0: depending if that event had a matching click;
    // we do that by assigning ""1"" to indices lower than the number of clicks
    // and ""0"" for the rest
    val clickIndicatorPerImp = impsIndices.map(index => if (clicks > index) 1 else 0)

    // finally we just convert into an array, to comply with the UDF signature
    clickIndicatorPerImp.toArray
  }
}

// explode the result of the UDF and calculate ImpressedNotClicked
df.withColumn(""Clicked"", explode(toClickedEvents($""Impressions"", $""Clicks"")))
  .select($""ID1"", $""ID2"", $""ID3"", $""Clicked"", abs($""Clicked"" - lit(1)) as ""ImpressedNotClicked"")

NOTE: original post was tagged with scala; If you can convert this into python, feel free to edit
"
Unable to put NA in the empty columns of dataset,"As you can see in the dput, the train$Cabin missing values are """".
So in order to change this to NA, you can't put space inside the quotation mark.
You just need to do this train$Cabin[train$Cabin==""""] <- NA
You need to specify you want the Cabin column to be changed, and r recognizes NA without the quotes.

As Frank commented, if you just read the .csv file with na.strings = """", it will automatically do the job. It would be like this:
train <- read.csv(""YOUR_PATH\\train.csv"", stringAsFactors = F, na.strings = """")


Some tips:

When you read.csv(), set stringsAsFactors = F, if you want your characters columns to continue as characters, and not factors
When you write.csv(), set row.names = F if you don't want it to create a column with lines id's.

"
How to use tabulate lib with float64 : python,"Quoting from the tabulate documentation,

The following tabular data types are supported:

list of lists or another iterable of iterables
list or another iterable of dicts (keys as columns)
dict of iterables (keys as columns)
two-dimensional NumPy array
NumPy record arrays (names as columns)
pandas.DataFrame


Your variable surv_age is a 1-D numpy array of shape (342,). You will need to re-shape into a 2-D numpy array. You can do this easily using numpy.reshape,
surv_age = np.reshape(surv_age, (-1, 1))

You can also do this using np.expand_dims like this,
surv_age = np.expand_dims(surv_age, axis=1)

"
TensorFlow - Unable to get Prediction,"I combined your train_neural_network and make_prediction function into one single function. Applying tf.nn.softmax to the model function would make the value range into from 0~1 (interpreted as probability), then tf.argmax extracts the column number with the higher probability. Note that the placeholder for y in this case needs to be one-hot-encoded. (If you are not one-hot-encoding y here, then pred_y=tf.round(tf.nn.softmax(model)) would convert the output of softmax into 0 or 1)
def train_neural_network_and_make_prediction(train_X, test_X):

    model = neural_network_model(x)
    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(model, y) )
    optimizer = tf.train.AdamOptimizer().minimize(cost)
    pred_y=tf.argmax(tf.nn.softmax(model),1)

    ephocs = 10

    with tf.Session() as sess :
        tf.initialize_all_variables().run()
        for epoch in range(ephocs):
            epoch_cost = 0

            i = 0
            while i< len(titanic_train) :
                start = i
                end = i+batch_size
                batch_x = np.array( train_x[start:end] )
                batch_y = np.array( train_y[start:end] )

                _, c = sess.run( [optimizer, cost], feed_dict={x: batch_x, y: batch_y} )
                epoch_cost += c
                i+=batch_size
            print(""Epoch"",epoch+1,""completed with a cost of"", epoch_cost)
        # make predictions on test data
        predictions = pred_y.eval(feed_dict={x : test_X})
    return predictions

"
How to see the plot made in python using pandas and matplotlib,"Try using plt.show() at the end.
EDIT:
Also, you may need to add %matplotlib inline as explained here: How to make IPython notebook matplotlib plot inline
"
Inception - mxnet model,"I think you're trying to run this notebook https://github.com/dmlc/mxnet/blob/master/example/notebooks/predict-with-pretrained-model.ipynb
You need to first download the pre-trained model from https://github.com/dmlc/mxnet-model-gallery/blob/master/imagenet-1k-inception-bn.md before this notebook can run.
"
SIFT from OpenCV in kaggle script editor,"SIFT is part from the opencv-contrib package, which is not shipped with the opencv package contain experimental and non-free modules - so be aware of the licensing and so...
You should build opencv with the extra modules, you need to:

Download or clone the opencv-contrib
use this flag in the cmake command line: cmake -DOPENCV_EXTRA_MODULES_PATH=<opencv_contrib>/modules <other_flags> <opencv_source_directory>

"
XGBoost predictor in R predicts the same value for all rows,"You must remove the Survived variable in your test set in order to use xgboost, since this is the variable you want to predict.
trmat = data.matrix(train[, colnames(train) != ""Survived""])

It should solve your problem.
"
Problems loading csv files into mysql with spanish accents / Kaggle BIMBO Competition?,"I guess you are working on Kaggle's Grupo Bimbo competition.  Could you show more output on your errors?
"
Python with Caffe: The custom data are all zeros when read from solver,"Try doing a net.forward(). You should be able to see your data if everything else is correct.
A simpler and safer way to write to LMDB is using caffe.io.array_to_datum as demonstrated here.
"
Gradient Boosting Classifier sparse matrix issue using pandas and scikit,"Thanks to @imaluengo.
Just in case anyone needs. The issue is in these lines.
train = tfidf.fit_transform(train)
test = tfidf.transform(test).toarray() # Update line

Both lines should have a toarray() to fix this.
train = tfidf.fit_transform(train).toarray()
test = tfidf.transform(test).toarray() # Update line

"
"Kaggle word2vec competition, part 2","This is UnicodeDecodeError, when your data is not in the proper encoding type (it should be 'unicode' instead of 'str'). Change to this may help:
`sentences += review_to_sentences(review.decode(""utf8""), tokenizer)`

But it may take time. Another way is to specify the encoding 'utf8' in the beginning when you read the input data:
`pd.read_csv(""input_file"", encoding=""utf-8"")`

"
TypeError: fit() takes exactly 3 arguments (2 given) with sklearn and sklearn_pandas,"Since sklearn_pandas doesn't currently support estimators accepting an y vector with labels, you will have to use it only to transform all features to a Numpy matrix and then use the KNeighborsClassifier in a separate step.
UPDATE 2015-08-10 - sklearn_pandas DataFrameMapper isn't intended to be used as a pipeline for transformation + model fitting, but only for transforming the columns selectively. If you want to transform then estimate a model, use a plain sklearn Pipeline with the dataframe mapper as first step.
"
How to avoid error in textmatrix function in R&#39;s LSA package,"I don't know how to make it ignore empty files. A sort-of workaround that I have used is to add a  word that was not yet in the corpus to every file. 
Advantages:

every file will have at least one word, so that textmatrix does not fail
the same word in every file will not affect the relevance of individual documents
you know that the number of words according to the textmatrix is one more than the number of words in the original documents

Disadvantage:

each file becomes a bit similar to all the others, because they all share one word.

(Note: there may be disadvantages that I haven't thought of.)
"
How can I update datasets in Kaggle code script,"You're looking for the
sep
parameter.
Even though your file is labeled *.csv,
it appears to use ; semicolon rather than , comma
as the field separator.
Share that insight with the .read_csv() function when you call it, and it will behave more to your liking.
"
Kaggle: cannot unpack non-iterable nonetype object,"Nevermind, I've already figured it out. I forgot put the XLNet Tokenizer
"
How do I use python version 3.7.1 in Kaggle,"
Is there any method through which I can use a python 3.7 environment

Yes,you can change the python version in Kaggle.
you can follow these link
for steps and commands.
Please note that the availability of specific Python versions may vary, and you should check the Kaggle environment settings for the most up-to-date information.
Additionally, keep in mind that using an older Python version may limit access to some libraries or features that are available in newer Python versions.
"
How to add kaggle dataset download stats in github readme file?,"The simplest way to do it would be to implement your own web scraper and use a github workflow to periodically scrape the required data.
Crude Python implementation
Create requirements.txt file
In the root directory of your github repository, create a requirements.txt file with the following:
selenium==4.6.0

The selenium web scraper will be used to fetch data from the kaggle website.
Create script
In the root directory of your github repository, create a badge_generator.py file with the following:
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
import time
from selenium.webdriver.common.by import By


def update_readme(readme_file_path, badge_id, new_badge):
    new_file_content = ''
    # id used to identify position of badge
    line_id = f'![kaggle-badge-{badge_id}]'
    badge_found = False

    # open readme and update badge
    with open(readme_file_path, 'r', encoding='utf-8') as f:
        # get all lines in readme
        lines = [line for line in f]
        for i in range(0, len(lines)):
            if line_id in lines[i]:
                # replace old badge with new badge
                lines[i] = new_badge
                badge_found = True
                break
        # concatenate lines
        new_file_content = ''.join(lines) if len(lines) > 0 else new_badge

    if not badge_found:
        raise Exception(
            str(f""Badge {badge_id} not found in {readme_file_path}""))
    # update readme
    with open(readme_file_path, 'w', encoding='utf-8') as f:
        f.write(new_file_content)


def create_badge(badge_id, badge_value,
                 badge_name='Downloads', badge_color='orange'):
    badge_url = (f'https://img.shields.io/badge/{badge_name}'
                 f'-{badge_value}-{badge_color}')
    markdown = (f'![kaggle-badge-{badge_id}]({badge_url})\n')
    return markdown


def get_download_count(kaggle_url: str):
    chrome_options = Options()
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--headless')
    driver = webdriver.Chrome(options=chrome_options)

    driver.get(kaggle_url)
    time.sleep(3)
    downloads_element = driver.find_element(
        By.XPATH,
        '//*[@id=""site-content""]/div[2]/div/div[5]/div[6]/div[2]/div[1]/div/div[3]/h1')
    download_count = downloads_element.get_attribute(""textContent"")
    return (download_count)


def main():
    readme_file_path = ""README.md""  # relative to root directory
    # change this url
    url = 'https://www.kaggle.com/datasets/utkarshx27/marijuana-arrests-in-columbia'
    badge_id = 1  # each badge must be given a unique id
    x = get_download_count(url)
    y = create_badge(badge_id, x)
    update_readme(readme_file_path, badge_id, y)


main()

Replace the value of url in main function with the URL of the kaggle card.
README modifications
Your README.md file must be in root directory. In your README file, add the following line at a line number where you want the badge to be:
![kaggle-badge-1]()

This line should be present before running script. When script is run, this line will be overwritten and the badge is updated.
Do not write anything else on this line.
Create github workflow
Create a .github folder in the root directory of your github repository and inside this folder create another folder workflows. Place badge.yml inside workflows:
name: Kaggle Badge Generator

on:
  push:
  workflow_dispatch:
  schedule:
    - cron: '0 * * * *' #  run every hour

jobs:
  build:
    runs-on: ubuntu-latest
    steps:

      - name: checkout repo content
        uses: actions/checkout@v3

      - name: setup python with pip cache
        uses: actions/setup-python@v4
        with:
          python-version: '3.9' 
          cache: 'pip' # caching pip dependencies

      - name : install any new dependencies
        run: pip install -r requirements.txt
          
      - name: execute py script 
        run: python badge_generator.py 
        
      - name: commit files
        run: |
          git config --local user.email ""action@github.com""
          git config --local user.name ""GitHub Action""
          git add -A
          timestamp=$(date -u)
          git diff-index --quiet HEAD || (git commit -a -m ""Last badge update : ${timestamp}"" --allow-empty)
          
      - name: push changes
        uses: ad-m/github-push-action@master
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          branch: main 

The python script will run every hour and will update the badge. The cron job can be modified to run more frequently.
The badge will look like this:

Your github file directory structure will be like this:
.github/
├─ workflows/
│  ├─ badge.yml
badge_generator.py
requirements.txt
README.md
... your stuffs

Other methods
If you don't want the script to directly modify your README, you will have to implement some sort of API. Look into free serverless functions on Vercel or REST API on Render. This could pair with the dynamic badges Actions.
"
Can&#39;t install gdown on kaggle,"you have to check your internet connection and then try again or if you have PyCharm you can install it directly by selecting the package you want to install .
"
Changing datatype of column from object to float in Kaggle,"Use to_numeric here to convert the column to float, here is how you could do it
import pandas as pd

# Load the dataset
df = pd.read_csv('dataset.csv')

# Clean the Expenses column
df['Expenses'] = df['Expenses'].astype(str).str.replace('+', '') # Remove the plus sign
df_clean = df.dropna() # Remove any rows with missing values
df_clean = df_clean[df_clean[""Expenses""] != 'nan'] # Remove any rows with Expenses value 'nan'
df_clean = df_clean[df_clean[""Expenses""] != '9998948894'] # Remove any rows with Expenses value '9998948894'

# Convert the Expenses column to float
df_clean['Expenses'] = df_clean['Expenses'].astype(float)

"
Kaggle Data Clean Up,"You're going to want to use the | instead of or when filter dataframes:
df_clean = df_clean[(df_clean[""Gender""] == 'Male') | (df_clean[""Gender""] == 'Female')]

You can take a look at the documentation for more info.
"
Error when trying to clone darknet on kaggle notebook . Help anyone?,"I haven't turned on internet on kaggle notebook. My bad..
"
"Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required","The issue is arising where you are calling mutual_info_regression over here -
mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)

As per sklearn's documentation, the parameter discrete_features should be a boolean mask that has True for discrete variables and False otherwise.
I checked your Kaggle code and it seems like your technique for identifying discrete and continuous features in your data frame is wrong.
A simple hack to get the code running would be to identify all features as continuous using the following code -
discrete_features = [False]*73
# 73 is the number of columns X has

However, the result might be wrong if the mutual_info_regression algorithm requires you to accurately identify discrete and continuous features.
"
LookupError: Resource omw-1.4 not found,"I faced the same problem, and as explained in https://github.com/nltk/nltk/issues/3024, you just need to download omw-1.4. Looks like that the new version of nlkt requires the presence of the omw-1.4 package, which was not needed in the past.
import nltk
nltk.download('omw-1.4')

"
How do I access the data from one column and make changes against another column in Pandas?,"You can groupby twp column and fillna in zip with value.
df[""zip""] = df[""zip""].fillna(df.groupby(""twp"")[""zip""].transform(""first""))

"
Trying to compare target sales vs actual sales,"Here is a way to extract the year and month from a date (it's also possible to connect the dates from the other table and you can present which info you want).
select *
from   t full join t2 on  year(MonthOrderDateFixed)  = t.Year
                      and month(MonthOrderDateFixed) = t.Month





Year
Month
TotalAmount
TotalProfit
MonthOrderDateFixed
Category
Target




2018
4
32726
-3960
2018-04-01 00:00:00.000
Furniture
10400


2018
5
28545
-3584
2018-05-01 00:00:00.000
Furniture
10500


2018
6
23658
-4970
2018-06-01 00:00:00.000
Furniture
10600


2018
7
12966
-2138
2018-07-01 00:00:00.000
Furniture
10800


2018
8
30899
-2180
2018-08-01 00:00:00.000
Furniture
10900


2018
9
26628
-4963
2018-09-01 00:00:00.000
Furniture
11000


2018
10
31615
3093
2018-10-01 00:00:00.000
Furniture
11100


2018
11
48086
11619
2018-11-01 00:00:00.000
Furniture
11300


2018
12
37579
5284
NULL
NULL
NULL


2019
1
61439
9760
NULL
NULL
NULL


2019
2
38424
5917
NULL
NULL
NULL


2019
3
58937
10077
NULL
NULL
NULL




Fiddle
"
Key error when trying to fill nan values of a extremely large dataset,"In this line you remove customer_ID from the dataframe.
features_num = [x for x in df._get_numeric_data().columns.values if x not in ['customer_ID', 'target']]
df = df[features_num].fillna(NAN_VALUE) 

And then in the next line you try to use customer_ID.
df_out = df.groupby(['customer_ID']).tail(1).reset_index(drop=True)

"
How to update tensorflow properly on kaggle?,"First, update the Tensorflow version in Kaggle kernel by
!pip install -U tensorflow==2.3.1

then, update GPU version in GPU cluster :
print(""update GPU server TensorFlow version..."")

!pip install cloud-gpu-client

import tensorflow as tf 

from cloud_gpu_client import Client

print(tf.__version__)

Client().configure_tpu_version(tf.__version__, restart_type='ifNeeded')

press CTL+SHIFT+P and select restart kernel
Kaggle itself has guides explaining the usage of these libraries on their kernel.
However, consider this sufficient for now.
"
"Try to replace the nan values by pandas , but Error: Columns must be same length as key","You are close, need specify column Age for replace missing values:
train_inf['Age']=train_inf['Age'].fillna(train_inf['Age'].median())

"
is there a way to reduce ram memory consumption for my python code,"If your data does not fit into your RAM, use a data pipeline. It works like this:

Read a single batch of data from disk into memory
Process that batch (e.g. normalize/scale/crop/...)
Feed that batch to the model and perform optimization step
Repeat from 1. until all samples in the dataset have been used.

Apply the steps for the training set first to run training for one epoch. After that you can iterate your validation set in the same way and subsequently continue with the second training epoch.
If you train your model using a GPU, the data loading and preprocessing can be handled by the CPU during the time it takes for the GPU to complete the optimization step. This provides a massive speed up as well.
There are packages in PyTorch and Tensorflow to make this really easy and even apply multiprocessing to speed up the whole routine.
Check this guide by tensorflow for the basics and this guide to optimize the pipeline.
PyTorch has something similar here
"
Import csv from Kaggle url into a pandas DataFrame,"
You can automate kaggle.cli
follow the instructions to download and save kaggle.json for authentication https://github.com/Kaggle/kaggle-api

import kaggle.cli
import sys
import pandas as pd
from pathlib import Path
from zipfile import ZipFile

# download data set
# https://www.kaggle.com/unsdsn/world-happiness?select=2017.csv
dataset = ""unsdsn/world-happiness""
sys.argv = [sys.argv[0]] + f""datasets download {dataset}"".split("" "")
kaggle.cli.main()

zfile = ZipFile(f""{dataset.split('/')[1]}.zip"")

dfs = {f.filename:pd.read_csv(zfile.open(f)) for f in zfile.infolist() }

dfs[""2017.csv""]

"
Is there any package required for HTML widgets error?,"There is a package called htmlwidgets.
You are missing the package. You are probably trying to knit the output to a HTML file.
Just add the library to the top of your markdown code, don't forget to install it once first.
CRAN: htmlwidgets package
install.packages(""htmlwidgets"")

Then just add it to the top of your markdown code the library as well like so:
library(htmlwidgets)

"
Download dataset directly from Kaggle to GoogleColab,"Colab doesn't remember where you are between commands. Try putting all the commands on a single line using &&. This might not work exactly for you since I don't have your exact environment.
!mkdir ~/.kaggle/ && cp kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json

"
name ImageList is not defined in kaggle website,"If you are getting this error even after defining the ImageList, Run the default first cell in the kaggle or make sure that U have run all the cells.
Even I faced the same issue during my initial practice on kaggle.
"
Is there a way to use Kaggle notebooks on the go?,"Step1. Install Docker
Follow the link to install docker in your machine
https://docs.docker.com/engine/install
Step2. Download the relevant Docker-file/Docker image.Here in this case
use docker pull kaggle/python
Step3: Launch docker container from the folder where you have the notebook using the command below.
docker run -v $PWD:/src -p 8888:8888 --rm -it kaggle/python jupyter notebook --no-browser --ip=""0.0.0.0"" --notebook-dir=/src
Step4: Copy the url from the terminal and paste it in your browser.
The url looks something like this http://127.0.0.1:8888/?token=xxxxxxxxx
"
Downloading partial data from Kaggle,"One way you can do it by clicking on the file one by one and click the download button on the top right of the file.

One more way you can do it download everything and delete the files you dont want.
Or you can use wget in command prompt or curl in terminal and use --max-filesize to limit it to the amount of bites you want.
"
What do KeyErrors means and how can I resolve them?,"The large majority of the time, a Python KeyError is raised because a key is not found in a dictionary or a dictionary subclass
--
check train_df test_df data-frame has column named 'Cabin' or not.
Here is an example,
import re
import pandas as pd

test_df = pd.read_csv(""test.csv"")
train_df = pd.read_csv(""train.csv"")

deck = {""A"": 1, ""B"": 2, ""C"": 3, ""D"": 4, ""E"": 5, ""F"": 6, ""G"": 7, ""U"": 8}
data = [train_df, test_df]

for dataset in data:
    dataset['Cabin'] = dataset['Cabin'].fillna(""U0"")
    dataset['Deck'] = dataset['Cabin'].map(
        lambda x: re.compile(""([a-zA-Z]+)"").search(x).group())
    dataset['Deck'] = dataset['Deck'].map(deck)
    dataset['Deck'] = dataset['Deck'].fillna(0)
    dataset['Deck'] = dataset['Deck'].astype(int)

train_df = train_df.drop(['Cabin'], axis=1)
test_df = test_df.drop(['Cabin'], axis=1)
print(train_df, test_df)

training/test files downloaded from here.
"
How to train categorical CNN?,"I am the creator of the data set you  are using. You really do not need much image augmentation as there are 35215 training images, 1250 test images(5 per species) and 1250 validation images(5 per species). So at the most I would only use horizontal_flip=True. All the rest will contribute little and increase processing time. This is a super clean data set where the bird region of interest is at least 50% of the pixels in the image.
In your train_gen you should have target_size=(128,128) and class_mode='categorical'. Also you have save_format='jpg'. This parameter is ignored when you do not specify save_to_dir. It is good you did not specify it as when you train you would fill up that directory with TONS of images.
In your model change input_shape=(150, 150, 3). In the code below I have added two callbacks early_stop, and rlronp . The first monitors validation loss and will halt training if the loss fails to decrease after 4 consecutive epochs. It saves the model with the weights for the epoch having lowest validation loss. The second monitors validation loss and reduces the learning rate by a factor of .5 if at the end of an epoch the loss failed to decrease. Documentation is here. Working code is shown below:
model.compile(Adam(lr=.001), loss='categorical_crossentropy', metrics=['accuracy']) 
train_dir=r'c:\temp\birds\train' # change this to point to your directory
valid_dir=r'c:\temp\birds\valid' # change this to point to your directory
test_dir=r'c:\temp\birds\test'   # change this to point to your directory
train_gen=ImageDataGenerator(rescale=1/255, horizontal_flip=True).flow_from_directory( train_dir, target_size=(150, 150),
                            batch_size=32, seed=123,  class_mode='categorical', color_mode='rgb',shuffle=True) 
valid_gen=ImageDataGenerator(rescale=1/255).flow_from_directory( valid_dir, target_size=(150, 150),
                            batch_size=32, seed=123,  class_mode='categorical', color_mode='rgb',shuffle=False)
test_gen=ImageDataGenerator(rescale=1/255).flow_from_directory( test_dir, target_size=(150, 150),
                            batch_size=32, seed=123,  class_mode='categorical', color_mode='rgb',shuffle=False) 
early_stop=tf.keras.callbacks.EarlyStopping( monitor=""val_loss"", patience=4, verbose=1, restore_best_weights=True)
rlronp=tf.keras.callbacks.ReduceLROnPlateau(monitor=""val_loss"", factor=0.5,  patience=1, verbose=1)    
history=model.fit(x=train_gen,  epochs=30, verbose=1, callbacks=[early_stop, rlronp],  validation_data=valid_gen,
                       validation_steps=None,  shuffle=True)
performance=model.evaluate( test_gen, batch_size=32, verbose=1, steps=None, )[1] * 100
print('Model accuracy on test set is ', performance, ' %')

With 250 classes your model will not achieve to high a value of accuracy. The more classes there are the more difficult the problem gets. I would create a more complex model with more convolutional layers and perhaps an additional dense layer. If you add an additional dense layer include a dropout layer to prevent over fitting.
"
How do you use GPU and TPU in kaggle?,"You need to specifically use the packages like https://rapids.ai/ to move data from CPU to GPU. I am assuming you using something like pandas to do data operations.
"
How can I sort data by bins using groupby in pandas?,"The simplest way to do this is use the first part of your code and simply make the last digit of the release_year a 0. Then you can .groupby decades and get the most popular genres for each decade i.e. the mode:
input:
import pandas as pd
import numpy as np
df = pd.DataFrame({
'show_id':['81145628','80117401','70234439'],
'type':['Movie','Movie','TV Show'],
'title':['Norm of the North: King Sized Adventure',
'Jandino: Whatever it Takes',
'Transformers Prime'],
'director':['Richard Finn, Tim Maltby',np.nan,np.nan],
'cast':['Alan Marriott, Andrew Toth, Brian Dobson',
'Jandino Asporaat','Peter Cullen, Sumalee Montano, Frank Welker'], 
'country':['United States, India, South Korea, China',
'United Kingdom','United States'], 
'date_added':['September 9, 2019',
'September 9, 2016',
'September 8, 2018'],
'release_year':['2019','2016','2013'],
'rating':['TV-PG','TV-MA','TV-Y7-FV'],
'duration':['90 min','94 min','1 Season'],
'listed_in':['Children & Family Movies, Comedies',
'Stand-Up Comedy','Kids TV'],
'description':['Before planning an awesome wedding for his',
'Jandino Asporaat riffs on the challenges of ra',
'With the help of three human allies, the Autob']})

code:
df['release_year'] = df['release_year'].astype(str).str[0:3] + '0'
df = df.groupby('release_year', as_index=False)['listed_in'].apply(lambda x: x.mode().iloc[0])
df

output:
    release_year  listed_in
0   2010          Children & Family Movies, Comedies

"
Error message in Kaggle when reading in csv file,"It means your CSV file has a column that consists of several different dtypes.
Just add low_memory=False to your read_csv function, and the warning message would be gone.
data = pd.read_csv('/kaggle/input/covidtracking/all-states-history.csv', low_memory=False, index_col=0, parse_dates=True)

But the problem is still there!
A better solution is to specify the column type. For example in your case, Columns (2):
data = pd.read_csv('/kaggle/input/covidtracking/all-states-history.csv', dtype={2: str}, index_col=0, parse_dates=True)

"
how to save preprocessed image in kaggle output directory?,"use something like this :
!zip-folder --auto-root --outfile /kaggle/working/dataset.zip /kaggle/working/
"
Pandas mean pivot table contains NaN values even though data was filled prior to aggregating,"This is an indication that there are no values that correspond to those combinations (i.e. no females in cabin T or males in cabin G).
You should be able to confirm with train[train.Cabin == 'T'].Sex.value_counts()
Here's a toy example to illustrate following the form of your data:
dat = pd.DataFrame({'sex': list('mfmfmfm'), 
                    'survived': [1, 0, 0, 1, 1, 0, 1], 
                    'cabin': ['a', 'a', 'b', 'b', 'c', 'c', 'd']})
dat
dat.pivot_table(values = 'survived', index='sex', columns = 'cabin', aggfunc=np.mean)

"
"Tensorflow: CNN which has 99% test accuracy,always wrong at prediction","The problem is you normalize the input image on train (x_train / 255) but when you predict you use the raw image directly.
Here
data = img.reshape(-1,IMG_SIZE,IMG_SIZE,1)
    
model_out = model.predict([data])

Should be
data = img.reshape(-1,IMG_SIZE,IMG_SIZE,1)
data = data / 255
model_out = model.predict([data])

This should give result relatively close to when you train.
But your network looks very small so if accuracy still not satisfying you should add some more CNN layers.
"
&#39;Unknown layer: Functional&#39; when I load a model,"The ResNet50 model contains a number of repeating blocks. I am not familiar with Colab, but based on the error you received, my guess is that the block of layers is implemented as custom layer and you should have its implementation defined and declared in custom_objects dictionary
"
ValueError: could not convert string to float: &#39;.&#39; .For Glove dataset,"I have the same problem and I print the content of the entry for '.'. Apparently, there is more than one word vector for '.'. The first one has 300 dimensions and the second one has 302 dimensions with two dots in position 0 and 1 (hence the error). I don't know why there are two vectors for '.' or why there are dots in the vector.
.   300 ['0.012001', '0.20751', ...]

.   302 ['.', '.', '-0.1573', '-0.29517', ...]

"
How to fix mallet on gensim,"I spent hours trying to solve it.
Tried this solution and nothing worked.
Looking to a previous sucessfull call I made to LDA Mallet, I noticed some parameters were not being set, then I made it like this:
gensim.models.wrappers.LdaMallet(mallet_path=mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word, prefix='temp_file_', workers=4)
I really hope it helps you. Finding a solution to this problem is a pain.
"
How to solve Error when checking input: with incorrect size,"you specified wrong the input dimension of your model. try to define your first layer in this way
model.add(layers.Dense(64, activation='relu', input_shape=(train_data.shape[1],)))

"
What is the equivalent of sklearn&#39;s `random_state` for XGBoost?,"For reference, the issue was not so much with XGBoost but with the data splitting. Thanks Venkatachalam for pointing it out!
I was splitting the data with train_test_split without setting the random_state! 
Fixed as below:
X_train_full, X_valid_full, y_train, y_valid = train_test_split(X_full, y,
  train_size=0.8, test_size = 0.2, random_state=1)

"
Best Practice for Kaggle Datasets with Colab,"There are 2 approaches that are both convenient:
1) Save your kaggle.json in GDrive. Then mount by just clicking (in the left pane). Then, copy it here.
!mkdir -p ~/.kaggle
!cp ""drive/My Drive/kaggle.json"" ~/.kaggle/
# the rest is the same

2) Embed the kaggle.json in Colab itself.
!mkdir ~/.kaggle
!echo '{""username"":""korakot"",""key"":""8db2xxx""}' > ~/.kaggle/kaggle.json
# the rest is the same

If you are worried, use the first which is more secure. 
If you are lazy, use the second.
"
Is there a way to automatically download output files in Kaggle Kernels or upload them to GoogleDrive?,"If we assume that all your data resides in a run/ folder in the Kaggle working directory, then you could zip the folder and download:
!pip install zip_files
!zip-folder --auto-root --outfile /kaggle/working/run.zip /kaggle/working/run 

"
Plotly Heatmap not compatible with Jupyter Notebooks?,"It is likely to be a mismatch between the version of Plotly on kaggle and on your laptop.
Please, update the version on your laptop by:
pip install -U plotly

"
Size issue with embedding layer in Keras,"As a follow-up, I've lifted this code into a different environment (Google Colab) and it's working there. Therefore, there must be a bug in the original runtime environment (Kaggle) and nothing wrong with the code itself.
"
How to import python classes from another directory,"Make sure you have 
__init__.py  

file exists in the directory you want to import classes from .  
"
"Crawler code run successfully locally in PyCharm, but have error in Notebook","Kaggle Notebooks won't allow internet connections unless you activate it.
You have to log in and set the internet activated in the right menu.
The process is described in this other question kaggle kernels: urllib.request.urlopen not working for any url
"
how to use celebA dataset with DCGAN,"Having a look at Keras sample will help.
(I know it is late but might be useful for next visitor)
"
ValueError: could not convert string to float: &#39;horse&#39;,"You can convert your category labels to numbers and then make a new column for those numbers. scikit-learn has a builtin for this but its easy enough without that:
import pandas as pd
df = pd.DataFrame({'label':['cat','dog','horse'],'b':[1,2,3]})
all_labels= df.label.unique().tolist()
all_labels.sort()
label_to_number={label:all_labels.index(label) for label in all_labels}
df['label_num']=df.apply(lambda r:label_to_number[r.label],axis=1)

Now you can send label_number to your training (y_col='label_number') .  This is all assuming that integer cateories are ok and you don't need 'one-hot encoding' - if you do then again scikit has provision for that. From here it seems like the integer categories are fine however.
"
proble while commiting in kaggle,"Pay attention to the markdowns you wrote. Sometimes you have addressed a file that cannot be loaded
"
Kaggle Dataset Download,"Scroll down on the page for dataset
You should see a download button on the upper right hand side of the dataset file preview
"
How to get datasets into kaggle,"This video shows you how to upload data in a kernel. Unless your data is publicly accessible you can't currently create a data from its URL. 
You can create datasets from URLs that point directly at a file. Sometimes you may see errors if there's a mismatch between the HTML metadata and what Kaggle looks for. 
"
"Solving Titanic(Kaggle) Dataset using RFC, Unknown label type: &#39;unknown&#39;","The result of train_test_split() is X_train, X_test, y_train, y_test and you are assigning values in a different order in your code.
Change the line:
trainX,trainY,valX,valY = train_test_split(X,y,random_state = 1)  

By this one:
trainX,valX,trainY,valY = train_test_split(X,y,random_state = 1)  

and you will be able to fit the model with the correct parameters.
"
Pandas sort_values,"I am not sure you've realized that each line corresponds to a different Employee. So when you do df = df[['Year','JobTitle','OvertimePay']].copy(), there are multiple occurrences of ""Deputy Sheriff"" in the same year, one for each employee. This can happen several times, because there are different employees with the same ""JobTitle"". 
In order to achieve what you want, you could drop_duplicates and get only the high paid employees for each ""Job Title"" in a ""Year"". However, I advise you to analyze if this is really what you are looking for.
Here is the code I would use:
import numpy as np
import pandas as pd

df = pd.read_csv('Salaries.csv')
df['OvertimePay'] = df['OvertimePay'].replace(""Not Provided"",np.nan).astype(float)
df = df[['Year','JobTitle','OvertimePay']].copy()
df.drop_duplicates(subset=['Year','JobTitle'])
df2 = df.sort_values('OvertimePay', ascending= False)

EDIT : To change the format I would use something like:
print(df2.iloc[0:20,].to_string(header=['Year','JobTitle',''],index=False,justify='left',
                                formatters={'JobTitle':'{{:<{}s}}'.format(df2['JobTitle'].str.len().max()).format}))

"
Troubles querying public BigQuery data using local workstation,"IN Colab - you need to first authenticate. 
from google.colab import auth
auth.authenticate_user()

That will authenticate your user account to a project.
"
opening a dataset through URL on WEKA,"Because you have to invoke a dataset into your URL
I mean : 
http://storm.cis.fordham.edu/~gweiss/data-mining/weka-data/xxx
xxx = Then simply add the filename at the end of this string (e.g., ""iris.arff"")
Try this, both on a web page and Weka to understand what is required to work properly
cheers
"
Where is kaggle_data？I could not find such a library,"It looks like that module was written by a the author. It's in the the same directory as the notebook in the GitHub repo: https://github.com/leriomaggio/deep-learning-keras-tensorflow/tree/master/2.%20Deep%20Learning%20Frameworks
"
How do I add GCS credentials to tensorflow?,"I am not experienced using Kraggle and I do not really understand what limits do you want to apply on the service account, but you can follow the next steps to determine a service account access for Google Cloud Storage while using TensorFlow:  

Follow this guide to implement GCS custom FileSystem in Tensorflow.
Check the Python client library to instantiate the client.
The service account permissions required for storage are listed here.
To grant roles to a service account, follow this guide. 
Check the snippet in Federico's post here, based on this documentation, to implement the service account in your Python code. 

Snippet: 
from google.oauth2 import service_account

SERVICE_ACCOUNT_FILE = 'service.json'
credentials = service_account.Credentials.from_service_account_file(
        SERVICE_ACCOUNT_FILE)

"
data.plot.bar() Mixing up index labels,"I was able to identify what is happening here if it happens to help anyone else in the future.  What it happening is its creating the index based on the 1st thing it sees, so if you notice that the 1st row in the data above would be classified as Mid traffic based on the cut points I have, it is naming the 1st column in the graph mid.  If i reorder the data before importing to train so that low is 1st, it orders the index correctly.  I am sure there is also a way to code that in the line, if anyone happens to know that feel free to respond, but at least I know why its happening.
"
Uploading data from GoogleColab into GitHub or Kaggle,"Since your question seems to be open ended, I'll answer it as best as I could. If you are thinking of a more specific kind of data, do add a concrete example.
One way is to simple use git command line tool.
github_repo = example_repo
file_to_sync = 'some_path/some_file'
!git clone https://github.com/user/$github_repo
!cp $file_to_sync $github_repo
!cd $github_repo && git add $file_to_sync && git commit -m ""file added""
!cd $github_repo && git push

"
Cannot use fillna when a condition is introduced - SyntaxError: invalid syntax,"Try this code:
for i in range (0,len(df.columns)):
    x=df.columns[i]
    df[df[x]<0]=np.nan

"
"DNN, targets[3] is out of range error","According to the documentation tf.nn.in_top_k expects tensor of type float32 with shape batch_size x classes. In you case you have 2 classes, so predictions tensor should have shape batch_size x 2.
"
correctly import Kaggle dataset into R,"I've written a quick kernel to double check the data. It reads in fine with base R, but does throw some warnings when you try to read it in as a tibble. It still seems to load in more or less fine, though.
As a note, some of your read.csv() arguments are just the default for that argument (like quote) so I just omitted them. The very bottom example uses your exact syntax and works fine, though, so I'm not sure where you're running into trouble.
"
"Cannot feed value of shape (64, 7) for Tensor &#39;targets/Y:0&#39;, which has shape &#39;(?,)&#39;","When you set to_one_hot to True in the regression function, it already converts your target to one-hot labels. So, it expects a value with shape (?,) and you should just provide the original data, y_train and y_val, to the fit function.
model.fit({'input': X_train}, {'targets': y_train}, n_epoch=6, batch_size=64, 
    validation_set=({'input': X_val}, {'targets': y_val}),
    snapshot_step=500, show_metric=True, run_id=MODEL_NAME)

"
Incorrect ROC score for Kaggle competition?,"You are using different testing data than would have been available - use just the test.csv file to find the best model and value for C, then evaluate it only on the impermium_verification_set.csv. When the competition was running, looks like only test was available to find a model, then models were locked and leaderboard was based on the verification set. You are using the full set of both to select the best model.
You can always ask on the discussion boards on the Kaggle competition page if you want - I'm sure people there will help also. Also some of the top placers, including the winner, have posted their code on the discussion page for interest.
"
Parsing issues bringing CSVs from Kaggle into pandas dataframe,"I figured this out. The problem is that even though the download result on the wget referred to the file as .csv, it was actually a zip that was downloaded. 
mv london-street.csv london-street.zip

unzip london-street.zip

There's the actual csv:
# ls
london-street.csv
london-street.zip

pulled it in with 
london_street = pd.read_csv('london-street.csv')
london_street

And was able to see the data with no problem.
"
Python Pandas: creating a dataframe using a function for one of the fields,"To convert the data type of a Series, you can use astype() function, this should work:
def didSurvive(sex):
    return (sex == ""female"").astype(int)

"
logistic regression classifier for prediction in Python,"You might do it like this:
alg = sklearn.linear_model.LogisticRegression()
alg.fit(x_train, y_train)
test_score = alg.score(x_test, y_test)

You should read the sklearn docs logistic regression and cross validation, which are very good and provide more sophisticated methods for validating your models. This tutorial for the Kaggle Titanic competition might also be useful.
"
Plots not showing in ggplot2 with geom_bar,"There are a few issues here:
First you should remove the rows where age is NA, otherwise you can't create a sequence.
train<-train[!is.na(train$Age),]

Then you should change your y value to train$Survived (why did you use length(train$PassengerId)? - it doesn't display anything)
The thing that @Pascal mentioned is also correct: You have to put + geom_bar(stat=""identity"", bin=breaks) outside.
and you need to add the axes and title differently in ggplot.
This is the complete working code:
train<-train[!is.na(train$Age),]

breaks <- seq(min(train$Age), max(train$Age), 10)
p <- ggplot(train, aes(x=train$Age, y=train$Survived), 
            fill = Survived)+ geom_bar(stat=""identity"", bin=breaks)
p <- p+labs(x=""age"", y=""count"")

p <- p+theme(plot.title= element_text(""survival""))


print(p)

Results in this graph:

"
Split the data into training and test sets,"if Data holds all your Datasets (independents and dependants) in a numpy array:
in this way
Data=([[1, 2, 3, 430],[...]...]) 

3 independents and one dependent
you can set the indices for slicing like that:
test_ind=int(Data.shape[0]*0.3)
train_ind=Data.shape[0]-test_ind

Data[:train_ind,:4] would be your training data
Data[train_ind:,:4] would be your test data
"
How to finish code to replace NA with median in R,"Or maybe this tidyverse one-liner
agedata %>% group_by(title) %>% mutate(age=ifelse(is.na(age), median(age, na.rm=TRUE), age))

"
Cannot download file to AWS Lambda,"As pointed out by @joran the import is trying to create some config directories
    config_dir = os.environ.get('KAGGLE_CONFIG_DIR') or os.path.join(
        expanduser('~'), '.kaggle')
    if not os.path.exists(config_dir):
        os.makedirs(config_dir)

You can set environment variables Using AWS Lambda environment variables or in this case you can point this directly to /tmp/  because that's the only directory available for you to write anything.
the corresponding code
From the documentation, it seems like you just need config credentials which you can put in AWS Parameter Store and fetch them inside your lambda.
Kaggale API Credentials
export KAGGLE_USERNAME=datadinosaur
export KAGGLE_KEY=xxxxxxxxxxxxxx

Once you fetched and exported the credentials then you can add the import statement for the API.
Or if you are adventurous enough can modify the code a bit and try to create a Configuration class object eventually use that in the initialization
        self.username = """"
        # Password for HTTP basic authentication
        self.password = """"


Configuration Class
"
How to approach a machine learning programming competition,"So, I had never heard of Kaggle until reading your post--thank you so much, it looks awesome.  Upon exploring their site, I found a portion that will guide you well.  On the competitions page (click all competitions), you see Digit Recognizer and Facial Keypoints Detection, both of which are competitions, but are there for educational purposes, tutorials are provided (tutorial isn't available for the facial keypoints detection yet, as the competition is in its infancy.  In addition to the general forums, competitions have forums also, which I imagine is very helpful.
If you're interesting in the mathematical foundations of machine learning, and are relatively new to it, may I suggest Bayesian Reasoning and Machine Learning.  It's no cakewalk, but it's much friendlier than its counterparts, without a loss of rigor.
EDIT:
I found the tutorials page on Kaggle, which seems to be a summary of all of their tutorials.  Additionally, scikit-learn, a python library, offers a ton of descriptions/explanations of machine learning algorithms.
"
Kaggle API in colab datasets `!kaggle datasets list` error,"I am encountering this problem as well. I noticed that if I set the use this call
kaggle datasets list --min-size 1

It will work. Note you will need version 1.5.6. I had 1.5.4 on a Colab instance and that version didn’t support that argument.
The problem seems to be bigquery/crypto-litecoin has no data. As a consequence of this, it looks like totalBytes is None in Dataset.
I've opened an issue on github and will created a PR. If you want a temporary work around, you can grab the file from my fork. You can use your traceback to determine where to put the file. Or alternatively, just use --min-size 1 so it will ignore the case when there are no data files.
"
Kaggle .csv file submission error,"Try adding index=False while saving csv like so:
output.to_csv(""output.csv"", index=False)
"
How to efficiently extract numbers from text in a data.table column in R,"Without using stringr, you could just use sub with "".*?(\\d+)[kgKG].*"" and back reference:
s = ""Tostado 210g CU BIM 1182""

sub("".*?(\\d+)[kgKG].*"", ""\\1"", s)
# [1] ""210""


use (\\d+)[kgKG] to match digits followed by k, K, g, G letters;
specify .* before and after the pattern so that strings other than the pattern can be removed;
use ? on the first .* to make the match unready so that all the three digits will be kept;
use \\1 to refer the capture group (\\d+);

"
How do I select the same features in my test data that I selected in my train data?,"RFE has a transform method, so you can do something like this (after fitting the selector to your training data):
X_test_selected = rfe.transform(X_test)

"
Import own Python module on Kaggle,"You do not need to pip install the utility file. Rather, just include the utilities_x_ray.py file in the same directory that you run the python notebook.
The python notebook looks for files within the same directory whenever importing as well as those modules that were installed via pip.
"
Bigquery maximum processing data size allowance?,"So Kaggle by default sets a 1GB limit on requests (to prevent your monthly quota of 5TB to run out). This is what causes this to happen. To prevent this, you can override it by using the max_gb_scanned parameter like this:
df = bq_assistant.query_to_pandas_safe(QUERY, max_gb_scanned = N)

where N is the amount of data processed by your query, or any number higher than it. 
"
Not able to change gender data to binary values,"That's because you din't save the modifications of your dataframe with that line :
train['Sex'].replace(['female', 'male'], [0, 1])

Try to replace it by this :
train['sex'] = train['Sex'].replace(['female', 'male'], [0, 1])

Same for train['Embarked'].
Update
You don't need to do it for train['Age'], the fillna already modify the existant dataframe with the inplace=true.
"
kaggle: Charts plotted using Altair visualization library did not show up after commit,"For Altair 2.2 you can use the notebook renderer to make altair charts render:
import altair as alt
alt.renderers.enable('notebook')

Note, however, that these charts will only display when the kernel is live, not when view of the kernel statically (e.g. when sharing a kernel).
In the master branch of Altair, we have just merged a kaggle renderer, and once this makes it into a release (Altair 2.3 or newer) you will be able to use
alt.renderers.enable('kaggle')

and then plots will show in both live and static views of kernels. I have an example of this in action here: https://www.kaggle.com/jakevdp/altair-kaggle-renderer-test
"
Scikit-Learn accuracy score does not show accuracy,"You need to print the results of the accuracy_score(val_y, val_predictions) line.
e.g. print(accuracy_score(val_y, val_predictions))
"
What are training and test data sets,"In ML, the Original data set is divided into training and test set (sometime cross-validation set as well).
Training set:
    The data set you use to fit the parameters for your algorithm.
Test set:
    The data set to evaluate how accurate your parameters for the algorithms is.
The training set, test set split is usually 80%,20% or 70%,30% respectively. 
It is advised to have the original data set randomized before making the split.
Always remember, in ML the error will always be lower on the data set that was used to fit the parameters. Never evaluate your algorithm using the training set.
"
Python [[0]] meaning,"I dont think the problem is with the syntax, your Dataframe just does not contain the index you are looking for.
For me this works:
In [1]: data = pd.DataFrame({0:[1,2,3], 1:[4,5,6], 2:[7,8,9]})
In [2]: data[[0]]
Out[2]: 
   0
0  1
1  2
2  3

I think what confuses you about the [[0]] syntax is that the squared brackets are used in python for two completely different things, and the [[0]] statement uses both:
A. [] is used to create a list. In the above example [0] creates a list with the single element 0.
B. [] is also used to access an element from a list (or dict,...). So data[0] returns the 0.-th element of data.
The next confusion thing is that while the usual python lists are indexed by numbers (eg. data[4] is the 4. element of data), Pandas Dataframes can be indexed by lists. This is syntactic sugar to easily access multiple columns of the dataframe at once.
So in my example from above, to get column 0 and 1 you can do:
In [3]: data[[0, 1]]
Out[3]: 
   0  1
0  1  4
1  2  5
2  3  6

Here the inner [0, 1] creates a list with the elements 0 and 1. The outer [   ] retrieve the columns of the dataframe by using the inner list as an index.
For more readability look at this, its the exact same:
In [4]: l = [0, 1]

In [5]: data[l]
Out[5]: 
   0  1
0  1  4
1  2  5
2  3  6

If you only want the first column (column 0) you get this:
In [6]: data[[0]]
Out[6]: 
   0
0  1
1  2
2  3

Which is exactly what you were looking for.
"
"Python 3.+, Scipy Stats Mode function gives Type Error unorderable types: str() &gt; float()","This is because you have mixed types in df.Embarked. Make sure all items are the same type (or types that can be compared).
Or use Series.mode(), which can handle mixed types.
"
How can i import data from kaggle while not downloading it?,"1st method
you can head into the dataset page and click on new notebook at the top right corner

2nd method
open any notebook and click on add data at the right menue

3rd method
not recommended, since each person that opens the ipynb will have to upload their own token first.
when using colab first download your kaggke json key and do the following steps

create a kaggle key, you can go into your settings then head into the account tab and you can find a create new token button in the api section, click on it to download your token.

install the kaggle library

! pip install  kaggle


upload your kaggle key or you can put the following code to upload the key

from google.colab import files
files.upload()


now you can download your dataset using the following code

!kaggle datasets download -d [user/data-name]

in our example here [user/data-name] is kukuroo3/body-performance-data

"
How to use both gpus in kaggle for training in pytorch?,"Using multiple GPUs is specific to machine learning libraries. I stumbled upon the same problem while doing image segmentation in Pytorch. The solution is to use the module torch.nn.DataParallel() with the model. The given code can be changed as follows:
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model = torch.nn.DataParallel(model, device_ids = [0,1]).to(device)

here, the device_ids is the index of GPUs. Suppose if you have 4 GPUs then it would be device_ids = [0,1,2,3] or whatever the index it maybe.
And the result of
using both GPUs is here!.
PS: This is my first post in the prestigious stack overflow, please do share your comments and views.
"
Upload Kaggle dataset into AWS S3 without downloading in my computer first,"It's probably easiest to launch an Amazon EC2 instance (a t3.nano is fine) and install that toolset.
Then, download the data to the EC2 instance and upload it to an Amazon S3 bucket.
You should assign an IAM Role to the instance with permission to access your S3 bucket.
"
How frequently the Kaggle Stackoverflow BigQuery dataset is Updated?,"Though the metadata says different, I found from the tables data that they are updating the dataset quarterly.
The most updated data can be found from in the stack exchange data explorer.


"
Why ! is used before pip command?,"In some python interactive shells like ipython or kaggle ! is prefixed to run external shell command.
"
Add Google Cloud Services (GCS) to kaggle notebook,"1) Check which identity you are using to log in. 
2) In the Google Cloud Console window select the project that you have credit for. 
3) If you are using the CLI, set up the identity:
gcloud auth login

4) Setup the correct project:
gcloud config set core/project PROJECT_ID

"
how do I add Kaggle dataset into elasticsearch?,"In order to upload a CSV file to elasticsearch:  

download the file. 


use logstash in order to read the file using file input
modify and transform the data as you need using logstash's CSV filter
output logstash to elasticsearch


For your follow up question - how can I host Elasticsearch - you can either run it by your own, in AWS EC2 for example, or use a managed service like Elastic cloud or AWS ES. good luck
"
UnicodeDecodeError: &#39;utf-8&#39; codec can&#39;t decode byte 0xa0 in position 24: invalid start byte,"I could read the file using encoding='ansi'. See this question for some info on ANSI encoding.
My solution:
import pandas as pd
df = pd.read_csv('crime.csv', encoding='ansi')

Update: If you are getting the error LookupError: unknown encoding: ansi use encoding='cp1252'.
"
Filling Null values with respective mean,"You can use
1] transform and lambda function
In [41]: df.groupby('Pclass')['Age'].transform(lambda x: x.fillna(x.mean()))
Out[41]:
0    22.0
1    38.0
2    26.0
3    35.0
4    35.0
5    22.4
6    54.0
7     2.0
8    27.0
9    14.0
Name: Age, dtype: float64

Or use 
2] fillna over mean
In [46]: df['Age'].fillna(df.groupby('Pclass')['Age'].transform('mean'))
Out[46]:
0    22.0
1    38.0
2    26.0
3    35.0
4    35.0
5    22.4
6    54.0
7     2.0
8    27.0
9    14.0
Name: Age, dtype: float64

Or use
3] loc to replace null values
In [47]: df.loc[df['Age'].isnull(), 'Age'] = df.groupby('Pclass')['Age'].transform('mean')

In [48]: df
Out[48]:
    Age  Pclass
0  22.0       3
1  38.0       1
2  26.0       3
3  35.0       1
4  35.0       3
5  22.4       3
6  54.0       1
7   2.0       3
8  27.0       3
9  14.0       2

"
How to deal with data when making a decision tree,"
For DT you need numerical data to be numerical, categorical - to be in dummies-style. No scaling is needed for numerical columns.
To process categorical data use one-hot encoding. Please be sure that before one-hot encoding you have rather big amounts of each feature (>= 5%), otherwise group small variables.
And consider other model. DT are good but it's old school and they are easy to be overfitted.

"
How to implement next_batch() function for custom data in python,"Apart from generating a batch, you may also want to randomly re-arrange data for each batch.
EPOCH = 100
BATCH_SIZE = 128
TRAIN_DATASIZE,_,_,_ = X_train.shape
PERIOD = TRAIN_DATASIZE/BATCH_SIZE #Number of iterations for each epoch

for e in range(EPOCH):
    idxs = numpy.random.permutation(TRAIN_DATASIZE) #shuffled ordering
    X_random = X_train[idxs]
    Y_random = Y_train[idxs]
    for i in range(PERIOD):
        batch_X = X_random[i * BATCH_SIZE:(i+1) * BATCH_SIZE]
        batch_Y = Y_random[i * BATCH_SIZE:(i+1) * BATCH_SIZE]
        sess.run(train,feed_dict = {X: batch_X, Y:batch_Y})

"
Getting HTML elements via XPath in bash,"
Getting HTML elements via XPath in bash   

from html file (with not valid xml)
One possibility may be to use xsltproc. (I hope it is available for MAC). xsltproc has an option --html to use html as input. But with that you need 
to have a xslt stylesheet. 
<xsl:stylesheet 
    xmlns:xsl=""http://www.w3.org/1999/XSL/Transform"" version=""1.0"">
  <xsl:output method=""text"" /> 

  <xsl:template match=""/*"">
    <xsl:value-of  select=""//*[@id='competitions-table']/tr[205]/td[1]/div/a/@href"" />
  </xsl:template>

</xsl:stylesheet>

Notice that the xapht is changed. There is no tbodyin the input file.
Call xsltproc:
xsltproc --html  test.xsl competitions.html 2> /dev/null

Where the xslproc complaining about errors in html is ignored  ( send to /devn/null ).
The output is: /c/R 
To use different xpath expression from command line you may use a xslt template and replace the __xpath__. 
E.g. xslt template:
<xsl:stylesheet 
    xmlns:xsl=""http://www.w3.org/1999/XSL/Transform"" version=""1.0"">
  <xsl:output method=""text"" /> 
  <xsl:template match=""/*"">
    <xsl:value-of  select=""__xpaht__"" />
  </xsl:template>
</xsl:stylesheet>

And use (e.g) sed for the replacement.  
 sed -e ""s,__xpaht__,//*[@id='competitions-table']/tr[205]/td[1]/div/a/@href,"" test.xslt.tmpl > test.xsl
 xsltproc --html  test.xsl competitions.html 2> /dev/null

"
Turning hexadecimal representation of code segment back to binary,"First you need to strip off the address numbers since they're not part of the code itself; they're like line numbers for hex code. I'd use awk for that. Then try using xxd -r -p again.

Awk syntax stolen from: Using awk to print all columns from the nth to the last

Try something like this (I don't have xxd handy so I couldn't test):
awk '{$1=""""; print $0}' yourhexfile |xxd -r -p >aFileContainingActualCode

"
Logistic Regression return only 0 on test but accuracy is high,"First of all, it is not unexpected that your accuracy is very high, even though you only predict 0. This is because the majority of your training data consists of data that is labeled with 0 (no heart disease). So if the model always predicts 0, it is doing ""fairly well"".
For how to fix the issue. You could lower the threshold or experiment with different combinations of epochs and learning rate. But know that you are not fair off.
Using the sklearn implementation of Logistic Regression, I get fairly similar results:
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from collections import Counter

model = LogisticRegression()
model.fit(train_x, train_y)
pred_y = model.predict(test_x)
print(Counter(pred_y))
print(accuracy_score(test_y, pred_y))

returns
Counter({0: 722, 1: 10})
0.8415300546448088

"
Function turns Values to NaN unwanted,"A shorter version of your code could be:
df['Age'] = df['Age'].fillna(df.groupby(['Sex', 'Pclass'])['Age'].transform('median'))

Compute the median Age per (Sex, Pclass) group and broadcast values to all rows with transform. Finally fill nan values with the computed value previously only and only if Age is null.
"
"Could not import torch_geometric, it says &quot;undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs&quot;","This issue is mentionned at  https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html :

undefined symbol: make_function_schema: This issue signals (1) a version conflict
between your installed PyTorch version and the ${TORCH} version
specified to install the extension packages, or (2) a version conflict
between the installed CUDA version of PyTorch and the ${CUDA} version
specified to install the extension packages. Please verify that your
PyTorch version and its CUDA version match with your installation
command:

python -c ""import torch; print(torch.__version__)""
python -c ""import torch; print(torch.version.cuda)""
nvcc --version


For re-installation, ensure that you do not run into any caching issues by
using the pip --force-reinstall --no-cache-dir flags. In addition, the
pip --verbose option may help to track down any issues during
installation. If you still do not find any success in installation,
please try to install the extension packages from source.

So, I would try these commands, and re-install all or part of the packages into a fresh environment.
"
How to download kaggle dataset?,"You can try this one using Colab
Step 1:
from google.colab import files

files.upload() # <<-- Upload your Kaggle API-Token 

! mkdir ~/.kaggle # <<-- Create a Kaggle folder inside Colab environment 

! cp kaggle.json ~/.kaggle/  # <<- Gets your Token into it

! chmod 600 ~/.kaggle/kaggle.json  # <<- Gives Colab permissions

! kaggle competitions download -c <competition_name>

file_path = '/content/<competition_file_name>'

After that, you'll get a .zip file, so...
Step 2:
from zipfile import ZipFile

with ZipFile(file_path, 'r') as unzip:

    unzip.printdir() # <<-- Check inside zip file

    unzip.extract('train.csv') # <<-- Extract a single file

    #unzip.extractall() # <<-- Extract all files at once
    

You can also use a submission command to upload your files straight to Kaggle servers
! kaggle competitions submit <competition_name> -f <submission_file> -m ""My submission message""

Have fun
"
How to solve duplicate date values in Time Series Analysis?,"There is no way (or it is highly unlikely) that actual good data would have values like this. You mentioned that this is from a Kaggle competition - I doubt they would leave these kind of things to any ambiguity.
What I'm thinking is that you didn't read the dataset carefully. Maybe it's the same date but for different variables? For example, maybe they measured your values on the same date, but in different areas?
You may want to check your other columns before jumping to conclusions.
"
Convert folder of images with labels in CSV file into a tensorflow Dataset,"It is possible but making it correct the first time will be managed it easy when application process or adjusting to the current process is because correcting logic later is sometimes a massive task.
Adjust the label and target directory variable fields targeted to your observation.
[ Sample ]:
import tensorflow as tf
import tensorflow_io as tfio

import pandas as pd

import matplotlib.pyplot as plt

""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
Variables
""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
# list_label_actual = [ 'Pikaploy', 'Pikaploy', 'Pikaploy', 'Pikaploy', 'Pikaploy', 'Candidt Kibt', 'Candidt Kibt', 'Candidt Kibt', 'Candidt Kibt', 'Candidt Kibt' ]
list_label_actual = [ 'Candidt Kibt', 'Pikaploy' ]

""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
: Dataset
""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
variables = pd.read_excel('F:\\temp\\Python\\excel\\Book 7.xlsx', index_col=None, header=[0])

list_label = [ ]
list_Image = [ ]
list_file_actual = [ ]

for Index, Image, Label in variables.values:
    print( Label )
    list_label.append( Label )
    
    image = tf.io.read_file( Image )
    image = tfio.experimental.image.decode_tiff(image, index=0)
    list_file_actual.append(image)
    image = tf.image.resize(image, [32,32], method='nearest')
    list_Image.append(image)


list_label = tf.cast( list_label, dtype=tf.int32 )
list_label = tf.constant( list_label, shape=( 33, 1, 1 ) )
list_Image = tf.cast( list_Image, dtype=tf.int32 )
list_Image = tf.constant( list_Image, shape=( 33, 1, 32, 32, 4 ) )

dataset = tf.data.Dataset.from_tensor_slices(( list_Image, list_label ))
list_Image = tf.constant( list_Image, shape=( 33, 32, 32, 4) ).numpy()

""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
: Model Initialize
""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
model = tf.keras.models.Sequential([
    tf.keras.layers.InputLayer(input_shape=( 32, 32, 4 )),
    tf.keras.layers.Normalization(mean=3., variance=2.),
    tf.keras.layers.Normalization(mean=4., variance=6.),
    # tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),
    # tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Dense(256, activation='relu'),
    # tf.keras.layers.Reshape((256, 225)),
    tf.keras.layers.Reshape((256, 32 * 32)),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(196, return_sequences=True, return_state=False)),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(196)),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(192, activation='relu'),
    tf.keras.layers.Dense(2),
])

""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
: Callback
""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
class custom_callback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs={}):
        if( logs['accuracy'] >= 0.97 ):
            self.model.stop_training = True
    
custom_callback = custom_callback()

""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
: Optimizer
""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
optimizer = tf.keras.optimizers.Nadam(
    learning_rate=0.000001, beta_1=0.9, beta_2=0.999, epsilon=1e-07,
    name='Nadam'
)

""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
: Loss Fn
""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""                               
lossfn = tf.keras.losses.SparseCategoricalCrossentropy(
    from_logits=False,
    reduction=tf.keras.losses.Reduction.AUTO,
    name='sparse_categorical_crossentropy'
)

""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
: Model Summary
""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
model.compile(optimizer=optimizer, loss=lossfn, metrics=['accuracy'] )

""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
: Training
""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
history = model.fit( dataset, batch_size=100, epochs=50, callbacks=[custom_callback] )

plt.figure(figsize=(6,6))
plt.title(""Actors recognitions"")
for i in range(len(list_Image)):
    img = tf.keras.preprocessing.image.array_to_img(
        list_Image[i],
        data_format=None,
        scale=True
    )
    img_array = tf.keras.preprocessing.image.img_to_array(img)
    img_array = tf.expand_dims(img_array, 0)
    predictions = model.predict(img_array)
    score = tf.nn.softmax(predictions[0])
    plt.subplot(6, 6, i + 1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(list_file_actual[i])
    plt.xlabel(str(round(score[tf.math.argmax(score).numpy()].numpy(), 2)) + "":"" +  str(list_label_actual[tf.math.argmax(score)]))
    
plt.show()

input('...')

[ Output ]:
Sample
"
KNeighborsRegressor as denoising algorithm,"It is a supervised learning algorithm - that is the best answer,
normally the algorithm is first trained with known data and it tries to interpret a function that best represents that data such that a new point can be produced for a previously unseen input.
Put simply it will determine the point for a previously unseen value based on an average of the k nearest points for which it has previously seen, a better, more detailed answer can be found below:
https://towardsdatascience.com/the-basics-knn-for-classification-and-regression-c1e8a6c955
in the kaggle code:
the time vector is:
df.index.values[:, np.newaxis]

and the signal vector is:
df.iloc[:, 0]

it appears the person in kaggle is using the data to first train the network - see below:
## define the KNN network
clf = KNeighborsRegressor(n_neighbors=100, weights='uniform')
## train the network 
clf.fit(df.index.values[:, np.newaxis], 
        df.iloc[:, 0])

giving him a function that represents the relationship between time and the signal value. With this he then passes the time vector back to the network to get it to reproduce the signal.
y_pred = clf.predict(df.index.values[:, np.newaxis])

this new signal will represent the model's best interpretation of the signal, as you can see from the the link I have posted above, you can adjust certain parameters which will result in a 'cleaner' signal but also could degrade the original signal
One thing to note is that using this method in the same way as that guy in kaggle means it would only work for that one signal since the input is time it cannot be used to interpret future values:
y_pred = clf.predict(df.index.values[:, np.newaxis] + 400000)
ax = pd.Series(df.iloc[:, 0]).plot(color='lightgray')
pd.Series(y_pred).plot(color='black', ax=ax, figsize=(12, 8))


"
ModuleNotFoundError: No module named &#39;maraboupy.MarabouCore&#39;,"I don't know the details, but I've got maraboupy working in a different set-up.
From the CMake Error it seems that you are running the installation commands in '/kaggle/working' if so, try running the commands in '/kaggle/working/Marabou' this is where my CMakeLists.txt is at least.
Another difference I can spot is that I added Marabou to path as well, don't know if that is important. In your case I suppose this would be:
!git clone https://github.com/NeuralNetworkVerification/Marabou.git
import sys
sys.path.insert(1, '/kaggle/working/Marabou/maraboupy')
sys.path.insert(2, '/kaggle/working/Marabou')

"
How to install the &quot;Tree Ensemble Layer&quot; on Kaggle Notebook,"turn on internet support first and clone the google-research repo from github:
!git clone https://github.com/google-research/google-research.git

then we need the compiling and linking options for g++ so run following code snippets:
import tensorflow as tf; 
print("" "".join(tf.sysconfig.get_compile_flags()))

and
import tensorflow as tf; 
print("" "".join(tf.sysconfig.get_link_flags()))

for my notebook I got following flags:
-I/opt/conda/lib/python3.7/site-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0
-L/opt/conda/lib/python3.7/site-packages/tensorflow -l:libtensorflow_framework.so.2

after that just replace the varialbes ${TF_CFLAGS[@]} and ${TF_LFLAGS[@]} with above outputs
!g++ -std=c++11 -shared google-research/tf_trees/neural_trees_ops.cc google-research/tf_trees/neural_trees_kernels.cc google-research/tf_trees/neural_trees_helpers.cc -o google-research/tf_trees/neural_trees_ops.so -fPIC -I/opt/conda/lib/python3.7/site-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0 -L/opt/conda/lib/python3.7/site-packages/tensorflow -l:libtensorflow_framework.so.2 -O2

at last we need to add system path
import sys
sys.path.insert(1, '/kaggle/working/google-research')

and run your snippet
from tensorflow import keras
from tf_trees import TEL

tree_layer = TEL(output_logits_dim=2, trees_num=10, depth=3)

model = keras.Sequential()
model.add(keras.layers.BatchNormalization())
model.add(tree_layer)

"
How can I calculate percentage of a groupby column and sort it by descending order?,"This doesn't address rows where there are multiple countries in the ""country"" field, but the lines below should work for the other parts of the question:
Create initial dataframe:
df = pd.DataFrame({
'show_id':['81145628','80117401','70234439'],
'type':['Movie','Movie','TV Show'],
'title':['Norm of the North: King Sized Adventure',
'Jandino: Whatever it Takes',
'Transformers Prime'],
'director':['Richard Finn, Tim Maltby',0,0],
'cast':['Alan Marriott, Andrew Toth, Brian Dobson',
'Jandino Asporaat','Peter Cullen, Sumalee Montano, Frank Welker'], 
'country':['United States, India, South Korea, China',
'United Kingdom','United States'], 
'date_added':['September 9, 2019',
'September 9, 2016',
'September 8, 2018'],
'release_year':['2019','2016','2013'],
'rating':['TV-PG','TV-MA','TV-Y7-FV'],
'duration':['90 min','94 min','1 Season'],
'listed_in':['Children & Family Movies, Comedies',
'Stand-Up Comedy','Kids TV'],
'description':['Before planning an awesome wedding for his',
'Jandino Asporaat riffs on the challenges of ra',
'With the help of three human allies, the Autob']})

Groupby country:
df2 = df.groupby(by=""country"", as_index=False)['show_id']\
    .agg('count')

Rename agg column:
df2 = df2.rename(columns={'show_id':'count'})

Create percentage column:
df2['percent'] = (df2['count']*100)/df2['count'].sum()

Sort descending:
df2 = df2.sort_values(by='percent', ascending=False)

Part of the issue in your Attempt #1 may have been that you didn't include the ""by"" parameter in your groupby function.
"
Why does my Ml CNN Kaggle Cats / Dogs prection only spit out an error?,"The Conv2D layer expects inputs of shape (batch_size, x, y, depth). Your X_train is being reshaped to only have size (batch_size, x*y) which is not what the Conv2D expects.
It may work to just take out this reshape: X_train = np.reshape(X_train,(19998,10000)). If not, you could reshape to (19998, 100, 100, 1).
"
Unable import &quot;cuxfilter&quot; package in Kaggle Notebook environment,"RAPIDS 0.15 release does not support CUDA10.0.
Please update your CUDA package or use RAPIDS 0.14 release
Command to install cuxfilter 0.14 using conda :
conda install -c rapidsai -c nvidia -c conda-forge -c defaults cuxfilter=0.14 python=3.7 cudatoolkit=10.0

"
Stuck with a For Loop,"Several things can be fixed regarding your code.
First, we will put the common element of all the if/elifs in a single if:
import math

for i in range(len(database)): 
    if math.isnan(database['Age'][i]) == True:
        if database['Prefix'][i] == ' Capt.' or database['Prefix'][i] == ' Col.':
            database['Age'] = 65.0
        elif database['Prefix'][i] == ' Sir.' or database['Prefix'][i] == ' Major.' or database['Prefix'][i] == ' Rev.' or database['Prefix'][i] == ' Lady.' or database['Prefix'][i] == ' Dr.':
            database['Age'] = 47.5
        elif database['Prefix'][i] == ' Don.' or database['Prefix'][i] == ' Jonkheer.' or database['Prefix'][i] == ' Mrs.' or database['Prefix'][i] == ' the Countess.':
            database['Age'] = 36.5
        elif database['Prefix'][i] == ' Mr.' or database['Prefix'][i] == ' Ms.':
            database['Age'] = 29.0
        elif database['Prefix'][i] == ' Mme.' or database['Prefix'][i] == ' Mlle.':
            database['Age'] = 24.0
        elif database['Prefix'][i] == ' Miss.':
            database['Age'] = 21.0
        elif database['Prefix'][i] == ' Master.':
            database['Age'] = 3.5

Then we will get rid of all the database[""Prefix""][i] checks by saving that into a variable, and use the in operator to avoid many prefix == ""something"" or prefix == ""something else"".
for i in range(len(database)): 
    if math.isnan(database['Age'][i]) == True:
        prefix = database['Prefix'][i] 
        if prefix in (' Capt.', ' Col.'):
            database['Age'] = 65.0
        elif prefix in (' Sir.', ' Major.', ' Rev.', ' Lady.', ' Dr.'):
            database['Age'] = 47.5
        elif prefix in (' Don.', ' Jonkheer.', ' Mrs.', ' the Countess.'):
            database['Age'] = 36.5
        elif prefix (' Mr.', ' Ms.'):
            database['Age'] = 29.0
        elif prefix (' Mme.', ' Mlle.'):
            database['Age'] = 24.0
        elif prefix == ' Miss.':
            database['Age'] = 21.0
        elif prefix == ' Master.':
            database['Age'] = 3.5

Then, notice that you where modifying database[""Age""] instead of database[""Age""][i] so we'll fix that too.
for i in range(len(database)): 
    if math.isnan(database['Age'][i]) == True:
        prefix = database['Prefix'][i] 
        if   prefix in (' Capt.', ' Col.'):                                age = 65.0
        elif prefix in (' Sir.', ' Major.', ' Rev.', ' Lady.', ' Dr.'):    age = 47.5
        elif prefix in (' Don.', ' Jonkheer.', ' Mrs.', ' the Countess.'): age = 36.5
        elif prefix (' Mr.', ' Ms.'):                                      age = 29.0
        elif prefix (' Mme.', ' Mlle.'):                                   age = 24.0
        elif prefix == ' Miss.':                                           age = 21.0
        elif prefix == ' Master.':                                         age = 3.5
        database['Age'][i] = age

Finally, if you wanted, you could write yourself a dictionary that matches prefixes with ages and use that to avoid the many if and elifs.
# Define how an age is matched with some prefixes.
ages_and_prefixes = ((65.0, (""Capt"", ""Col"")),
                     (47.5, (""Sir"", ""Major"", ""Rev"", ""Lady"", ""Dr"")),
                     (36.5, (""Don"", ""Jonkheer"", ""Mrs"", ""the Countess"")),
                     (29.0, (""Mr"", ""Ms"")),
                     (24.0, (""Mme"", ""Mlle"")),
                     (21.0, (""Miss"",)),
                     (3.5,  (""Master"",))
                    )

prefix_to_age_dict = {}
for data in ages_and_prefixes:
    age = data[0]
    prefixes = data[1]
    for prefix in prefixes:
        prefix_to_age_dict[prefix] = age

# The replacement step in the database is now much simpler.
for i in range(len(database)):
    if math.isnan(database['Age'][i]):
        prefix = "" "" + database['Prefix'][i] + "".""
        age = prefix_to_age_dict[prefix]
        database['Age'][i] = age

"
How to upgrade Scipy in Kaggle notebooks,"Kaggle kernels are interactive sessions running in a Docker container with pre-installed packages.
Try changing them using the settings option in the kernel editor then clicking the packages option you can customize packages.
This can be done by accessing the “Settings” tab in the Notebook editor. Once you upgrade the package under Settings-->Docker-->you will options ""Original"" and ""Latest available"". Select the latest available, then you can use your updated packages.
Next to “Docker image”, there is a dropdown menu containing all Docker images you’ve created when installing new packages. Pin a specific Docker image for use in a Notebook if there are multiple custom images available. You can select one of the images to be used persistently within that Notebook.
Hope it will help
"
Shape of output change during TPU training Keras funcional API,"Just want to mention that i get a similar error.
It makes me believe that there must be a bug in tensorflow.(also getting ""16"" as first output shape)
I already gave ""batch_size"" a ""1"" on the predict function just to make shure.
Sadly it has no impact at all.
Maybe it receives the batch of 16 from the tpu?
That could lead that self.results is unexpected at this location:
https://github.com/tensorflow/tensorflow/blob/e5bf8de410005de06a7ff5393fafdf832ef1d4ad/tensorflow/python/keras/engine/training_utils.py#L346
But I'm currently clueless of how to fix it without patching tensorflow itself.
EDIT:
Thats damn difficult to debug ... currently i think the error is somewhere nearby this:
https://github.com/tensorflow/tensorflow/blob/e5bf8de410005de06a7ff5393fafdf832ef1d4ad/tensorflow/python/keras/engine/training_v2_utils.py#L262
Since it somehow handles the output shape based on ""num_replicas_in_sync"".
But can't realy figure it out without debugging.
Some Debuginfo with pdb:
(Pdb) b /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py:262
Breakpoint 7 at /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py:262
(Pdb) c
> /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py(262)_aggregate_predict_results()
-> nested_outs = batch_outs[i * num_replicas:i * num_replicas + num_replicas]
(Pdb) print(len(batch_outs))
8
(Pdb) print(batch_outs[0].shape)
(2, 7, 355, 235, 1)

The ""2"" is already wrong for me at the beginning, not shure why this is happening.(it later gets changed to 16, leading to ""batch_outs"" with multiple duplicates)
Edit again (28.04.2020):
The problem seems to happen earlier:
(Pdb) b /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py:128
Breakpoint 3 at /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py:128
(Pdb) c
> /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py(128)run_one_epoch()
-> batch_outs = execution_function(iterator)
(Pdb) n
> /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py(159)run_one_epoch()
-> if mode != ModeKeys.PREDICT:
(Pdb) print(len(batch_outs))
16
(Pdb) print(batch_outs[14].shape)
(2, 7, 355, 235, 1)
(Pdb) np.array_equal(batch_outs[14],batch_outs[15])
True

Edit again (29.04.2020):
As a workaround alteast the following seems to work:
  from tensorflow.python.tpu import device_assignment as device_assignment_lib
  tf.keras.backend.set_floatx('float32')
  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection
  tf.config.experimental_connect_to_cluster(tpu)
  topology = tf.tpu.experimental.initialize_tpu_system(tpu)
  device_assignment = device_assignment_lib.DeviceAssignment(
          topology, core_assignment=device_assignment_lib.
          SINGLE_CORE_ASSIGNMENT)
  strategy = tf.distribute.experimental.TPUStrategy(tpu,device_assignment)

Of course thats not the best fix, since that way you are only using one core.
Atleast it confirms my guess that tensorflow is not correctly filtering out the results from all cores.
"
One-Hot Encoding Question - Concept and Solution to My Problem (Kaggle Dataset),"
So my first question is, when it comes to one - hot encoding, shouldn't NAs just be treated like any other category within a particular column? 

NA's are just the absence of data, and so you can loosely think of rows with NA's as being incomplete. You may find yourself dealing with a dataset where NAs occur in half of the rows, and will require some clever feature engineering to compensate for this. Think about it this way: if one hot encoding is a simple way to represent binary state (e.g. is_male, salary_is_less_than_100000, etc...), then what does NaN/null mean? You have a bit of a Schrodinger's cat on your hands there. You're generally safe to drop NA's so long as it doesn't mangle your dataset size. The amount of data loss you're willing to handle is entirely situation-based (it's probably fine for a practice exercise).

And second question is, if i want to remove these NAs, what's the most efficient way? I tried imputation, but it looks like that only works for numbers? 

May I suggest this.
"
"Deep Learning model not accurately predicting , Keras?","In the classifier.compile you use the loss='binary_crossentropy' that is used only where the labels are binary (only two classes). When you have multiclass classification you must use the appropriate loss function based on the numbers and types of your labels (i.e. 'sparse_categorical_crossentropy').
Try to read this useful blog post that explains every loss function in details. 
"
How to add legends in this context?,"You need to set year as a factor each time (or externally), not just once. I don't have your data, so I'll use mtcars.
library(ggplot2)
library(dplyr)

# first plot
mtcars %>%
  ggplot(aes(factor(carb), disp, fill=carb)) +
  geom_bar(stat=""identity"")

# second plot
mutate(mtcars, carb = factor(carb)) %>%
  ggplot(aes(carb, disp, fill=carb)) +
  geom_bar(stat=""identity"")

# alternate code for second plot, not shown
mtcars %>%
  ggplot(aes(factor(carb), disp, fill=factor(carb))) +
  # both     ^^^^^^        and        ^^^^^^
  geom_bar(stat=""identity"")


(There are numerous ways to convert to a factor. I'm using dplyr here, but it can easily be done in base or data.table.)
I included the ""alternate"" code above that shows the manual factor being applied to each use of carb; this is not the preferred method in my mind, since if you're doing it multiple times, just do it once before the plotting and use it multiple times. If you need both the ordinal year and the numeric version, you can add a new field, such as ordinal_year=factor(year).
"
Cannot unzip .7z files to google colab,"It seems like your file train.csv.7z is stored inside /content but you try to uncompress ~/content/train.csv.7z which evaluates to /root/content/train.csv.7z for you seem to be the root user and ~ gets evaluated to the user's home directory.
Try:
!p7zip -d /content/train.csv.7z


Maybe os.getcwd() will not be the same as !pwd. When !ls shows the file it seems to be in your cwd.
Try:
!p7zip -d train.csv.7z

"
Getting Substring after certain char in python,"Please use str.partition instead of str.split. This is robust, since you can always expect 3 items, unlike, split which maybe tricky to handle if the input string doesn't have the split character, 
>>> word = '0011165.jpg_Fish'
>>> not_required, split_char, required = word.partition('_')
>>> required
'Fish'

"
Using LSTM for large text,"There are some things you could try to  speed things up:
1. Use CUDNN version of LSTM
It is usually faster, check available layers here keras.layers.CuDNNLSTM is what you are after.
2. Use Conv1d to create features
You can use 1 dimensional convolution with kernel_size specifying how many words should be taken into account and stride specifying the jump of moving window. For kernel_size=3 and stride=3, padding=""SAME"" it would drop your dimensionality three times.
You may stack more convolutional layers.
On top of that you can still employ LSTM normally.
3. Drop LSTM altogether
You may go with 1d convolutions and pooling for classification, RNNs are not the only way.
On the upside: you will not encounter vanishing gradients (could be mitigated a little by Bidirectional LSTM as well).
On the downside: you will lose strict dependence between words, though it shouldn't be much of a problem for binary classification (I suppose it's your goal).
"
I am having an import error while trying to load the module pynrrd in Kaggle,"You probably didn't allow internet from settings
On the right side of the kernel

Click settings
Toggle Internet to ""on""

You'll find also an option to install packages from settings
The import should be done like this
import nrrd

"
How do I use h2o4gpu in kaggle kernel?,"The h2o4gpu package for R requires the h2o4gpu python package to be installed as well. From the docs:

At this point, you should have installed the H2O4GPU Python package successfully.

The error you are experiencing looks like it matches the description of a missing python package, described under ""Python issues"" here: https://github.com/h2oai/h2o4gpu/tree/master/src/interface_r 
If you want to use h2o4gpu from within a Kaggle kernel, I'm not sure if this is possible, since Kaggle currently disables external packages when using a GPU.
"
Why would the LM Prediction function in R add a row to my output,"First, a note: you have to reassign the output of na.omit() to get rid of missing values.
See here:
df <- data.frame(x = c(1, 2, 3), y = c(0, 10, NA))
df
  x  y
1 1  0
2 2 10
3 3 NA
na.omit(df)
  x  y
1 1  0
2 2 10
df
  x  y
1 1  0
2 2 10
3 3 NA

As you can see, the last call to df showed you the initial version including the NAs. You will need to reassign using df <- na.omit(df). 

The actual issue:
As pointed out by @42 in the comments, using formulas correctly will resolve this issue, i.e. you will not have this error message any longer. You will however have a different one. First, let me show you:
#read in the data
testdf <- read.csv(""test.csv"")
train <- read.csv(""train.csv"")

# run initial model, and run model as suggested by 42
model_original <- lm(train$SalePrice ~ train$LotArea * train$GarageArea * factor(train$FullBath) * train$YearBuilt * factor(train$OverallQual))

mod_42 <- lm(SalePrice ~ LotArea * GarageArea * factor(FullBath) * YearBuilt * factor(OverallQual), data = train)

Now, let us run predictions:
prediction <- data.frame(predict(model_original, testdf))
Warning messages:
1: 'newdata' had 1459 rows but variables found have 1460 rows 
2: In predict.lm(model_original, testdf) :
  prediction from a rank-deficient fit may be misleading

This led to the same error as you have. Now, let us run the predictions using the second approach:
prediction <- data.frame(predict(mod_42, testdf))
Error in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels) : 
  factor factor(FullBath) has new levels 4

Note that the error message is different now, and points to a more interesting problem. 
"
"Tensorflow and Keras Problem ValueError: Error when checking input: expected dense_9_input to have shape (24,) but got array with shape (0,)","Your input of Keras does not match your numpy array. Figure out the size of your numpy array and change the input shape accordingly.  In case of more than 2 dimensions use a flatten layer before dense layers. 
Remember that the first dimension of your np array will be interpreted as the batch size
"
Python - TypeError: &#39;DataFrame&#39; object is not callable,"Your problem here is only on the syntax. 
In Python you don't want to express print = ('something') because that means that you are assigning a value to a variable called print. Instead you have to do print('something')
I hope it helped
"
Unable to Generate creds for the Drive FUSE library,"Rather than debugging this third-party wrapper, it's simpler to use the built-in Drive FUSE client:
from google.colab import drive
drive.mount('/content/gdrive')

Then, your files will be available, and you can interact with them in the built-in file browser.

"
Kaggle competition submission error : The value &#39;&#39; in the key column &#39;&#39; has already been defined,"This issue was because fullVisitorId was numeric instead of character, so It dropped all the leading zeros. Therefore, using read.csv() with colClases argument or fread() can make it work. 
I left this just because there could be someone else who are having the similar trouble like me
"
ValueError: &quot;metrics can&#39;t handle a mix of binary and continuous targets&quot; with no source,"You are using a DecisionTreeRegressor, which as it says, is a regressor model. The Kaggle Titanic problem is a classification problem. So you should use a DecisionTreeClassifier.
As for why your code is throwing an error, it is because val_y has binary values (0,1) whereas val_predictions has continuous values because you used a Regressor model.
"
Resnet cannot be loaded,"You're getting this error because Kaggle kernels don't (currently) have internet access, so you're not able to fetch things via URL. 
You can add these models to your kernel by adding the relevent datasets (linked below) and then reading them in as you would any other file from the file path ""../input/[name_of_dataset]/[name_of_file]"". (You should replace [name_of_dataset] and [name_of_file] with the actual names of your dataset and desired file, of course. :)

ResNet50 dataset
ResNet34 dataset

Hope that helps!
"
Does the test set need data cleaning in machine learning?,"A common saying for machine learning goes garbage in, garbage out. Often, feature selection and data preprocessing is more important than your model architecture. 
First question:
Yes
Second question:
Since payment_type of 2, 3, 4, 5 all result in 0, why not just keep it simple. Replace all payment types that are not 1 with 0. This will let your model easily correlate 1 to being paid and 0 to not being paid. It also reduces the amount of things your model will have to learn in the future. 
Third question:
If the ""underlying true tip"" is not reflected in the data, then it is simply impossible for your model to learn it. Whether this inaccurate representation of the truth is what we want or not what we want is a decision for you to make. Ideally you would have data that shows the actual tip.
Preprocessing your data is very important and will help your model tremendously. Besides making some changes to your payment_type features, you should also look into normalizing your data, which will help your machine learning algorithm better generalize relations between your data.
"
what are the best methods to classify the user gender based on names?,"You could use character-level embeddings (i.e. your input classes are the different characters, so 'a' is class 1, 'b' is class 2 etc..). One-hot encoding the classes and then passing them through an embedding layer will yield unique representations for each character. A string can then be treated as a character-sequence (or equally a vector-sequence), which can be used as an input for either a recurrent or convolutional network. If you feel like reading, this paper by Kim et al. will provide you all the necessary theoretical backbone.
"
c# Script to login and download from Kaggle,"We have created a forum post to help you accomplish what you wanted to do,  Accessing Kaggle API through C#.  Feel free to post here or on the forum if you have additional questions.
"
Why are Logistic Regression and SVM predictions multiplied by constants at the end?,"This is just a meta-predictor using two sub-predictors (LogReg and SVM).
There are tons of approaches of combining multiple prediction-models and this convex-combination is one of the most simple ones.
The values are probably also trained with some cross-validation approach, leading to these numbers where the SVM-classifier is taken more seriously!
I'm not sure what exactly the task is, but i think the number of classes should be 2 (0 and 1 or -1 and 1; at least in this prediction-step; there might be some outer OvO or OvA scheme) to make sense here.
"
logits and labels must have the same first dimension,"I think the issue is that
training_batches[0][1] 

is a list and not a numpy.array, you should modify create_datasets accordingly...
"
Decision Trees with SKlearn and Visualization,"did  you check: http://scikit-learn.org/stable/modules/tree.html mentions how to plot the tree as PNG image :
 from IPython.display import Image 
 import pydotplus
 dot_data = tree.export_graphviz(my_tree_one, out_file='tree.dot')  
 graph = pydotplus.graph_from_dot_data(dot_data)  `
 Image(graph.create_png())

"
Very slow performance on fitting Keras model in Windows 10,"This is because you have not g++ installed on your system. If you are on Windows, you need to install two things:
1) TDM-GCC : This can be downloaded from here. You need to add the path of this to your environmental variables as well.
2) OpenBLAS : This can be downloaded from here. Add the path to this to your theano flags in .theano file.
"
My rows are mismatched in my SVM scripting code for Kaggle,"#10 items in training set
y <- sample(0:1, 10, T)
x <- rnorm(10)
bestModel <- svm(y~x,kernel = ""linear"", cost = 1)

#Six in test set
prediction <- predict(bestModel, newdata=rnorm(6), type=""response"")

#Output has 10 values (unexpected)
prediction
#           1          2          3          4          5          6       <NA>       <NA> 
#  0.05163974 0.58048905 0.49524846 0.13524885 0.12592718 0.06082822 0.55393256 1.08488424 
#        <NA>       <NA> 
#  0.94836026 0.47679646 

#For correct output, remove names with <NA>
prediction[na.omit(names(prediction))]
#         1          2          3          4          5          6 
#0.05163974 0.58048905 0.49524846 0.13524885 0.12592718 0.06082822 

"
How do you work on an AWS machine in kaggle?,"You need to launch an EC2 instance in AWS and connect it through SSH to run your algorithms. The following link can help you further.
http://www.grant-mckinnon.com/?p=6
"
Python tfidf returning same values regardless of idf,"Since you provided a list containing 1 document, all terms idfs will have an equal 'binary frequency'.
idf is the inverted term frequency over the set of documents (or just inverted document frequency). Most if not all idf formulas only checks for term presence in a document, so it does not matter how many times it appears per document.
Try feeding a list with 3 distinct documents for instance, this way the idfs will not be the same.
"
Indexing using iloc,".iloc[] is the primary method to access row and column index of pandas DataFrames (or Series, in this case index only). It is quite well explained in the Indexing docs. 
In this specific case, from the scikit-learn docs:

KFold divides all the samples in k groups of samples, called folds
  (if k = n, this is equivalent to the Leave One Out strategy), of equal
  sizes (if possible). The prediction function is learned using k - 1
  folds, and the fold left out is used for test. Example of 2-fold
  cross-validation on a dataset with 4 samples:
import numpy as np
from sklearn.cross_validation import KFold

kf = KFold(4, n_folds=2)
for train, test in kf:
    print(""%s %s"" % (train, test)) 
[2 3] [0 1] [0 1] [2 3]


In other words, KFold picks the index positions, these are used in the for loop over kf and passed to .iloc so that is selects the appropriate row index (and all columns) from the titanic[predictors] DataFrame containing the training set.
"
Infrastructure for running Spark,"It's not that hard to start a standalone cluster on Linux or OS X using the bundled scripts, which could be sufficient if you can work with one node, or each contribute your development computers to a cluster (on the same LAN).
When you need to scale, AWS EMR is pretty simple.
For a little more money, Databricks offers Spark as a managed service. Which means you really don't have to think too much about running the cluster.
"
Updating a notebook in Kaggle,"Based on the information you shared, there is likely an issue with the slug field in your kernel-metadata.json file. This happened because Kaggle ""sanitizes"" URL slugs, changing _ to -.
There are two ways to fix this. Either you can manually edit your kernel-metadata.json by replacing underscore characters with dashes. Or, you can do a kaggle kernels pull.
"
Cuda Error while trying to run oobabooga textgen as an api on kaggle notebooks,"the issue was that pytorch version wasn't compatible with cuda version
"
How to solve &quot;MemoryError&quot; when download dataset by kaggle?,"Note this line in rest.py:
r.data = r.data.decode('utf8')

This is very naive and, for this particular dataset, is quite wrong.
You can decode this dataset with cp037 but to do so you'll need to edit rest.py appropriately
"
Graphs that executed fine in Rstudio aren&#39;t working on Kaggle. Why?,"I run into same problem and just found the solution.
If you used setwd() to set a new directory to read csv file, remember to set the work directory back to the default one by setwd('/kaggle/working')
or please do not change your working directory and just read your csv files from ""../input""
"
RandomForest for Big Data,"Before using Hadoop, let's clear what it provides and do you need that.
Hadoop is a tech stack provides distributed file system (HDFS), resource negotiator for distributed processing (YARN) and MapReduce framework.
It is useful when you want to make your computing distributed and resolve your problem faster using more resources. But it maybe an overkill to create hadoop cluster for your task.
You can use  Spark and RandomForestClassifier. Spark is an inmemory distributed computing engine with good support of ML algorithms.
Spark can be running with Hadoop but it is not a strict requirement. You can run Spark also in Standalone mode or with Mesos or Kubernetes.
"
Upload dataset from google drive to Kaggle,"1 - Get your kaggle json file from Kaggle -> Your Profile -> Account -> API -> CreateNewToken
2 - Create New Colab notebook in drive
3 - Connect your drive to notebook
4 - Copy kaggle.json file and specify path.
import os
os.environ['KAGGLE_CONFIG_DIR'] = ""/content/drive/MyDrive/ColabNotebooks""
# my kaggle.json file in /content/drive/MyDrive/ColabNotebooks

5 - Initiate dataset metadata
!pip install kaggle
!kaggle datasets init -p /content/drive/MyDrive/ColabNotebooks/Resized
# my dataset in Resized folder.

6 - Edit dataset metadata
{
""title"": ""Augmented HardHat-Vest Dataset"",
  ""id"": ""muhammetzahitaydn/augmetned-hardhat-vest-dataset"",
  ""licenses"": [
    {
      ""name"": ""CC0-1.0""
    }
  ]
}

7 - Upload your dataset, In my case my dataset in 'zip' format. I used '--dir-mode skip'. But if your dataset not zipped and you want kaggle zip dataset for you use '--dir-mode zip' argument.
!kaggle datasets create -p /content/drive/MyDrive/ColabNotebooks/Resized --dir-mode skip

dataset uploaded in 4 minutes !
"
Can&#39;t use PySpark DF cached in Google Colab,"It seems like there's not enough space to cache the dataframe in memory! It's an rdd long linkage error due to memory overflow in JVM.
I'm not sure if you can increase memory in google collab, so either use smaller files in google collab, or test locally if you have enough memory.
"
How can I read csv form kaggle,"You have to adapt that path to the downloaded file.
df = pd.read_csv('/kaggle/input/ibm-hr-analytics-attrition-dataset/WA_Fn-UseC_-HR-Employee-Attrition.csv')

Is only an example path. Everyone has to change this path to the location where the downloaded .csv file from their homepage got saved.
The .csv file for download is available here:
https://www.kaggle.com/datasets/pavansubhasht/ibm-hr-analytics-attrition-dataset
"
Display name is empty for tensor flow lite model generated using teachable machine,"The model only outputs as numbers, each of the unique number representing a category.
In the above example, you need to create a map that assigns a Category to display name: {'0': 'Cat', '1': 'dog'}.
"
Download or working with such Large Dataset,"Use distributed system like Apache Spark framework. PySpark and Dask are very efficient to handle big data.
"
How load libraries in Python use VSC,"Hi Miguel :) Remove the %matplotlib inline. Your code should look like:
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
pd.plotting.register_matplotlib_converters()
print('setup complete')

Hope it helps! :)
"
Reading datasets on kaggle vs collab gives different shapes,"I was reading the data like this
df_tr = pd.read_csv('2019-Dec.csv')
df_tr.head()

so when I ran the below step
df_tr.shape

it shows fewer records. when I removed df_tr.head() then it shows the proper count as the one in kaggle.
"
Kaggle Dataset command returning wrong data,"You are not passing the search term to the function call search_kaggle(); but the string kaggle via variable dtsite:
if dtsite == ""kaggle"":
   results = search_kaggle(dtsite)

Change this to:
if dtsite == ""kaggle"":
   results = search_kaggle(searchkey)

"
Unable to import pandas profiling in kaggle noteook,"Even though Kaggle comes with pandas-profiling as default, I would recommend running this command in a cell to use the latest version of the package: %pip install -Uqq pandas-profiling[notebook]
"
Is there a way of installing OptaPy in Kaggle?,"From the error message:
ERROR: Ignored the following versions that require a different python version: 8.11.0a0 Requires-Python >=3.9; 8.11.0a1 Requires-Python >=3.9; 8.11.0a2 Requires-Python >=3.9; 8.14.0a0 Requires-Python >=3.9; 8.16.1a0 Requires-Python >=3.9; 8.17.0a0 Requires-Python >=3.9; 8.19.0a0 Requires-Python >=3.9; 8.19.0a1 Requires-Python >=3.9; 8.21.0a0 Requires-Python >=3.9; 8.23.0a0 Requires-Python >=3.9

From https://www.kaggle.com/general/173536, it seems Kaggle uses Python 3.7 and does not provide a direct way to upgrade. OptaPy requires Python 3.9 or higher (it uses 3.9 syntax in its sources, so 3.8 and below won't work).
"
Caused by error in `as.POSIXlt.character()`: ! character string is not in a standard unambiguous format. How can I change CHR to POSIXCT?,"There are alternatives -- at some point I found having to chase (known) formats to be too repetitive and boring and wrote a package that does it for me (and fast):
> library(anytime)
> datevec <- c(""5/30/2021 11:58"", ""5/30/2021 12:10"", ""5/30/2021 11:29"")
> anytime(datevec)
[1] ""2021-05-30 11:58:00 CDT"" ""2021-05-30 12:10:00 CDT"" ""2021-05-30 11:29:00 CDT""
> 

The example just shows the first three of your (non-reproducibly presented) dates.  The package has other functions too for converting dates, or specific timezones as well as formatters.  Take a look: anytime at CRAN -- and yes it of course also works in pipes and with other packages and whatnot. It ""just"" aims to take care of converting 'any time or date in any format' to POSIXct or Date.
"
Merging in R returns columns full of NA values,"Ok after fighting back and forth I found out the issue is caused by the different date format in the dataframes. The ActivityDate column has no zeros before the month:
ActivityDate = c(""4/12/2016"",
                     ""4/13/2016"", ""4/14/2016"")

SleepDay = c(""04/12/2016"",
                 ""04/13/2016"", ""04/15/2016"")

Formatting both columns as date fixes the issue:
dailyActivity$ActivityDate<- as.Date(dailyActivity$ActivityDate , format = ""%m/%d/%y"")
sleepDay$SleepDay<- as.Date(sleepDay$SleepDay , format = ""%m/%d/%y"")

Thanks a lot for the help!
"
"how to drop rows if a variale is less than x, in sql","We can delete on a CTE and use returning to get the id's of the rows being deleted, but they still exist until the transaction is comitted.

CREATE TABLE t (
    id serial,
    variale int);
insert into t (variale) values
(1),(2),(3),(4),(5);


✓


5 rows affected




with del as
(delete from t 
where variale < 3
returning id)
select 
  t.id,
  t.variale,
  del.id ids_being_deleted
from t
left join del
on t.id = del.id;


id | variale | ids_being_deleted
-: | ------: | ----------------:
 1 |       1 |                 1
 2 |       2 |                 2
 3 |       3 |              null
 4 |       4 |              null
 5 |       5 |              null



select * from t;


id | variale
-: | ------:
 3 |       3
 4 |       4
 5 |       5


db<>fiddle here
"
How to write on output cell of jupyter notebook in vscode,"Do you get the pop up at the top of the screen (Press 'Enter' to confirm or 'Escape' to cancel)? You type your Kaggle username there.


"
How to ensure python binaries are on your path?,"kaggle is python module but it should also install script with the same name kaggle which you can run in console/terminal/powershell/cmd.exe as
kaggle datasets list -s demographics

but this is NOT code which you can run in Python Shell or in Python script.
If you find this script kaggle and open it in editor then you can see it imports main from kaggle.cli and it runs main()
And this can be used in own script as
import sys
from kaggle.cli import main

sys.argv += ['datasets', 'list', '-s', 'demographics']
main()

But this method sends results directly on screen/console and it would need assign own class to sys.stdout to catch this text in variable.
Something like this:
import sys
import kaggle.cli

class Catcher():
    def __init__(self):
        self.text = ''
        
    def write(self, text):
        self.text += text
 
    def close(self):
        pass
    
catcher = Catcher()    

old_stdout = sys.stdout  # keep old stdout
sys.stdout = catcher     # assing new class

sys.argv += ['datasets', 'list', '-s', 'demographics']
result = kaggle.cli.main()

sys.stdout = old_stdout  # assign back old stdout (because it is needed to run correctly `print()`

print(catcher.text)


Digging in source code on script kaggle I see you can do the same using
import kaggle.api

kaggle.api.dataset_list_cli(search='demographics')

but this also send all directly on screen/console.

EDIT:
You can get result as list of special objects which you can later use with for-loop
import kaggle.api

result = kaggle.api.dataset_list(search='demographics')
                                 
for item in result:
    print('title:', item.title)
    print('size:', item.size)
    print('last updated:', item.lastUpdated)
    print('download count:', item.downloadCount)
    print('vote count:', item.voteCount)
    print('usability rating:', item.usabilityRating)
    print('---')                                 

"
How to create a new data frame where existing columns become a new column in R,"Here this is how you can make a quick small example:
participants <- 33

my_dat <- data.frame(
    id = 1:participants,
    VeryActiveMinutes = rnorm(participants, 0, 5),
    FairlyActiveMinutes = rnorm(participants, 0, 5),
    LightlyActiveMinutes = rnorm(participants, 0, 5),
    SedentaryMinutes = rnorm(participants, 0, 5)
)

This format is called ""wide"" you want it ""tidy"" or ""long"". It look like you are using the tidyverse so I used it.
columns <- names(my_dat)[-1] # vector of columns you want to pivot

library(tidyr)

my_dat_tidy <- my_dat %>% 
    tidyr::pivot_longer(cols =  columns,
                        names_to = ""activity levels avg_activity_minutes"",
                        values_to = ""I_do_not_know"")

"
ValueError: could not convert string to float: &#39;$257.26&#39; - sklearn.tree.DecisionTreeClassifier - Python,"For one column:
stock_data = stock_data["" Close/Last""].str.strip("" $"")

A more general solution (applies to every column except for Date and  Volume):
stock_data = stock_data.drop([""Date"", "" Volume""], axis=1).apply(lambda x: x.str.strip("" $""))

Edit:
If you want to keep all the columns:
stock_data.drop([""Date"", "" Volume""], axis=1) = stock_data.drop([""Date"", "" Volume""], axis=1).apply(lambda x: x.str.strip("" $""))

After this, you can drop any column you wish:
stock_data.drop([""Date""], axis=1, inplace=True) 

"
SimpleTransformers &quot;max_seq_length&quot; argument results in CUDA out of memory error in Kaggle and Google Colab,"This happened because max_seq_length defines the number of input neurons for the model thus increasing the number of trainable parameters which will require it to allocate more memory which might exceed your memory limits on those platforms.
Most of the time, max_seq_length is up the dataset, and sometimes adding too much could be wasteful in terms of training time and model size.
What you can do is to find the max number of words per sample in your training dataset and use that as your max_seq_length.
"
does RNN suitable with kaggle Titanic - Machine Learning from Disaster,"Just like you suspected, RNN is not suitable for this kind of problem.
The link you shared offer a way to use RNN to solve the problem but I think using RNN in this case is an over-kill and that other, more simple models might even get you better results.
"
Stop displaying report in fit a model,"You can set the verbosity of xgboost to 0 for silent, 1 for warning, 2 for info and 3 for debug as follows:
import xgboost as xgb
xgb.set_config(verbosity=1)

"
how to submit kaggle competition submission file ? . Please disable internet in the Notebook editor and save a new version,"In some kernel competitions, access to internet ressources are not allowed. You need to disable internet access (there's a slider to do so when you edit the notebook) before commiting again your notebook.
"
My Error: unknown extension ?Pd at position 1,"Where it says (?Pd+), type (?P<temp_num>\d+) instead like they did on this
example.
"
feature_engine package&#39;s modules not getting imported in Kaggle,"Answering my own question after finding the solution.
Basically, I had to upgrade the version of feature_engine
Feature-engine is in active development regularly publishing new or updated transformers. Hence, ran below to upgrade
$ pip install -U feature-engine

In new version (1.0), we need to import categorical encoders from feature_engine.encoding as below
from feature_engine.encoding import OneHotEncoder

The format below is not used anymore :
from feature_engine import categorical_encoders as ce

Official Doc
"
SQL: Variable string input in WHERE clause?,"Use a Stored Procedure to construct (via CONCAT) and then PREPARE and EXECUTE it.
"
VS Code: Failed to find a kernelspec to use for ipykernel launch,"You need to check whether you have installed ipython and ipykernel with the command pip list.
Then try to reinstall or upgrade it with command:
pip install ipython
pip install ipykernel

or
pip install --upgrade ipython
pip install --upgrade ipykernel


"
"Python: BERT Model Pooling Error - mean() received an invalid combination of arguments - got (str, int)","Since one of the 3.X updates, the models return now task-specific output objects (which are dictionaries) instead of plain tuples. You can either force the model to return a tuple by specifying return_dict=False:
o1, _ = self.bert(
            ids,
            attention_mask=mask,
            token_type_ids=token_type_ids,
            return_dict=False)

or by utilizing the basemodeloutputwithpoolingandcrossattentions object:
o = self.bert(
            ids,
            attention_mask=mask,
            token_type_ids=token_type_ids)
#you can view the other attributes with o.keys()
o1 = o.last_hidden_state

"
Kaggle: Dealing with extra unlabelled test data in CNN,"Unless you label the data yourself (by hand) or have another (superior) model at hand, there is not much you can do with the unlabeled ""test"" data.
The idea of test data is to compare the predicted results with the true labels - if you don't have them, you should discard the data from the test set.
"
Multi-Channel Contacts Problem: Find related tickets based on their information,"This is my solution using Python3. I used 2 dictionaries to store ticket connections. The first dictionary stores connected tickets by value. Second dictionary stores each id connection (trace). Finally, calculate contacts using trace from the second dictionary. Yeah, also a very slow way. It requires 3 loops. It takes up to 15 seconds to calculate all 500.000 rows in Kaggle kernel.
Kaggle kernel
# Import tools
import numpy as np
import pandas as pd

# Load data
df = pd.read_json('/kaggle/input/scl-2021-da/contacts.json')
npdata = df.values

# Initialize memory
memory = {}
connections = {}

# Store connected tickets by value
def add_to_memory(ticket_id, value):
    if value != """":
        if value in memory:
            memory[value].add(ticket_id)
            return
        memory[value] = {ticket_id}

for row in npdata:
    ticket_id = row[0]

    # Order Id
    add_to_memory(ticket_id, row[4])

    # Email
    add_to_memory(ticket_id, row[1])

    # Phone
    add_to_memory(ticket_id, row[2])

# Calculate Trace
for ids in memory.values():
    current_connection = set(ids)

    for uid in ids:
        if uid in connections:
            current_connection.update(connections[uid])

    for uid in current_connection:
        connections[uid] = current_connection

# Calculate contacts and add to list
output = []
for ticket_id, trace in sorted(connections.items()):
    contacts = np.sum(npdata[list(trace), 3])
    trace = ""-"".join([str(_id) for _id in sorted(trace)])
    answer = ""{}, {}"".format(trace, contacts)
    output.append({""ticket_id"": ticket_id,  ""ticket_trace/contact"": answer})

# Convert to pandas DataFrame & save to csv
output_df = pd.DataFrame(output)
filename = ""output.csv""
output_df.to_csv(filename, index=False)

Reference
"
multi line v single line for-loop different results,"normalised = (token.strip("",."").lower() for token in tokens)  returns a tuple generator. Let's explore this:
>>> a = [1,2,3]
>>> [x**2 for x in a]
[1, 4, 9]

This is a list comprehension. The multi-line equivalent is:
>>> a = [1,2,3]
>>> b = []
>>> for x in a:
...     b.append(x**2)
...
>>> print(b)
[1, 4, 9]


Using parentheses instead of square brackets does not return a tuple (as one might suspect naively, as I did earlier), but a generator:
>>> a = [1,2,3]
>>> (x**2 for x in a)
<generator object <genexpr> at 0x0000024BD6E33B48>

We can iterate over this object with next:
>>> a = [1,2,3]
>>> b = (x**2 for x in a)
>>> next(b)
1
>>> next(b)
4
>>> next(b)
9
>>> next(b)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
StopIteration

This can be written as a multi-line expression like this:
>>> a = [1,2,3]
>>> def my_iterator(x):
...     for k in x:
...             yield k**2
...
>>> b = my_iterator(a)
>>> next(b)
1
>>> next(b)
4
>>> next(b)
9
>>> next(b)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
StopIteration

In the original example, an in comparison is used. This works for both the list and the generator, but for the generator it only works once:
>>> a = [1,2,3]
>>> b = [x**2 for x in a]
>>> 9 in b
True
>>> 5 in b
False
>>> b = (x**2 for x in a)
>>> 9 in b
True
>>> 9 in b
False

Here is a discussion of the issue with generator reset: Resetting generator object in Python
I hope that clarified the differences between list comprehensions, generators and multi-line loops.
"
Kaggle Dataset - Letter and numbers meaning,"The number 0.00256 is expressed as 2.56E-03 in scientific notation .
What this means is that the number before E (2.56 in this case) is multiplied by 10 raised to the power of -3 , which is 2.56*10^(-3) .
Similarly the number 744.67 is expressed as 7.4467E+02.
For example , your first y value (1.975000E-01) would be 0.1975 since it is 1.975 * 10^(-1) .
Your first x value (4.0E+00) would simply be 4 since it is 4.0*10^(0).
"
Get the name of the zip file from Kaggle dataset,"I don't think this is possible with Kaggle API python CLI but, if the .zip folder is created in the API, there should somewhere be the naming logic.
However, I think it would be easier to just download the dataset into an empty folder, then check .zip files in that folder and use that.
This could be done with pathlib:
from pathlib import Path
import subprocess

# Create Path object and create ""new_folder"" in current working directory (Path.cwd())
path = Path(Path.cwd(), ""new_folder"")
path.mkdir()

# Download
subprocess.run([""kaggle"", ""datasets"", ""download"", ""-d"", DATA_URL, ""-p"", path])

# Get .zip files in the ""new_folder""
zip_files = path.glob("".zip"")
dataset_zip_name = [f.name for f in zip_files][0]


"
How can I download notebook from Kaggle Using Python?,"You can use the Kaggle API to download public kernels.
kaggle kernels pull -k [KERNEL] -p /path/to/download -m
"
Tensorflow input generator ran out of data,"You have specified step_per_epoch incorrectly.
The steps_per_epoch should be equal to
steps_per_epoch = ceil(number_of_samples / batch_size)

For your case
steps_per_epoch = ceil(1161 / 16) = ceil(72.56) = 73

Try specifying steps_per_epoch = 73
As you can your entire data is exhausted in 73 steps. Now, if you specify steps_per_epoch any higher than 73 ie 74
There is no data available. Therefore you get input generator ran out of data
More Information:
Model training comprises of two parts forward pass and backward pass.
1 train step = 1 forward pass + 1 backward pass

A single train step(1 forward pass + 1 backward pass) is calculated on a single batch.
So if you have 100 samples and your batch size is 10.
Your model will have 10 train steps.
Epoch: Epoch is defined as complete iteration over the dataset.
Therefore, for your model to completely iterate over the dataset of 100 samples, it should undergo 10 train steps.
This train step is nothing but steps_per_epoch.
The steps_per_epoch argument is usually specified when you give infinite data generator to your fit() command and does not need to be specified if you have finite data.
"
How to plot Month on the x-axis and Rainfall on the y-axis with Seaborn?,"
Using a seaborn.barplot.
kaggle: india_monthly_rainfall_data.csv
The API doesn't know how to order the months unless you specify the order.

Use pandas.Categorical to order the Month column and use the calendar module to get an ordered list of months, or create a list manually.


See Weather Visualization for Portland, OR: 1940 - 2020
See Plot Daily Max & Min Temp - This could be used for rainfall too.

import pandas as pd
import seaborn as sns
import calendar

# load data
df = pd.read_csv('data/india_monthly_rainfall_data.csv')

# melt
dfm = df.melt(id_vars=[""State"", ""District"",""Year""],  var_name=""Month"",  value_name=""Rainfall"")

# groupby sum
rdf = dfm.groupby(['Month'])['Rainfall'].sum().reset_index()

# ordered
rdf.Month = pd.Categorical(rdf.Month, categories=list(calendar.month_abbr)[1:], ordered=True)

# display(rdf)
Month     Rainfall
  Apr  2.15743e+06
  Aug  1.52092e+07
  Dec  6.44887e+05
  Feb  9.17824e+05
  Jan  7.38981e+05
  Jul  1.65285e+07
  Jun  1.02515e+07
  Mar  1.17930e+06
  May  4.09921e+06
  Nov  1.56662e+06
  Oct  4.33708e+06
  Sep  1.00620e+07

# plot
p = sns.barplot('Month', 'Rainfall', data=rdf)
p.set_ylabel('Rainfall (mm)')
p.set_title(f'1901 - 2002: Total Cumulative Monthly Rainfall')


"
why there is a &quot;.&quot; after the number 3600?,"It specifies the type as float.
If you check
# Python2.7
print(type(3600.))

You will get that it is a float.
Without the period,
# Python2.7
print(type(3600))

You get int.
This changes the type of division you are using: floating point arithmetic or integer arithmetic.
Look at these two examples to see the difference.
# Python2.7
1 / 2 # = 0
1 / 2. # = 0.5

This is the significance of writing 3600., the writer of the code wanted to specify to use floating point division.
"
Pandas Dataframe get trend in column,"Hope this helps!
    #get the average values
    mean_df=df1.groupby(['day','item'])['price'].mean().reset_index()
    #rename columns 
    mean_df.columns=['day','item','average_price']
    #sort by day an item in ascending
    mean_df=mean_df.sort_values(by=['day','item'])
    #shift the price for each item and each day 
    mean_df['shifted_average_price'] = mean_df.groupby(['item'])['average_price'].shift(1)
    #combine with original df 
    df1=pd.merge(df1,mean_df,on=['day','item'])
    #replace the price by difference of previous day's 
    df1['price']=df1['price']-df1['shifted_average_price']
    #drop unwanted columns
    df1.drop(['average_price', 'shifted_average_price'], axis=1, inplace=True)

"
A comparison between model training approaches,"For machine learning, basically you will have to put all the training data altogether so that your model won't miss any of the pattern that your dataset contains. And I would said that there is a big step before you train your model which is Shuffle. Imagining that you was ask to do pure math all the time, and suddenly your teacher put you in front of a geography problem and ask you to solve it, you will be kind of unknowing what to do somehow, machine it's the same! So be sure to use all the data to train your model and if your A--->B--->C [Epoch 1] is a shuffled data, then that's great, or you have to shuffle it.
"
No output when runing csv file in kaggle,"It is working as it is supposed to. You don't see anything because you are not printing.
import pandas as pd

# save filepath to variable for easier access
melbourne_file_path = 'input/melb_data.csv'
# read the data and store data in DataFrame titled melbourne_data
melbourne_data = pd.read_csv(melbourne_file_path)
# print a summary of the data in Melbourne data
print(melbourne_data.describe())

"
Download kaggle package on aws sagemaker notebook,"If you want to download Kaggle datasets in the AWS Sagemaker Notebook instance you can follow the following steps :

Generate API key in Kaggle web UI
Upload it to your root folder (i.e.  /home/ec2-user/SageMaker/ ) via the web interface
In the terminal (of Jupyter lab), use the following commands

pip install kaggle
mkdir /home/ec2-user/.kaggle
mv /home/ec2-user/SageMaker/kaggle.json /home/ec2-user/.kaggle
kaggle datasets download -d  name_of_the_dataset
reference: https://freddiek.github.io/2018/06/10/accessing-Kaggle-from-SageMaker-instance.html
"
What happened when I used pandas to read csv files for multiple time in kaggle&#39;s notebook?,"You must put the CSV file in the folder where python saves projects.
Run this to find out the destination:
%pwd 

Put the file in the destination and run this: 
seeds = pd.read_csv('WNCAATourneySeeds.csv')

You can also run this:
seeds = pd.read_csv(r'C:\Users....\WNCAATourneySeeds.csv')

Where ""C"" is the disk where your file is saved and replace ""..."" by the computer path where the file is saved. Use also ""\"" not ""/"". 
"
kaggle directly download input data from copied kernel,"The notebook you list contains two data sources;

another notebook (https://www.kaggle.com/davidmezzetti/cord-19-analysis-with-sentence-embeddings)
and a dataset (https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge)

You can use Kaggle's API to retrieve a kernel's output:
kaggle kernels output davidmezzetti/cord-19-analysis-with-sentence-embeddings

And to download dataset files:
kaggle datasets download allen-institute-for-ai/CORD-19-research-challenge


"
Can&#39;t access directory Tensorflow Google Colab,"I am assuming this is because of the different file system structure between a normal Linux machine and the runtime hosted by Google Colab.
As a workaround, pass the cache_dir='/content' argument to the get_file function to be as follows: path_data_dir = tf.keras.utils.get_file(origin='https://www.kaggle.com/c/dogs-vs-cats/download/0iMGwZllApFLiU35zX78%2Fversions%2Fm5lLqMS0KLfxJUozn3gR%2Ffiles%2Ftrain.zip',fname='train',untar= True, cache_dir='/content')
Be aware that the returned value path_data_dir is a full path to the file, so the function call os.list_dir(data_dir) will fail since data_dir points to a file and not a directory.
To fix this, change entries = os.listdir(data_dir) to entries = os.listdir(data_dir.parent)
"
How to iterate over a DataFrame for a selected coulmn using python?,"KeyError is encountered in the function since apply() method on a dataframe assumes axis=0. This means that the function will be applied on every column and not every row. To remove this error, the apply() call needs to be replaced as:
_result=_d.apply(lambda x:handling_nan(x), axis=1)


Looking at the edit, the question is to replace NaNs with grouped means in the dataset.
This can be done using fillna() and transform() method as following:

l = [[""M"", 30], [""M"", 45], [""M"", None], [""F"", 76], [""F"", 23], [""F"", None]]
df = pd.DataFrame(l, columns=[""sex"", ""age""])
df['age'] = df['age'].fillna(df.groupby(""sex"")['age'].transform('mean'))


This answer has other alternative solutions.
Hope this helps.
"
MPII pose estimation dataset: visibility flag not present,"Well, it seems like empty is_visible flags are refering to visible, head points instead. I've plotted here these points in green, while visible and not visible are in red and black, respectively. I don't know the reason behind it though.



"
Python - RandomForestClassifier and XGBClassifier have exact same score,"This is true that both algorithms are tree based. However, you can see that you have a single tree in the RandomForestClassifier so you are actually a DecisionTreeClassifier while using an ensemble for the gradient-boosting algorithm. One could expect different results.
Thus the only thing that makes the performance to be equal is actually your data. You have only 2 features which are moreover categorical features. Therefore, with these data, you cannot learn a complex model. All trees should be identical. you could check the number of node in the tree (e.g. my_pipelines[0][-1].estimators_[0].tree_.node_count; I have only 11).
Add 2 additional numerical features (e.g. fare and age) and you will see that the trees can further find additional rules and the performance will then change.
"
Add text over the bars in text of barplot,"After reading Rui's answer,I thought of another solution .
first a function to calculate the ratio of men and women (by continent) and then sapply .
CreaEtiq <- function(conti){
  NumHContin <- dim(Response[Response$GenderSelect==""Male"" & Response$continent==conti,])[1]
  NumMACntin <- dim(Response[Response$GenderSelect==""Female"" & Response$continent==conti,])[1]
  return(round(NumHContin/NumMACntin,2))
}
EtiquetaBarPlot <- sapply(levels(Response$continent),CreaEtiq)

And to finish:
bp_Continent <- barplot(table(Response$continent),
                        main = ""Distribucion de DS por continentes"",
                        ylim = c(0,3500)
)
text(x = bp_Continent, y= table(Response$continent), 
     label = paste(""H/M = "", EtiquetaBarPlot) ,
     pos = 3, cex = 0.8, col = ""red"")

obtaining the following graph

"
How to select which columns are good for visualisation in k-Means clustering algorithm?,"They have 3 features that they can use to cluster. Usually they will just take the euclidean distance of all the features to get the distance from cluster to cluster.
This is very easy to visualize in two dimensions. Take two points and the distance between them is the hypotenuse of a triangle.  In three dimensions, it's a little harder to visualize. The author is simply using 2 dimensions so she can plot it later. However, to use all three dimensions you would simply modify the code to:
X = dataset.iloc[:,[1:3]].values
and that will use age,income and spending score in the algorithm
"
how to move files from kaggle to google drive usig google colab,"I have answered a similar question here,
upload files from kaggle to google drive.
Using code below to generate download link in Kaggle and using wget in Colab I was able get the file in colab. From there it can copied to drive using python or shell command.
from IPython.display import FileLink
FileLink(r'processed_file.zip')

"
How to work on &quot;age bins&quot; in Pandas Dataframe which are saved as string?,"Not entirely sure what output you are looking for. See if the below code & output helps you.
df['Lage'] = df['Ages'].str.split('[-+]').str[0]
df['Uage'] = df['Ages'].str.split('[-+]').str[-1]

or 
df['Lage'] = df['Ages'].str.extract('(\d+)', expand=True) #you don't get the fractions for row 17 & 18
df['Uage'] = df['Ages'].str.split('[-+]').str[-1]

Input
    Ages
0   6-12
1   12+
2   7-12
3   10+
4   5-12
5   8-12
6   4-7
7   4-99
8   4+
9   9-12
10  16+
11  14+
12  9-14
13  7-14
14  8-14
15  6+
16  2-5
17  1½-3
18  1½-5
19  9+
20  5-8
21  10-21
22  8+
23  6-14
24  5+
25  10-16
26  10-14
27  11-16
28  12-16
29  9-16
30  7+

Output1
Ages    Lage    Uage
0   6-12    6   12
1   12+     12  
2   7-12    7   12
3   10+     10  
4   5-12    5   12
5   8-12    8   12
6   4-7     4   7
7   4-99    4   99
8   4+  4   
9   9-12    9   12
10  16+     16  
11  14+     14  
12  9-14    9   14
13  7-14    7   14
14  8-14    8   14
15  6+  6   
16  2-5     2   5
17  1½-3    1½  3
18  1½-5    1½  5
19  9+  9   
20  5-8     5   8
21  10-21   10  21
22  8+  8   
23  6-14    6   14
24  5+  5   
25  10-16   10  16
26  10-14   10  14
27  11-16   11  16
28  12-16   12  16
29  9-16    9   16
30  7+  7   

Output2
Ages    Lage    Uage
0   6-12    6   12
1   12+     12  
2   7-12    7   12
3   10+     10  
4   5-12    5   12
5   8-12    8   12
6   4-7     4   7
7   4-99    4   99
8   4+  4   
9   9-12    9   12
10  16+     16  
11  14+     14  
12  9-14    9   14
13  7-14    7   14
14  8-14    8   14
15  6+  6   
16  2-5     2   5
17  1½-3    1   3
18  1½-5    1   5
19  9+  9   
20  5-8     5   8
21  10-21   10  21
22  8+  8   
23  6-14    6   14
24  5+  5   
25  10-16   10  16
26  10-14   10  14
27  11-16   11  16
28  12-16   12  16
29  9-16    9   16
30  7+  7   

"
How to access files downloaded from kaggle into a Colaboratory notebook?,"After some further experimentation, I found that the python os module works similarly in Colab Notebooks as it does on an individual computer. For example, in a Colab Notebook the command
os.getcwd()

returns '/content' as an output.
Also, the command os.listdir() returns the names of all the files I downloaded and extracted.
"
Deep learning for computer vision: What after MNIST stage?,"You should try hyperparameter tuning, it will help improve your model performance. Feel free to surf around various articles, fine tuning your model will be the next step as you have fundamental knowledge regarding how model works.
"
Python can&#39;t take input while using functions,"Reusing code is a good idea, but beware how the scope of variables change when you put code into a function.
The error you are getting is caused because there are NaN values in the array you input into the random forest. In your feature_engineering_and_selection() function, you are removing the NaN values, but df is never returned from the function, so the original, unmodified df is used in the model.
I suggest splitting up your feature_engineering_and_selection() function into different components. Here I made a function that just removes NaNs.
# Iterates through the columns and fixes any NaNs
def remove_nan(df):
    replace_dict = {}

    for col in df.columns:

        # If there are any NaN values in this column
        if pd.isna(df[col]).any():

            # Replace NaN in object columns with 'N/A'
            if df[col].dtypes == 'object':
                replace_dict[col] = 'N/A'

            # Replace NaN in float columns with 0
            elif df[col].dtypes == 'float64':
                replace_dict[col] = 0

    df = df.fillna(replace_dict)

    return df

I suggest filling NaN numerical values with 0 instead of the mean. For this data, there are 3 numerical columns with nan values: LotFrontage (feet of street connected to property), MasVnrArea (masonry veener area), GarageYrBlt (garage year built). If there is no garage, then there is no garage year built, so it makes sense to have the year as 0 instead of the average year, etc.
There is also some work that needs to be done with the one hot encoder you have set up. Creating a one-hot-encoding can be tricky, because the training data and the test data need to have the same columns. If you have the following training and test data
Train
| House Type |
| ---------- |
| Mansion    |
| Ranch      |

Test
| House Type |
| ---------- |
| Mansion    |
| Duplex     |

Then if using pd.get_dummies() the train columns will be [house_type_mansion, house_type_ranch] and the test columns will be [house_type_mansion, house_type_duplex], which won't work. However, using sklearn, you can fit a one hot encoder to your train data. When transforming the test dataset, it will create the same columns as the train data set. The handle_unknown parameter will tell the encoder what to do with duplex in the test set, either ignore or error.
# Fits an sklearn one hot encoder
def train_one_hot_encoder(df, categorical_columns):
    # take one-hot encoding of categorical columns
    categorical_df = df[categorical_columns]
    enc = OneHotEncoder(sparse=False, handle_unknown='ignore')
    return enc.fit(categorical_df)

To combine the categorical and non categorical data, again I suggest making a separate function
# One hot encodes the given dataframe
def one_hot_encode(df, categorical_columns, encoder):
    # Get dataframe with only categorical columns
    categorical_df = df[categorical_columns]
    # Get one hot encoding
    ohe_df = pd.DataFrame(encoder.transform(categorical_df), columns=encoder.get_feature_names())
    # Get float columns
    float_df = df.drop(categorical_columns, axis=1)
    # Return the combined array
    return pd.concat([float_df, ohe_df], axis=1)

Finally, your feature_engineering_and_selection() function can call all of those functions.
def feature_selection_and_engineering(df, encoder=None):
    df = remove_nan(df)
    categorical_columns = get_categorical_columns(df)
    # If there is no encoder, train one
    if encoder == None:
        encoder = train_one_hot_encoder(df, categorical_columns)
    # Encode Data
    df = one_hot_encode(df, categorical_columns, encoder)
    # Return the encoded data AND encoder
    return df, encoder

There were a few things I had to fix to make the code run, I have included the entire modified script in a gist here https://gist.github.com/kylelrichards11/6be90d92a7dd6a5cc9a5290dae3ff94e
"
Dataset throwing KeyError while looping through a list of variables,"Your concenating the data on the wrong axis
df = pd.concat([df, OHE_sdf], axis = 1, ignore_index = True)
# Should be
df = pd.concat([df, OHE_sdf], axis = 0, ignore_index = True)

However this will cause another error to throw in that you one hot encoded some of columns listed in na_columns, for instance Garage_Type has been encoded into multiple columns one for each potential value as such it no longer exists so it can't have its nan values replaced.
Edit:
I've updated several parts of the question code to ensure that it runs in it's entirety.
Firstly we need to import all the libraries we will be using, note the addition of numpy
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel
from sklearn.model_selection import train_test_split
import numpy as np

secondly we need to get the data from the source
train = pd.read_csv(""https://raw.githubusercontent.com/oo92/Boston-Kaggle/master/train.csv"")
test = pd.read_csv(""https://raw.githubusercontent.com/oo92/Boston-Kaggle/master/test.csv"")

Now we will remove all the NaN's from the data set
# Create a series of how many NaN's are in each column
nanCounts = train.isna().sum()
# Find the total number of NaN's and print it (used to check that this bits doing somethin)
nanTotal = train.isna().sum().sum()
print('NaN\'s found: ', nanTotal)

# Create a template list
nanCols = []
# Iterate over the series and if the value is more than 0 (i.e there are some NaN's present)
for i in range(0,len(nanCounts)):
    if nanCounts[i] > 0:
        # If it is append the current column to the list of columns that contain NaN's
        nanCols.append(train.columns[i])

# Iterate through all the columns which are known to have NaN's
for i in nanCols:
    if train[nanCols][i].dtypes == 'float64':
        # If the column is of the data type float64 (a floating point number), replace it with the mean of the column
        train[i] = train[i].fillna(train[i].mean())
    elif train[nanCols][i].dtypes == 'object':
        # If it's of the data type object (a text string) replace it with XX
        train[i] = train[i].fillna('XX')

# Reprint the total number of NaN's
nanTotal = train.isna().sum().sum()
print('NaN\'s after removal: ', nanTotal)

Now that there are no NaN's in the dataset it is possible to assemble a list of the categorical data
# Create a template list
categorical = []
# Iterate across all the columns checking if they're of the object datatype and if they are appending them to the categorical list
for i in range(0, len(train.dtypes)):
    if train.dtypes[i] == 'object':
        categorical.append(train.columns[i])
# Print out the list of categorical features
print('Categorical columns are: \n', categorical)

Now the code is very similar to the original with a few minor changes due to variable changes
# take one-hot encoding
OHE_sdf = pd.get_dummies(train[categorical])

# drop the old categorical column from original df
train.drop(columns = categorical, axis = 1, inplace = True)

# attach one-hot encoded columns to original data frame
train = pd.concat([train, OHE_sdf], axis = 1, ignore_index = False)

print('splitting dataset')
x_train, x_test, y_train, y_test = train_test_split(train, train['SalePrice'], test_size = 0.3, random_state = 42)

print('Selecting features')
# Note that here i changed the threshold so that it would actually show some features to use
sel = SelectFromModel(RandomForestClassifier(n_estimators = 100), threshold = '1.25*mean')
sel.fit(x_train, y_train)
# Also just straight up save the boolean array it will be quicker and i prefer the formatting this way
selected = sel.get_support()

# Print the boolean array of selected features
print(selected)
# Print the finally selected features
print(train.columns[selected])

All together it looks like
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel
from sklearn.model_selection import train_test_split
import numpy as np

train = pd.read_csv(""https://raw.githubusercontent.com/oo92/Boston-Kaggle/master/train.csv"")
test = pd.read_csv(""https://raw.githubusercontent.com/oo92/Boston-Kaggle/master/test.csv"")

nanCounts = train.isna().sum()
nanTotal = train.isna().sum().sum()
print('NaN\'s found: ', nanTotal)

nanCols = []
for i in range(0,len(nanCounts)):
    if nanCounts[i] > 0:
        nanCols.append(train.columns[i])

for i in nanCols:
    if train[nanCols][i].dtypes == 'float64':
        train[i] = train[i].fillna(train[i].mean())
    elif train[nanCols][i].dtypes == 'object':
        train[i] = train[i].fillna('XX')

nanTotal = train.isna().sum().sum()

print('NaN\'s after removal: ', nanTotal)

categorical = []
for i in range(0, len(train.dtypes)):
    if train.dtypes[i] == 'object':
        categorical.append(train.columns[i])

print('Categorical columns are: \n', categorical)

# take one-hot encoding
OHE_sdf = pd.get_dummies(train[categorical])

# drop the old categorical column from original df
train.drop(columns = categorical, axis = 1, inplace = True)

# attach one-hot encoded columns to original data frame
train = pd.concat([train, OHE_sdf], axis = 1, ignore_index = False)

print('splitting dataset')
x_train, x_test, y_train, y_test = train_test_split(train, train['SalePrice'], test_size = 0.3, random_state = 42)

print('Selecting features')
sel = SelectFromModel(RandomForestClassifier(n_estimators = 100), threshold = '1.25*mean')
sel.fit(x_train, y_train)
selected = sel.get_support()

print(selected)
print(train.columns[selected])

"
Download a Kaggle dataset using rvest,"If you're looking to download Kaggle datasets programatically your best bet is probably to use the API rather than scraping.

Docs: https://www.kaggle.com/docs/api
GitHub repo: https://github.com/Kaggle/kaggle-api

It is in Python but you can use it as a command line tool. (RStudio has a built in terminal you can use to run the commands.)
"
Is the commit time in kaggle is the same as that of running the code?,"Yep! Committing will run all your code from top to bottom in a new environment. This is to help ensure reproducibility and that your final notebook output doesn't rely on manually running cells in a specific order.
"
Getting a value error for Linear Regression model,"Try this replacement and it will work:  
x_text = data[data['Age'] != None].drop(columns='Age')
y_test = data[data['Age'] != None]['Age']

This will help.
"
unable to install the desired spacy version ==2.0.18 on kaggle,"Say ""yes"" in the command line:
!pip uninstall -y spacy

"
Unable to use &quot;scheme&quot; attribute when plot GeoDataFrame,"PySAL 2.0 has a new structure. There is a fix for GeoPandas coming soon (like today). You can either wait for that (version 0.4.1) or use GeoPandas from master via pip install git+git://github.com/geopandas/geopandas.git. Or alternatively downgrade PySAL to 1.x.
"
Why is my large file when extracted is of very less size on google colab?,"You're looking at the size of the directory instead of the size of its contents.
Try checking the size with du instead.
"
Where is the kaggle competitions submodule?,"The module you're trying to use is only available in the Kernels environment and it's specific to that particular competition. It's a custom module used to interact with this competition's dataset within the Kernels environment.
By contrast, the module you're pip installing is Kaggle's public API.
I hope that helps clarify the behavior you're seeing.
"
Is there a way to find the Malware family from the given malware binaries?,"Well, as far as I know, you can use VirusTotal free API (VirusTotal API) and
upload malwares to get the result of the Anti-malwares programs but they are not 
going to give you the exact family. I am not aware of any public malware family classification and I guess VT is best option you have.
(you can only send 4 request per minute I guess) 
"
Optimize Random Forest regressor due to computational limits,"Random Forest by nature puts a massive load on the CPU and RAM and that's one of its very known drawbacks! So there is nothing unusual in your question.
Furthermore and more specifically, there are different factors that contribute in this issue, to name a few: 

The Number of Attributes (features) in Dataset.
The Number of Trees (n_estimators).
The Maximum Depth of the Tree (max_depth).
The Minimum Number of Samples required to be at a Leaf Node (min_samples_leaf).

Moreover, it's clearly stated by Scikit-learn about this issue, and I am quoting here:

The default values for the parameters controlling the size of the
  trees (e.g. max_depth, min_samples_leaf, etc.) lead to fully grown
  and unpruned trees which can potentially be very large on some data
  sets. To reduce memory consumption, the complexity and size of the
  trees should be controlled by setting those parameter values.


What to Do?
There's not too much that you can do especially Scikit-learn did not add an option to manipulate the storage issue on the fly (as far I am aware of).
Rather you need to change the value of the above mentioned parameters, for example:

Try to keep the most important features only if the number of features is already high (see Feature Selection in Scikit-learn and Feature importances with forests of trees).
Try to reduce the number of estimators.
max_depth is None by default which means the nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.
min_samples_leaf is 1 by default: A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.

So try to change the the parameters by understanding their effects on the performance, the reference you need is this.

The final and last option you have is to create your own customized Random Forest from scratch and load the metadata to hard disk..etc or do any optimization, it's awkward but just to mention such option, here is an example of the basic implementation!


Side-Note: 
Practically I experienced on my Core i7 laptop that setting the parameter n_jobs to -1 overwhelms the machine, I always find it more efficient to keep the default setting that is n_jobs=None! Although theoretically speaking it should be the opposite!
"
How can import data into kaggle kernel if the folder is zipped?,"To see what's in your ../input/ directory (where datasets you attach to your kernel are located), run !find ../input/ in a code cell. This will print all paths to dataset files. You should see that the uncompressed directories and files are accessible.
If this doesn't help, perhaps you can share more details to help reproduce your issue (e.g., a link to the dataset and/or your kernel).
"
how to edit the content of dataset-metadata.jason file in google colaboratory,"You can use %%writefile magic in Colab.
%%writefile Music/dataset-metadata.json
{
  ""title"": ""My Title"",
  ""id"": ""x/MY_SLUG"",
  ""licenses"": [
{
 ""name"": ""CC0-1.0""
    }
   ]
 }

"
How to fetch data to train my neural network model in Kaggle?,"It seems you may have misspelled your actual folder name ""predictive"" and instead have spelled it ""predective"" in the code.
-Lando
"
impute data based on fitted linear model,"First, you made a mistake at the procedure of imputing missing values.
Data$Y[is.na(Data$Y)] <- Intercept + (Slope * Data$X)

The values in front of and behind the <- symbol have different lengths.
It results in a warning.
You should revise it as :
Data$Y[is.na(Data$Y)] <- (Intercept + (Slope * Data$X))[is.na(Data$Y)]

And if you wanna add a simple regression line, you can use :

(1) geom_abline( )

+ geom_abline(slope = Slope, intercept = Intercept)
But it's under the situation that you have slope & intercept.
And geom_abline() can only make a straight line.(Simple linear regression)

(2) geom_smooth( )

+ geom_smooth(method = ""lm"")
It use smoothing methods to fit data, eg. lm, glm, gam, loess, MASS::rlm.
You can search the help page to get detailed informations.
"
How to use NLTK dataset in Kaggle kernel?,"In kaggle kernel, the datasets are already pre-downloaded and packaged. 
You can simply do:
import nltk
from nltk.corpus import stopwords

stopwords.words('english')

See https://www.kaggle.com/alvations/nltk-datasets?scriptVersionId=5266785 
"
"Input contains NaN, infinity or a value too large.. when using gridsearchcv, scoring = &#39;neg_mean_squared_log_error&#39;","You can solve this error by adding this line. I am also Kaggler and also face a similar problem. 
An error will only be raised using linear regressors such as lasso, ridge and elasticnet not in tree-based regressors such as XGB and lightGBM because that lightgbm and XGB handle missing value by itself. But in linear regression sci-kit learn model not handle missing value by itself, 
 so we have to perform some pre-processing task. 
your dataset may contain a null value, missing value, inf values. so, we have to fill missing value and clip the infinite value to some range.
To add this line in sci-kit learn model which solve your issue.
df = df.fillna(df.median()).clip(-1e11,1e11)

"
NLP model&#39;s accuracy stuck on 0.5098 while training,"I think the way you defined the model architecture doesn't make sense! Try looking at this example on IMDB movie reviews with LSTM on Keras github repo: Trains an LSTM model on the IMDB sentiment classification task.
"
Error when using Kaggle CLI,"From the timing of your question, it looks like you may have hit a server glitch we were having. It should be fixed now. :)
"
Kaggle file sumbission error (Santander Value Prediction Challenge),"I guess you have used excel or LibreOffice Calc. Opening file in excel to view the output will collapse your format. Generally the best thing to do is avoid Excel entirely. Are you using Python? Easiest thing do is load the sample submission, replace the target column, and save:
ss = pd.read_csv('sample_submission.csv')
ss.loc[:, 'target'] = preds

ss.to_csv('sub.csv',
      index=False)

"
I get a &quot;-bash: command not found&quot; when trying to use the kaggle API,"If you download kaggle.json from https://www.kaggle.com/[account_name]/account
pip install kaggle
rm ~/.kaggle
mkdir ~/.kaggle
cp ~/Downloads/kaggle.json ~/.kaggle
kaggle --version

"
plt.imshow() changes output of seaborn.countplot(),"You're plotting one plot on top of another. Try something along the lines of:
# beginning of your code here...
# open new figure and make first plot
plt.figure()
g = sns.countplot(Y_train) 
# rest of your code here
# open second figure and make second plot
plt.figure()
s = plt.imshow(X_train[0][:,:,0])
# showing your plots will show both plots separately
plt.show()

"
Searching a Pandas DataFrame column for empty values gives contradictory results,"The operator in, when applied to a Series, checks if its left operand is in the index of the right operand. Since there is a row #1 (the numeric representation of True) in the series, the operator evaluates to True. 
For the same reason False in df['Sex'].isna() is True, but False in df['Sex'][1:].isna() is False (there is no row #0 in the latter slice).
You should check if True in df['Sex'].isna().values.
"
Why some portion of statistics is not used in data science,"You're most likely to see statistical hypothesis testing in data science if you're looking at something like A/B testing, where your goal is to determine whether there is a reliable difference between two samples and the size of that difference.
Kaggle competitions specifically are supervised learning problems rather than hypothesis testing, which is why you don't see people using things like chi-squared. (Which makes sense: if you have ten people do hypothesis testing on the same dataset, they should all get pretty much the same answer, which would make for a pretty uninteresting competition.)
Personally, I think it's good to be familiar with both statistical hypothesis testing and machine-learning techniques, since they have different uses. Hope that helps! :)
"
Submission on Kaggle,"You can e.g. create Pandas DataFrame with columns of interest and save it as csv.
submission = pandas.DataFrame({
    ""LoanId"": LoanId[""LoanId""],
    ""Prediction"": predictions.astype(int)
})

submission.to_csv(""kaggle.csv"", index = False)

"
kaggle dataset or python split CLI,"just FYI - I found the answer for my question...
It turns out I was using the test data which indeed should not contain the label in the dataset.  I download the train data and it does have the label (dog/cat) in the filename.
thanks!
Mira
"
regression on images using keras,"In keras you can load an image with:
from keras.preprocessing.image import img_to_array, load_img

img_path = 'img_56.jpg'
img = load_img(img_path)  # this is a PIL image
x = img_to_array(img)

Source
This will give you a tensor of shape (channels, height, width), where channels is typically 3 for an RGB image.
"
Why does my implementation of linear regression in Tensorflow done on Ames Housing dataset converge very very slowly?,"Working Code
import csv
import tensorflow as tf
import numpy as np

with open('train.csv', 'rt') as f:
    reader = csv.reader(f)
    your_list = list(reader)

def toFloatNoFail( data ) :
    try :
        return float(data)
    except :
        return 0

data = [ [ toFloatNoFail(x) for x in row ] for row in your_list[1:] ]
data = np.array( data ).astype( float )
x_train = data[:,:-1]
print x_train.shape
y_train = data[:,-1:]
print y_train.shape


num_features = np.shape(x_train)[1]

# Input
tf_train_dataset = tf.constant(x_train, dtype=tf.float32)
tf_train_labels = tf.constant(y_train, dtype=tf.float32)

# Variables
weights = tf.Variable(tf.truncated_normal( [num_features, 1] , dtype=tf.float32))
biases = tf.Variable(tf.constant(0.0, dtype=tf.float32 ))

train_prediction = tf.matmul(tf_train_dataset, weights) + biases

loss = tf.reduce_mean( tf.square( tf.log(tf_train_labels) - tf.log(train_prediction) ))

optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)

num_steps = 10001

def accuracy(prediction, labels):
    return ((prediction - labels) ** 2).mean(axis=None)


with tf.Session() as session:
    tf.global_variables_initializer().run()
    print('Initialized')
    for step in range(num_steps):
        _, l, predictions = session.run([optimizer, loss, train_prediction])
        if (step % 1000 == 0):
            print('Loss at step %d: %f' % (step, l))

Explanation of Critical Change
Your loss function wasn't scaled for the price.  The above loss function takes into account that you are really only interested in the error in prices scaled with the original price.  So, for a million dollar house being off by $5,000 shouldn't be as bad as being off by $5,000 for a $5,000 house.
The new loss function is :
loss = tf.reduce_mean( tf.square( tf.log(tf_train_labels) - tf.log(train_prediction) ))

"
converting csv to arff,"Here's what I did: 
From Kaggle, I downloaded train.csv (5568 instances, highest ID numbeer 6960).
I didn't use the converter -- just loaded it into the Weka Explorer as a CSV file.  Some problems and their solution:

Line 3: First instance of ""Bachelor's Degree"".  It did NOT like that single quote (""line 3, read 7, expected 108"").  Got rid of all single quotes (using a global replace in a text editor). Then I tried to load it into Weka again.
The file doesn't have a CR (the Enter key on the keyboard) at the end of the last line, which caused an error  (""null on line 5569""). I added one, again in a text editor. Then I loaded it into Weka, and took a look at the variables.
YOB (Year of Birth) is missing for about 300 instances, with ""NA"" filled in. So, it didn't evaluate as either string or numeric. Edited these to be empty cells instead. Then I loaded it into Weka.
And, of course, moved Party to be the class variable (at the end). I did this in Weka.
Saved this as train.arff
Loaded it back in, and it seems to work OK.  I generated 51% accuracy with a OneR classifier, but you wouldn't expect a OneR classifier to work well here. I'm sure you can do better.  

Note I didn't do any manual typing of headers. That must have taken a while!
Good luck!
"
"How to get F1,Precision and Recall for a Cross Validated Data Set in R","The current approach reads the survival outcome as integer, which leads rpart to perform regression rather than classification. Better to recode to a factor level. 
Evaluation metrics such as precision, recall, and F1 are available via the wonderful confusionMatrix function.
library(caret)
train <- read.csv(""train.csv"")
test  <- read.csv(""test.csv"")
tc <- trainControl(""cv"",10)
rpart.grid <- expand.grid(.cp=0.2)

# Convert variable interpreted as integer to factor
train$Survived <- as.factor(train$Survived)

(train.rpart <- train(  Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare 
                    + Embarked, 
                    data=train, 
                    method=""rpart"",
                    trControl=tc,
                    na.action = na.omit,
                    tuneGrid=rpart.grid))
# Predict
pred <- predict(train.rpart, train) 

# Produce confusion matrix from prediction and data used for training
cf <- confusionMatrix(pred, train.rpart$trainingData$.outcome, mode = ""everything"")
print(cf)

"
How to convert format csv to kaggle submission format in rapidminer,"Once you have the example set containing the results you need to export using the Write CSV operator. This will create columns in the CSV file with names matching the names of the example set attributes.
"
TensorFlow - Training accuracy not improving in MNIST data,"I suggest you change your bias_variable function - not sure how a tf.Variable(tf.constant) behaves, plus that we usually initialise biases at zero, not 0.1:
def bias_variable(shape):
    return tf.zeros((shape), dtype = tf.float32)

If this doesn't help, try initializing your weights with stddev=0.01
"
Predict the output based on multiple input variable using XGBoost in Python,"Its kind of difficult to evaluate what you might be doing wrong without access to any error msg or data input/results.
But without getting into detail about your specific case example, my suggestion to you as a beginner with xgboost would be to try to replicate a few demo/examples to get a better understanding of how it works.
A great blog/site for begginers is Jason Brownlee's. (edit: just realized you mentioned him)
An example case worked by him can be found here: http://machinelearningmastery.com/develop-first-xgboost-model-python-scikit-learn/
He also has some ebooks on ML and xgboost, they are really good starting material (better than most video courses)
A personal sugestion of mine would be for you to first do your code line by line when building a model instead of doing a one-instance application/function like you have done. That way its easier to evaluate when/if a mistake is made.
edit 2: another good advice is to use the xgboost wrapper for python (xgb.XGBClassifier) which is easier to understand/work with at first instance.
Hope that helps,
"
How to display dataframes?,"Whether you are using the REPL in Sublime Text or just running the program, you can display a dataframe called titanic as:
# prints first 5 rows in dataframe format
print(titanic.head())

# prints all rows in dataframe format
print(titanic)

If you want to display the data frame in CSV format, you need to convert it to CSV first using the to_csv function:
# prints first 5 rows in CSV format
print(titanic.head().to_csv())

# prints all rows in CSV format
print(titanic.to_csv())

"
Creating Dataframe from a json file,"You need to unnest() the column of interest before operating on it (e.g. before using group_by() or other dplyr verbs):
library(jsonlite)
library(tidyverse)

rbs <- fromJSON(""train.json"") %>%
  bind_rows()

rbsg <- rbs %>%
  unnest(bedrooms) %>%
  group_by(bedrooms)

rbs_filtered <- rbs %>%
  unnest(bathrooms) %>%
  filter(bathrooms > 5)

"
Python sklearn kaggle/titanic tutorial fails on the last feature scale,"You can help insure that the math transformation only occurs on numeric columns with the following.
numeric_cols = combined.columns[combined.dtypes != 'object']
combined.loc[:, numeric_cols] = combined[numeric_cols] / combined[numeric_cols].max()

There is no need for that apply function.
"
How to run .sqlite files?,"hi you can easily find a sample like this referenced from here
library(""RSQLite"")
con = dbConnect(drv=""SQLite"", dbname=""country.sqlite"")
alltables = dbListTables(con)
p1 = dbGetQuery( con,'select * from populationtable' )
p2 = dbGetQuery( con,'select count(*) from areastable' )
p3 = dbGetQuery(con, ""SELECT population WHERE DATE(timeStamp) < DATE('now', 'weekday 0', '-7 days')"")
dbClearResult(p3)
p4 = dbGetQuery(con, ""select * from populationtable where jobdescription like '%manager%'"")

"
Kernel gets busy when using nltk.download,"When you run nltk.download(), it launches an interactive GUI window that you can use to download resources. But very often this window is hidden behind other windows on your screen. Look for it, download anything you need and then close the downloader window in order for your script to return control to the notebook kernel.
To avoid hanging when your code gets to a download command, you could use a non-interactive download command instead. E.g., nltk.download(""brown"") for the Brown corpus, or nltk.download(""book"") to get all resources needed when reading through the nltk book. These carry out the download (even if you already have the requested resource) without opening a GUI window. For this you'll need to know, or guess, the internal name of the resource you want.
"
Plot proportion from Dataset,"you can do it this way:
import matplotlib
matplotlib.style.use('ggplot')

df = pd.read_csv(r'D:\download\train.csv')

clean = df.dropna(subset=['Age'])

(clean.groupby(pd.cut(clean.Age, np.arange(0, 90, step=10)))
      .Survived.mean().mul(100)
      .to_frame('Survival rate')
      .plot.bar(rot=0, width=0.85, alpha=0.5, figsize=(14,10)))


"
ValueError: array length does not match index length,"The problem is that you defining X_test twice as @maxymoo mentioned. First you defined it as
X_test = df_test.drop(['ID'], axis=1).values

And then you redefine that with:
X_train, X_test, y_train, y_test = cv.train_test_split(X_train, y_train, random_state=1301, test_size=0.4)

Which means now X_test have size equal to 0.4*len(X_train). Then after:
y_pred = clf.predict_proba(X_test)

you've got predictions for that part of X_train and you trying to create dataframe with that and initial id_test which has length of the original X_test.
You could use X_fit and X_eval in train_test_split and not hide initial X_train and X_test because for your cross_validation you also has different X_train which means you'll not get right answer or you cv would be inaccurate with public/private score.
"
Scikit-learn TruncatedSVD documentation,"I like the documentation Here a bit better.  Sklearn is pretty consistent in that you almost always use some kind of combination of the following code:
#import desired sklearn class
from sklearn.decomposition import TruncatedSVD 

trainData= #someArray
testData = #someArray

model = TruncatedSVD(n_components=5, random_state=42)
model.fit(trainData) #you fit your model on the underlying data

if you want to transform that data instead of just fitting it,
model.fit_transform(trainData) #fit and transform underlying data

Similarly, if you weren't transforming data, but making a prediction instead, you would use something like:
predictions =  model.transform(testData)

Hope that helps...
"
kaggle titanic Subset Women and Children,"By using  panda or scikit learning python libraries, and by using python language, you can combine feature variables. 
"
Read multiple files from a directory using Spark,"Instead of using 
sc.textfile(""path/*/**"") or sc.wholeTextFiles(""path/*"")

You can use this piece of code. Because spark internally lists all the possible values of a folder and subfolder so it can cost you time on large datasets. Instead of that you can use Unions for the same purpose.
Pass this List object which contains the locations to the following piece of code, note : sc is an object of SQLContext
var df: DataFrame = null;
  for (file <- files) {
    val fileDf= sc.textFile(file)
    if (df!= null) {
      df= df.unionAll(fileDf)
    } else {
      df= fileDf
    }
  }

Now you got a final Unified RDD i.e. df
"
how to filter data which integer64 class in data.table in r,"If I recall correctly, integer64 are just doubles masked as integer. Maybe the best way to obtain your subset without making any copy is to use the setattr function in data.table. Try this:
#remove the integer64 class
setattr(transaction$id,""class"",NULL)
custom_sample<-sample(unique(transaction$id),20000)
sample_transac<-transaction[id %in% custom_sample,]
#give the integer64 class back
setattr(sample_transac$id,""class"",""integer64"")

"
Trying to use separate to split one column into more than 2 columns,"Having looked at this data, I think the easiest way to do it is using something like str_match() from package stringr. If you assume data$Name is in the form 
""[Lastname], [Salutation]. [Firstname]"" 
the regular expression to match this is 
str_match(data$Name, ""([A-Za-z]*),\\s([A-Za-z]*)\\.\\s(.*)"")
#      [,1]                                                  [,2]        [,3]   [,4]                                   
# [1,] ""Braund, Mr. Owen Harris""                             ""Braund""    ""Mr""   ""Owen Harris""                          
# [2,] ""Cumings, Mrs. John Bradley (Florence Briggs Thayer)"" ""Cumings""   ""Mrs""  ""John Bradley (Florence Briggs Thayer)""
# [3,] ""Heikkinen, Miss. Laina""                              ""Heikkinen"" ""Miss"" ""Laina""                                
# [4,] ""Futrelle, Mrs. Jacques Heath (Lily May Peel)""        ""Futrelle""  ""Mrs""  ""Jacques Heath (Lily May Peel)""        
# [5,] ""Allen, Mr. William Henry""                            ""Allen""     ""Mr""   ""William Henry""                        
# [6,] ""Moran, Mr. James""                                    ""Moran""     ""Mr""   ""James"" 

So you need to add columns 2 to 4 above to your original data frame. I am not sure you can do this with separate actually. Writing
separate(data, Name, c(""Lastname"", ""Salutation"", ""Firstname""), sep = ""[,\\.]"") 

will try to split each entry by comma or dot, but it runs into a problem in the 514th entry that looks like ""Rothschild, Mrs. Martin (Elizabeth L. Barrett)"" (notice the second dot). 
In short, the easiest way I can see of doing what you want is 
data[c(""Firstname"", ""Salutation"", ""Lastname"")] <-
    str_match(data$Name, ""([A-Za-z]*),\\s([A-Za-z]*)\\.\\s(.*)"")[, 2:4]

"
IndexError: too many indices on Python,"Assuming that this is your actual code, the problem is that you never open the file. Your csv_file_object is still just the fileName, and thus your data is made up of the characters of that file name, resulting in a 1D numpy array.
Instead, you should open the file and create a csv.reader for it.
import csv
with open(fileName) as f:
    reader = csv.reader(f)
    data=[]
    for row in reader:
        data.append(row)
    data = np.array(data)

Or shorter: data = np.array([row for row in csv.reader(f)])

Update: The new error you are getting is probably due to you accidentally changing 
os.path.join('train.csv') to os.path('train.csv'), i.e., instead of calling the join function from the os.path module, you are (trying to) call the module itself.

Update: It seems your train.csv file is not in the same directory as your Python script, thus the script won't find the file if you just use the filename. You have to use the absolute path together with the filename:
fileName = os.path.join('/Users/scdavis6/Desktop', 'train.csv')

Or just fileName = '/Users/scdavis6/Desktop/train.csv'. Alternatively, move your train.csv file to the same directory as your Python script. This might indeed be the better and more robust option, unless you are using this file in multiple scripts in different directories.
"
NoClassDefFoundError while executing mahout program (JAR) using maven,"
Being also new to Java I missed the point of making available the dependent JARS. I finally bundled them into one Runnable JAR (I know - not a good idea) and was able to run it! 

"
Type Mismatch Error using randomForest in R,"For a R newbie like me... 
They are right when they say ""The error message means exactly what it says: there is at least one variable in your training data whose type does not match the equivalent variable in your test data.""
Do run the following to confirm nothing is obviously different:
str(training) and str(NewData)
That will list the training and new data's features and types.  The reason why you might still be confused, as I was, is the datatypes might appear to match and yet the error. It's probably that while a feature/column in both sets is listed as a factor the levels are not the same. My new data was much smaller, didn't have all the levels the training data did. That will blow you up with this error. The fix is: when you are processing your new data and go to factor it, pass in all the possible levels. That will get you to match and things will work.
dataframe$ColToFactor <- factor(dataframe$ColToFactor, levels=c(""PossibleLvl1"", ""PossibleLvl2"", ""PossibleLvl3"", account for all possible))

That was the deal for me.
"
ModuleNotFoundError: No module named &#39;seaburn&#39; in Kaggle&#39;s Notebook,"you made a typo error, it meant to be
import seaborn as sns

but you use
import seaburn as sns

"
What is the proper way of referencing of Figures and Tables using RMarkdown on Kaggle?,"you can demonstrate it show figures and tables using chunk labels as one method to assigned labls in Rmarkdown kaggle:
---
title: ""Example RMarkdown""
output:
  html_document:
    toc: true
    toc_float: true
---

### Figure Reference

```{r scatterplot, fig.cap=""Scatter Plot Caption""}
# Example data
x <- 1:10
y <- x^2
plot(x, y)

# Example data
mydata <- data.frame(
  Name = c(""Alice"", ""Bob"", ""Charlie""),
  Age = c(25, 30, 28)
)
# Print table
knitr::kable(mydata, caption = ""Example Table Caption"")

1-Set up RMarkdown document with data.

Then, I demonstrate how to reference a figure (scatterplot) and a table (example_table) using both chunk labels and manually assigned labels.
The figures and tables are created using R code chunks, and the fig.cap and caption arguments are used to provide captions for them.
In the last part, I reference these figures and tables in the text using \@ref(label) syntax.

you can adjust accordingly to your needs the above code. I hope this helps
"
Weird PyTorch Multiprocessing Error Where Main Loop Is Not Defined In __main__ | Kaggle,"To make the DDP code work when running in a notebook, you must include:
%%writefile ddp.py at the top of the DDP code.
To run the code, and train the model, in another cell call:
!python -W ignore ddp.py
"
Kaggle/Jupyter Notebook for TFLite Custom Model,"Jupyter notebooks require iPython kernel to be available in the environment and that supports the interactive sessions. However, you actually don't need a model training to be interactive. You just need a python script to keep running and storing the best model from time to time (called checkpoints) so that you can resume the training from the same epoch your training stopped, unfortunately.
To do so, save your Colab notebook as a Python script like this:
File -> Download -> Download .py
Once you have downloaded the file, you can run using terminal on a GCP instance. Just start a GCP VM using required configuration. (You will need to install dependencies for your model training).
Two things to keep in mind:

Keep logging the loss/metrics in a txt file/tensorboard. This is required because if your session to the terminal ends due to network issue, you will lose the printed losses.
Use tmux for persisting/resuming terminal sessions on VMs.

"
Python Key Error=0 raise KeyError(key) from err,"I tried the kaggle notebook, but I cannot reproduce the error.
If the local environment has been used, it may relate to the version of python dependencies.
"
AttributeError: Can&#39;t pickle local object. How to fix this issue?,"If i am not wrong, TensorFlow models cannot be directly pickled using 'pickle' due to compatible problems.
Instead of using pickle, you should use TensorFlow's built-in methods for saving and loading models.
Try with:
Save the Model:
tf.saved_model.save(bert_tf, 'my_model3')

Load the Model:
loaded_model = tf.saved_model.load('my_model3')

"
Filter Keras image_dataset_from_directory classes,"It might be simpler to use tensorflow.keras.preprocessing.image.ImageDataGenerator because you can directly filter classes.
from tensorflow.keras.preprocessing.image import (
    DirectoryIterator, ImageDataGenerator
)

directory = r'path/to/image/directory'
batch_size = 10
image_size = 256

img_iterator = ImageDataGenerator(
    rescale=1./255.
)

iterator = DirectoryIterator(
    directory=directory,
    image_data_generator=img_iterator,
    classes=desired_classes,
    target_size=(image_size, image_size),
    batch_size=batch_size
)

"
mutate() error while kniting Kaggle R Notebook,"I assume it has nothing do with R Markdown.
I created a kaggle account and downloaded your data dailyActivity_merged.csv. For reproducibility I have uploaded the first ten rows.
After this line
daily_activities$activity_date <- as.POSIXct(daily_activities$ActivityDate, 
                                             format=""%m/%d/%Y"", 
                                             tz = Sys.timezone())

consider to change your code to either of the following

using {dplyr} syntax

daily_activities |>
  dplyr::mutate(DayOfWeek = weekdays(activity_date))



or using base R

daily_activities$DayOfWeek <- weekdays(daily_activities$activity_date)

You may change the variable names accordingly.
Data
daily_activities10 <- structure(list(Id = c(1503960366, 1503960366, 1503960366, 1503960366, 
                                            1503960366, 1503960366), ActivityDate = c(""4/12/2016"", ""4/13/2016"", 
                                                                                      ""4/14/2016"", ""4/15/2016"", ""4/16/2016"", ""4/17/2016""), TotalSteps = c(13162L, 
                                                                                                                                                          10735L, 10460L, 9762L, 12669L, 9705L), TotalDistance = c(8.5, 
                                                                                                                                                                                                                   6.96999979019165, 6.73999977111816, 6.28000020980835, 8.15999984741211, 
                                                                                                                                                                                                                   6.48000001907349), TrackerDistance = c(8.5, 6.96999979019165, 
                                                                                                                                                                                                                                                          6.73999977111816, 6.28000020980835, 8.15999984741211, 6.48000001907349
                                                                                                                                                                                                                   ), LoggedActivitiesDistance = c(0, 0, 0, 0, 0, 0), VeryActiveDistance = c(1.87999999523163, 
                                                                                                                                                                                                                                                                                             1.57000005245209, 2.44000005722046, 2.14000010490417, 2.71000003814697, 
                                                                                                                                                                                                                                                                                             3.19000005722046), ModeratelyActiveDistance = c(0.550000011920929, 
                                                                                                                                                                                                                                                                                                                                             0.689999997615814, 0.400000005960464, 1.25999999046326, 0.409999996423721, 
                                                                                                                                                                                                                                                                                                                                             0.779999971389771), LightActiveDistance = c(6.05999994277954, 
                                                                                                                                                                                                                                                                                                                                                                                         4.71000003814697, 3.91000008583069, 2.82999992370605, 5.03999996185303, 
                                                                                                                                                                                                                                                                                                                                                                                         2.50999999046326), SedentaryActiveDistance = c(0, 0, 0, 0, 0, 
                                                                                                                                                                                                                                                                                                                                                                                                                                        0), VeryActiveMinutes = c(25L, 21L, 30L, 29L, 36L, 38L), FairlyActiveMinutes = c(13L, 
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         19L, 11L, 34L, 10L, 20L), LightlyActiveMinutes = c(328L, 217L, 
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            181L, 209L, 221L, 164L), SedentaryMinutes = c(728L, 776L, 1218L, 
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          726L, 773L, 539L), Calories = c(1985L, 1797L, 1776L, 1745L, 1863L, 
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          1728L)), row.names = c(NA, 6L), class = ""data.frame"")

"
"Different MAE values from XGBoost.regressor between a GPU and a CPU notebooks, why?","It seems that XGBoost uses different sample methods when using GPU or CPU.
Check sampling_method in the scikit-learn API documentation.

sampling_method –
Sampling method. Used only by the GPU version of hist tree method.
uniform: select random training instances uniformly.
gradient_based select random training instances with higher
probability when the gradient and hessian are larger. (cf. CatBoost)

"
Sentence-transformers installlation in offline Kaggle competition,"you can put sentence-transformers (or any other lib) into a dataset, link it to your notebook, then copy to working, which is writable, and then build, like
!cp -r /kaggle/input/dataset-with-all-stuff/sentence_transformers ./
!pip install ./sentence_transformers/sentence-transformers-2.2.2/sentence-transformers-2.2.2 --no-index -find-links=file:///kaggle/working/sentence_transformers/

"
Model training failing after 1 epoch,"Thanks to @Dr. Snoopy answer.There were corrupt images in my data set so used a simple python script in my root directory to remove truncated images
#pip install pillow
from PIL import Image
import os

def find_truncated_images(dataset_dir):
    truncated_images = []
    for root, _, files in os.walk(dataset_dir):
        for filename in files:
            file_path = os.path.join(root, filename)
            try:
                with Image.open(file_path) as img:
                    img.load()
            except (IOError, OSError) as e:
                # Log the file path if it's a truncated image
                print(f""Truncated image: {file_path}"")
                truncated_images.append(file_path)
    return truncated_images

def remove_truncated_images(truncated_images):
    for file_path in truncated_images:
        try:
            os.remove(file_path)
            print(f""Removed: {file_path}"")
        except OSError as e:
            print(f""Error removing {file_path}: {e}"")

if __name__ == ""__main__"":
    dataset_dir = "".""  # Set the path to your dataset directory
    truncated_images = find_truncated_images(dataset_dir)
    remove_truncated_images(truncated_images)

"
Kaggle and saving/continuing train progress,"To upload the state_dict files, a good approach is to select ""file"" from the menu bar at the top left area in Kaggle notebook, select ""upload data"". You can upload you dataset, better still compress the file(s) as zip, kaggle will unzip it.
On kaggle notebook, on the right panel there's an option for ""persistence"" select ""Variables and Files"". This will keep your file and variables saved.
To access your Google drive you need to ""pip install PyDrive2"". PyDrive2 is the forked maintained library of PyDrive
You could access this link for more instruction [https://www.projectpro.io/recipes/upload-files-to-google-drive-using-python][1]
"
SSL: DECRYPTION_FAILED_OR_BAD_RECORD_MAC in NPM and Python,"This issue was resolved by moving from wifi to lan internet, for some reason this is causing issue for my pc. which is odd because on my laptop connected to same wifi this is working perfectly fine. So underlying issue is still not known.
"
“element 0 of tensors does not require grad and does not have a grad_fn”,"The trainer has gradient tracking disabled in the validation/testing loops (for speed). If you want to enable them, you can do this in your validation step:
def validation_step(self, batch, batch_idx):
    torch.set_grad_enabled(True)
    ...

One more important step is to disable inference_mode in the Trainer
trainer = Trainer(inference_mode=False)  # true by default

Full example:
import torch
from torch.utils.data import DataLoader, Dataset

from lightning.pytorch import LightningModule, Trainer


class RandomDataset(Dataset):
    def __init__(self, size, length):
        self.len = length
        self.data = torch.randn(length, size)

    def __getitem__(self, index):
        return self.data[index]

    def __len__(self):
        return self.len


class BoringModel(LightningModule):
    def __init__(self):
        super().__init__()
        self.layer = torch.nn.Linear(32, 2)

    def forward(self, x):
        return self.layer(x)

    def test_step(self, batch, batch_idx):
        torch.set_grad_enabled(True)
        assert torch.is_grad_enabled()
        assert all(p.requires_grad for p in self.parameters())
        loss = self(batch).sum()
        loss.backward()
        self.log(""test_loss"", loss)

    def configure_optimizers(self):
        return torch.optim.SGD(self.layer.parameters(), lr=0.1)


def run():
    test_data = DataLoader(RandomDataset(32, 64), batch_size=2)
    model = BoringModel()
    trainer = Trainer(max_epochs=1, accelerator=""cpu"", inference_mode=False)
    trainer.test(model, dataloaders=test_data)


if __name__ == ""__main__"":
    run()

"
RuntimeError: &quot;element 0 of tensors does not require grad and does not have a grad_fn&quot;,"A workaround for you is to add torch.set_grad_enabled(True) at the beginning for training_step, or use the AdamW optimizer from torch.
"
Split list by 2 delimiters,"You can do it with str.extractand regex:
left = x.str.extract(r""\[(.*?) -"")
right = x.str.extract(r""- (.*?)\]"")

"
extract Kaggle files by using flask API in python,"Normally, the / character is used to separate components of the request path. That means that a request for /one/two/three is indicates a different resource than /one/two/four.
In your code, your kaggle dataset includes / characters, so Flask is looking for a route that matches /get-kaggle-data/alphiree/..., and no such route exists.
If you want a parameter to consume everything after a particular path component, even if it includes /, you need to tell Flask by using a path parameter type:
@app.route(""/get-kaggle-data/<path:dataset_value>"", methods=[""GET""])
def get_kaggle_data(dataset_value):

With the above code, a request for /get-kaggle-data/one/two/three will match this function and will result in dataset_value having the value one/two/three.
"
(HuggingFace Transformers) NLP with RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn,"I found the solution!!!
I tried to change the versions according to what I’ve read here:
Therefore, I changed my install packages part to:
!pip install torch==2.0.0+cu117
!pip install pytorch-lightning==1.9.4
!pip install accelerate==0.21.0
!pip install tokenizers==0.13.3
!pip install transformers==4.26.1

But the error was still popping up. So, I thought the error could be related to the optimizer used. My optimizer was this one:
def configure_optimizers(self):
        optimizer = AdamW(self.parameters(), lr=2e-5)

        scheduler = get_linear_schedule_with_warmup(
          optimizer,
          num_warmup_steps=self.n_warmup_steps,
          num_training_steps=self.n_training_steps
        )

        return dict(
            optimizer=optimizer,
            lr_scheduler=dict(
                scheduler=scheduler,
                interval='step')
        )

When I changed my method to use a simple Adam optimizer:
def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=2e-5)
        return [optimizer]

It worked!
So, the problem is in the AdamW with a scheduler. Reversing the install packages to just:
!pip install -q transformers

Makes the training work.
As the AdamW is deprecated, I think it is a good idea change the code to use the torch.optim.Adam/AdamW for instance.
So, summarizing, the new version of transformer might have introduced a bug in AdamW that is making the tensors lose their gradient function. Something like a detach anywhere in the code, probably.
But, anyway, this is a bug in the transformers.AdamW.
"
How to upload checkpoint to Kaggle?,"It turns out you have to zip it and upload a .zip file, even though the ckpt file is basically zipped already.
"
AttributeError: Can&#39;t get attribute &#39;AttentionBlock&#39; on &lt;module &#39;diffusers.models.attention&#39;,"I'm not sure why it isn't working for you to use the same version of diffusers that worked before, but, from this it appears that AttentionBlock has been replaced by just Attention. From here it sounds like you may be able to use _convert_deprecated_attention_blocks to update your model.
"
How can I upload a file from my local machine to a Jupyter notebook on Kaggle and run it there?,"On the kernel creation page, you'll see an option to upload a dataset. Click on the ""Upload dataset"" button. You can upload the your file there and when you are running jupyter notebook on kaggle, use the kaggle api/package to download the file.
https://www.kaggle.com/docs/api
"
Meaning of f{col :-&lt;50},"Your code format the value of col (here column name) by alignment with width of 50 characters, padding remaining characters left with -, in another words for nicer formating.
col = 'class'
print ( f'{col:-<50}')
class---------------------------------------------

"
In Kaggle while installing paramiko getting connectionErrors,"click on your photo profile , go to settings and verify your acount with phone verification.Finally, in your notebook go to ""Notebook options"" and activate the internet option. And there you go.
ps:I just found this solution, and it works for me.
you can check it here
"
Upgrading tesseract version on Kaggle,"It seems the short answer is that Kaggle does not allow an executable (.exe) to be run from a user's input directory.
"
Predicting House Prices on Kaggle not able to test existing data set,"It's a bit unclear from your description exactly what you are trying to do with your model. I am assuming you are trying to predict house prices.
First of all, house price prediction is a regression problem and commonly used cost/loss function for regression problems are MSE, RMSE, MAE, etc. I noticed that under the 'kfold' function where you are running the training loop, you are printing out RMSE at the validation stage which leads to my assumption that you are trying to predict house prices. However, you defined your loss function to be a cross entropy loss which is primarily used for classification problems (e.g. classifying house color, pattern, or other details).
You need to replace the following:
def loss(self, y_hat, y):
    fn = nn.CrossEntropyLoss()
    return fn(y_hat, y)

with this (to get RMSE loss):
def loss(self, y_hat, y):        
    return torch.sqrt(torch.mean((yhat-y)**2))

or with this (to get MSE loss):
def loss(self, y_hat, y):
    fn = nn.MSELoss()
    return fn(y_hat, y)

Secondly, your validate function would not yield proper result (in fact no result at all) as you are applying Softmax here to ultimately calculate accuracy. Since in regression problem we deal with continuous variables/labels, it is not ideal (or realistically possible) to calculate accuracy in such cases. Hence, your loss function is the primary evaluation metric here unless the problem requires you to evaluate with any other specific metric.
"
How to nullify the effect of global/operation level seed in the subsequent cells of Kaggle kernel?,"I tried some answers available on Stack Overflow and Kaggle comments around using javascript to restart the Kaggle kernel, but they did not work for my use case.
So, I uploaded my snippet in a python file and uploaded this file as a dataset and ran the python file independently in the kernel's cell using:
!python /kaggle/input/tensorflow-global-and-operation-level-seeds/random_seed_case_5.py

Example of the above can be found out at - https://www.kaggle.com/code/adeepak7/tensorflow-s-global-and-operation-level-seeds
This is a kind of hack, but I encourage folks to share their methods/tricks.
"
Kaggle Code doesn&#39;t download &quot;gpt2&quot; language model,"kaggle could not download resnet50 pretrained model
This one really helped.
I verified with my phone number, then got the internet connection.
"
Trying to run a query on postgreSQL that displays the following:,"If it where guaranteed that only one movie in a release year would receive the highest number of votes, then the following query would work:
SELECT DISTINCT ON (release_year), *
  FROM paramount
  WHERE release_year BETWEEN 2013 AND 2022
  ORDER BY release_year DESC, imdb_votes DESC;

Unfortunately, more than one movie in a release year could tie for the highest number of votes. The following query addresses this possibility:
WITH ranked AS (
  SELECT *,
         DENSE_RANK() OVER (PARTITION BY release_year ORDER BY imdb_votes DESC) AS rank
    FROM paramount
)
SELECT *
  FROM ranked
  WHERE rank = 1
    AND release_year BETWEEN 2013 and 2022
  ORDER BY release_year DESC, title;

"
Reading a dataset from Kaggle,"You can do this with the Kaggle Python API:
import kaggle

kaggle.api.authenticate()

kaggle.api.dataset_download_files('birdy654/cifake-real-and-ai-generated-synthetic-images', path='CIFAKE', unzip=True)

Authentication with kaggle.json is detailed in the link above.
I have also uploaded the dataset to GitHub as an alternative which may be more useful, the repo can be found here.
If you have any further questions then feel free to get in touch. I hope our dataset is useful for your project, good luck!
"
Why am I unable to load an audio file with torchaudio whenever I use a GPU on kaggle?,"This is different when you load the data into a single GPU or GPUs. It means that you should indicate that where do you process the data.
Normally, Pytorch use following code:
Object.to(device)
But, your case might be different with the one if you use another module, which is not PyTorch. So, please indicate that what moduel you are using to load the data. And what do you want to process the data following the step. Kaggle is not main thing to solve this problem.
"
use pretrained models in kaggle competitions with no internet,"You can create a private data set in Kaggle that holds the model files (my_model, my_token).  A Kaggle data set can hold any type of file.  Then in your inference notebook you can access this private data set and read the files.
"
R notebook not displaying dataframe correctly,"I figured it out. Thank you. Just got rid of the shorthand.
"
How can &#39;EntryPoints&#39; object has no attribute &#39;get&#39; error be fixed,"I switched platforms to Google Colab instead of Kaggle and the issue got resolved. It seemed the issue was the platform.
"
How can I pull data from a .sqlite file into Databricks Spark?,"If you are using pyspark and SQLContext, try the following code.
Add extraClassPath to your spark conf.
spark.executor.extraClassPath=<jdbc.jar>
Code snippet:
from pyspark import SparkContext
sc = SparkContext.getOrCreate()
from pyspark.sql import SQLContext
sqlCtx = SQLContext(sc)

sqlContext.read.format(""jdbc"").options(
    url=""jdbc:sqlite:{folder_path}/{file_name}.db"",
    driver=""org.sqlite.JDBC"",
    dbtable=""employee"")
    .load().take(10) 

"
InvalidArgumentError: Graph execution error:,"When you are doing this kind of coding keep the number of classes properly according to your data.
model = tf.keras.models.Sequential([
tf.keras.layers.Conv2D(64, (2,2), activation=tf.nn.relu,input_shape=(60, 40, 3)),
tf.keras.layers.BatchNormalization(),
tf.keras.layers.Conv2D(64, (2,2), activation=tf.nn.relu,padding = 'Same'),
tf.keras.layers.MaxPooling2D(2,2),

tf.keras.layers.Conv2D(128, (3,3), activation=tf.nn.relu,padding = 'Same'),
tf.keras.layers.MaxPooling2D(2,2),

tf.keras.layers.Flatten(),

tf.keras.layers.Dense(256, activation=tf.nn.relu),
tf.keras.layers.Dense(6, activation = tf.nn.softmax)

])
model.summary()
tf.keras.layers.Dense(6, activation = tf.nn.softmax)
I have 6 classes so I gave 6. Similarly keep according to your classes.
"
IndexError: Target 5 is out of bounds,"Adding new columns then selecting by column index is error-prone.
In scikit-learn>=0.23.0, fetch_california_housing can already return a dataframe with the as_frame parameter.
If you need dataframes, your code should be structured like this:
from sklearn.datasets import fetch_california_housing

california = fetch_california_housing(as_frame=True)

X = california.data
y = california.target

"
Loops are devil,"I really don't know what is happened, but after re-paste the code and resetting Kaggle to factory defaults the code started to work.
"
R chart in Kaggle,"rides_hour must be a data.frame. If you are using data = rides_hour you do not need to use rides_hour %>% ggplot....
Instead, use ggplot(data = rides_hour) + geom_bar(mapping = aes(x = time, y = ride_count, fill = member_casual)).
The ideal would be to share with us the data you are using. ;)
"
Model&#39;s prediction doesn&#39;t work. (Facial Expressions Kaggle) InvalidArgumentError: Computed output size would be negative: -1,"There are some shape errors which are evident from the error log which bring us back to the basics of CNNs , filters/kernels and related topics more of which you can explore in this cool blog.
Coming to the code fixes lets see them one by one and find where we need correction.
1. InvalidArgumentError : Here the tensor shape specified is less than the filter hence the numerical representation of the image needs to be corrected.
2. ValueError - you can refer to this StackOverflow thread. However, enclosing the relevant snippet.

The below example here is the clarify about Conv2D
input_layer= layers.InputLayer(input_shape=(2,2,1)) conv1 =
layers.Conv2D(3,(2,2)) X= np.ones((2,2)) X
=X.reshape(1,X.shape[0],X.shape1,1) # shape of X is 4D, (1, 2, 2, 1)  conv1(input_layer(X)) TL;DR
Now let's elaborating above codes
Line 1 input_layer was defined with the shape of 3D, but at line no.4
X was reshaped to 4D shape which is not matching the shape at all.
However, in order to feed any input X to input_layer or Conv2D must
pass with 4D shape.

3. InvalidArgumentError : This can be solved by making the input shape to be 4D.
"
"Failed to load Json File in Kaggle Kernel using load_dataset(), but works in Google Colab and Local System","load_dataset on kaggle need json with struct like:
{
 ""data"" : [{
   ""a"": ""one"",
   ""b"": ""two""
},
{
   ""a"": ""three"",
   ""b"": ""four""
}]
}

and use load_dataset as below:
dataset = load_dataset(""json"", data_files=file_path, field=""data"")

"
Getting the same tensor for &quot;prob&quot; and &quot;class&quot; in FastAI TabularLearner,"procs is a list of TabularProcedures. These are transforms that are applied to your data when it becomes available. For instance, FillMissing will fill any missing column values with some value (by default, the median of the available column values). Some of the transforms (like FillMissing) are necessary to keep your data well-formed, while others like Normalize may not be strictly necessary but are likely to help your model converge faster.
I don't know why clas isn't behaving as expected. The docs agree with your expectation and I checked with code similar to yours and had the same result you did.
"
"Found array with 0 feature(s) (shape=(10792, 0)) while a minimum of 1 is required","This is not how for loops work in Python.
for col in combi:
    if combi[col].dtype==""object"":
        # ...

col isn't an index into the collection you're iterating over (combi), it is the dereferenced element itself. Change all instances of combi[col] inside your for loop to col to correct. Example:
for col in combi:
    if col.dtype==""object"":

You didn't post all of your code so it's unclear if this will resolved the problem you're seeing, but it is certainly a step in the right direction.
"
Getting this error called on Kaggle as &quot;&quot;ImportError: cannot import name &#39;DecisionBoundaryDisplay&#39; from &#39;sklearn.inspection&#39;&quot;&quot;,"DecisionBoundaryDisplay requires nightly build version of sklearn as it's a new feature. (https://scikit-learn.org/dev/modules/generated/sklearn.inspection.DecisionBoundaryDisplay.html)
If you run this in your Kaggle notebook:
import sklearn; sklearn.show_versions()

you should see that the version is insufficient. (I got sklearn 1.0.2)
Unfortunately, I don't think you can upgrade the sci-kit learn package in Kaggle further as the latest build requires a newer version of Python (3.8).
Below I used this code to get an error on purpose to see what are the available versions of sklearn, and you can see that all the later versions require Python 3.8.
!pip install scikit-learn==

ERROR: Ignored the following versions that require a different python version: 1.1.0 Requires-Python >=3.8; 1.1.0rc1 Requires-Python >=3.8; 1.1.1 Requires-Python >=3.8; 1.1.2 Requires-Python >=3.8; 1.1.3 Requires-Python >=3.8
ERROR: Could not find a version that satisfies the requirement scikit-learn== (from versions: 0.9, 0.10, 0.11, 0.12, 0.12.1, 0.13, 0.13.1, 0.14, 0.14.1, 0.15.0b1, 0.15.0b2, 0.15.0, 0.15.1, 0.15.2, 0.16b1, 0.16.0, 0.16.1, 0.17b1, 0.17, 0.17.1, 0.18, 0.18.1, 0.18.2, 0.19b2, 0.19.0, 0.19.1, 0.19.2, 0.20rc1, 0.20.0, 0.20.1, 0.20.2, 0.20.3, 0.20.4, 0.21rc2, 0.21.0, 0.21.1, 0.21.2, 0.21.3, 0.22rc2.post1, 0.22rc3, 0.22, 0.22.1, 0.22.2, 0.22.2.post1, 0.23.0rc1, 0.23.0, 0.23.1, 0.23.2, 0.24.dev0, 0.24.0rc1, 0.24.0, 0.24.1, 0.24.2, 1.0rc1, 1.0rc2, 1.0, 1.0.1, 1.0.2)
ERROR: No matching distribution found for scikit-learn==

Based on this, it seems we are unable to change Python version in Kaggle environment: https://www.kaggle.com/questions-and-answers/210493
They also mention workarounds, how to use Kaggle APIs to access the dataset from your local environment, where you would be able to install the required versions for python/sklearn for your needs.
"
Minimax algorithm for ConnectX not working as expected,"After an hour, I figured out my mistake. It was a very silly one.
In the drop_piece function, the correct line should be
next_grid[row][col] = mark

But I wrote:
next_grid[row][col] == mark

Everything is working fine now.
"
"What does &quot;ValueError: cannot reshape array of size 279936 into shape (529,529)&quot; mean?","If you divide 279936 by 529 you'll get 529.1795841210, it's ALMOST a perfect square.
You can't resize a square matrix of size Nx1 to a matrix of size MxM if N is not equal to M^2 (is not a perfect square).
Your issue lays in your image or in img_to_array function. Please, ensure that those have/return the proper dimensions.
"
Install latest package version in Kaggle,"You actually did reinstall pandas. In order for the change to take effect, you need to restart the kernel. Run -> Restart & clear cell output
"
bertTokenizer | ValueError: Connection error,"turn on the internet from the settings it worked for me.
"
TensorFlow save model UnimplementedError,"Kaggle TPU (still) uses TPU Node architecture, for which user VM communicates with a remote TPU host over gRPC.
You can use tf.saved_model.LoadOptions with tf.saved_model.LoadOptions=""/job:localhost"" to save/restorer a TPU model on local file system:
model_dir = ""saved_model/xception""

localhost_save_option = tf.saved_model.SaveOptions(experimental_io_device=""/job:localhost"")
model.save(model_dir, options=localhost_save_option)

# Restore the weights
model2 = tf.keras.models.load_model(model_dir, options=localhost_save_option)


For TPU VM you have direct (root) access to the TPU host and can directly save the model on a local drive with:
model_sub.save('saved_model/xception')

"
How to figure where is q0.solution() or q0.hint() ? | Kaggle,"Both are modules...
from learntools.python.ex1 import *

This line imports all variables, classes, and functions defined in that module. That's probably where q0 is defined.
You can see all variables of the current scope by printing both locals() and globals()
Or look at the source code
https://github.com/Kaggle/learntools/blob/master/learntools/python/ex1.py
"
is there difference between print and return inside a function,"The difference between print and return is that when you use print, you don`t have to call your function with print, example:
def sign(x):
    if x > 0:
        print('123')
    elif x < 0:
        print('134')
    else:
        print('Hey')

sign(1) # That is how you call your func with no print

Otherwise, if you use returns in your code, than you have to use print when you call your fucn, example:
def sign(x):
    if x > 0:
        return 1
    elif x < 0:
        return -1
    else:
        return 0

print(sign(1)) # That is how you call your func with print

Also if you need to get the result value of func, then you can assign a value to a variable, example:
def func(x=0):
    x = x ** 2
    return x

a = func(5)
print(a) # 25

"
ValueError: Data must be 1-Dimensional error while creating a dataframe,"Important sidenote: Please, take some time to look into How to make good reproducible pandas examples, there are great suggestions there on how you could ask your question better.
Now for your error:
Data must be 1-dimensional

That means pandas wants a 1-dimensional array, i.e. of the form [0,0,1,1,...,1]. But your preds array is 2-dimensional, i.e. of the form [[0],[0],[1],[1],...,[1]].
So you need to flatten the preds array here:

Instead of for-loops consider using list comprehensions to change your code to something like this:
predictions = [1 if p>0.5 else 0 for p in preds]
df = pd.DataFrame({'PassengerId': test['PassengerId'].values, 
                   'Survived': predictions})

Also, in the meantime look into ndarray.round method - maybe it will better fit your use case:
predictions = preds.round()

"
Specifying output directory in vaex.from_csv(),"from_csv is just a method to read a csv file with vaex.
So you should do something like
df = vaex.from_csv('path/to/file.csv')

Then once you have the dataframe loaded, you can do:
df.export_hdf5('path/to/converted_file.hdf5')

If you are confused about the convert kwarg in from_csv, it says you can pass a string, which should be interpreted as the path of the converted file.
"
Is it normal for a CNN model to get ~20% accuracy with a dataset that contains 60-80 images per class?,"Looks like this person got up to around 40% validation accuracy: https://www.kaggle.com/code/diegofreitasholanda/chess-pieces-image-classification
But in general, yes that is a small dataset, and it will be hard to learn a good, generalizable network well, especially when the images all look very different from one another (I see some real pictures, others are clip art, etc).
"
How can I silence logs of a command in .ipynb file?,"As described in the first sentence here you can use IPython's/Jupyter cell magic to 'capture' the noisy output of a cell. And then just not do anything with it. The cell would resemble this for your example code:
%%capture
import sys
!{sys.executable} -m pip install -U pandas-profiling[notebook]
!jupyter nbextension enable --py widgetsnbextension

The use of the cell magic %%capture at the starting line of a cell is handy trick for suppressing all (I believe) output in a cell, in addition to using it to actually capture cell output for recall/use later.
"
Getting diff. num of features in train and test data after passing through the same pipeline,"Apologies Guys,
I am answering my own question here.
I made a mistake and sharing it here because I think I learned something very important.
The reason that I was getting this error was because a category column 'Embarked' also had null values in it, which were not present in the test_data set. So when I passed it to OneHotEncoder() in my full_pipeline. It created four columns of 0 and 1 to distinguish between the four values ( 3 original values, 1 null), which my model thought to contain an extra feature.
As soon as I removed the null rows everything went well.
Learning:
Always remove nulls values from your categorical columns before passing them to encoders.
"
Unable to load Pickled Glove 840b 300d in Jupyter Notebook,"The external website from the Kaggle link shows a 404 error message, so there is no file available under that URL. Try to log in into Kaggle, go to this URL: https://www.kaggle.com/datasets/authman/pickled-glove840b300d-for-10sec-loading, download the file manually and then use a local path in your code as you did for fasttext. Or you download the zipped file from the official website (https://nlp.stanford.edu/projects/glove/) and pickle it yourself.
"
ggscatter displays a positive pearson correlation coefficient in kaggle notebook instead of a negative (R),"This looks like a kaggle bug in the graphics driver.  Running this code in kaggle also leaves out the minus sign:
plot(1, main = expression(-1))

On the other hand, this works:
plot(1, main = ""-1"")

According to the docs for ggpubr::ggscatter(), you should be able to choose to display the stats in text rather than using an expression, but it didn't work when I tried this:
activity_sleep <- tibble::tibble(TotalMinutesAsleep = rnorm(20),
   SedentaryMinutes = rnorm(20) - TotalMinutesAsleep)
ggpubr::ggscatter(activity_sleep, x = ""TotalMinutesAsleep"", 
                  y = ""SedentaryMinutes"", shape = 21, add = ""loess"" ,
                  add.params = list(color = ""blue"", fill = ""darkgrey""), 
                  conf.int = TRUE, 
                  cor.coef = TRUE, cor.method = ""pearson"", 
                  cor.coef.args = list(output.type = ""text"")) +
    labs(title=""Sedentary Minutes vs. Minutes Asleep"")

Edited to add:  I reported the bug to kaggle and they replied:  ""This is a suitable question to best ask in the discussion or competition forums.""   So I guess it's not going to be fixed.
"
How do I find my trained model trained by &quot;kaggle.com&quot;,"The refresh button of browser refreshes the whole environment that you were working on, all variables that were set, any file that were saved in storage instance allotted to you for that particular session.
You are given an instance of a server resource, i.e., RAM, few GB of temporary storage, CPU/GPU for a certain period of time. Refreshing the browser starts completely new instance losing all local changes to that instance.

So your model even though it might be saved in the local storage of
that instance (on server), it will get deleted once that session
expires (after 20 minutes of inactivity) or when you run out of quota
or when you refreshes your page.

Solution is to don't refresh your page, look where the model is saved by exploring in side bar by clicking dropdown button and downloading the model for future use case.

Side Note: Committing your notebook will only commit, i.e., save that particular version of your code in the notebook, and will not save anything that you saved to the local storage of that instance, like in your case a saved model file.

"
Reshape or recreate code for neural network output,"Since you're using old tensorflow style, predict using old tensorflow style at the end:
feed_dict = {X: X_test}
classification = tf.run(y_proba, feed_dict)
label = numpy.argmax(classification, axis=-1)

Maybe you need to create a new session for this, or use the existing sess.run, I'm not sure how this works, try to see which option gives out reasonable results.
"
RMarkdown error when running in Kaggle - runs OK locally,"I see the issue though I do not know why it has happened.
I had to edit the RMarkdown, so when I edit, after every character, it displays a popup message: ""Code Editor out of Sync"" ...
I click OK not knowing what else to do and apparently some of the characters in the update weren't saved and I got ""nonsense"". I only saw that when I click on the <> on the left vertical panel.
Any idea on how am I suppose to update the RMarkdown in the Kaggle notebook?
Thanks!
"
The graph is not showing up in my jupyter notebook in kaggle,"I would add plt.show() on the line after the plot instruction.
If it doesn't work please provide a few lines of your dataset.
"
scraping name of dataset in kaggle using python,"Using Selenium
from selenium.webdriver.chrome.options import Options
opt = Options()
opt.add_argument('--headless')
driver = webdriver.Chrome(executable_path = 'yourdriverpath', options=opt)

driver.get(""https://www.kaggle.com/heptapod/titanic"")
time.sleep(5)
datasetname = driver.find_element(By.XPATH, ""//div[@role='button']//div//div"").text
print(datasetname)

Output:
train_and_test2.csv

Process finished with exit code 0

dataset snapshot
"
Building the output layer of NLP model (is the &quot;embedding&quot; layer),"logits


You have to put [0] in order to have torch.Tensor for computation.
You can also try output.logits instead of output[0]
ps. I used AutoModelForMaskedLM, not TFBertModel. It might be little different, but just try to print out your embedding first = ]
"
ValueError: could not convert string to float: &#39;Mme&#39;,"is it printing column labels in first line?
if so then you do proper data assigning so assign the array starting from second row array[1:,:]
otherwise try to look into it and see where is ""Mme"" string located so you understand how the code is fetching it.
"
"Pytorch, torchvision.transforms.Normalize((0.5, 0.5, 0,5), (0.5, 0.5, 0,5)) used on kaggle anime face data gives value error because of zero std","It appeares that, problem was created by defining stat = (0.5, 0.5, 0.5),(0.5, 0.5, 0.5) outside Normalize(stat). It is vierd as Python does not restrict that. Is it problem of pytorch or the problem of google colab, does anyone know anything?
Also only way to clear the failure appeared to be termination of session from colab -> runtime -> manage sessions , othervise crush persists.
And I lost whole day figuring it out :/
"
Installing pyodbc kaggle,"Try following commands:
sudo apt-get install unixodbc-dev

sudo apt-get install python-pip

pip install pyodbc

if that will not work, try
yum install unixODBC-devel

If still it is not working, read documentation : Installation of pyodbc
"
Can not run model with TPU,"As of now Kaggle and Colab only support remote TPU Devices which prevents TPUs to access your local files or run a custom Python image generator code.
New TPU-VM architecture solves that issue by attaching the TPUs to the host VM. It will be supported in Kaggle and Colab soon.
Meanwhile, as a workaround you can move your data to a GCS bucket and use
tf.keras.preprocessing.image_dataset_from_directory or tf.data.Dataset and combining it with Keras preprocessing layers.
"
Plotting the string value frequency in python,"Not knowing what your dataframe looks like at all, it is a little tough to give a precise answer, and I didn't check the below code on a dataframe because I didn't have something handy. (also, I assume you are using pandas here).
It sounds like you want (or at least could use) a dataframe that has a column of families and then the next column is just the count of that family in the original. You can accomplish this with groupby().
df2 = dfc.groupby(['family']).count()

If you want to then just have the top 10 left on there to make it easy to plot, you can use the nlargest() function in pandas.
df2 = df2.nlargest(10,'family')

"
Hide &quot;Requirement already satisfied&quot; warning from pip in Kaggle kernel,"Looks like a solution exists at
Hide ""Requirement already satisfied"" warning
pip install -r requirements.txt | grep -v 'already satisfied'
"
How to view .mdb dataset on kaggle?,"Try using the following snippet:
!sudo apt install mdbtools #  Library that allows access to MDB files programatically
!pip install meza # A Python toolkit for processing tabular data

from meza import io

records = io.read('database.mdb') # Use only file path, not file objects
print(next(records))

"
How to write pre processed image,"As you used OpenCV to open the image and you already have that as a dependency, there's little reason to suddenly introduce PIL or matplotlib as further deoendencies as in 1) and 3). Furthermore, your colours will be wrong because OpenCV uses BGR ordering and the others use RGB.
So, let's look at 2). If output is a directory, that can't work. You need to write to a file, so try:
cv2.imwrite('output/result.png',img)

"
Changing environment in Kaggle? (Option Disabled),"It was a problem on Kaggle's end. They have rolled out a fix.
More details
If it happens again, create a thread on kaggle forum I guess, just like I did.
"
How could I upload an interactive Rmarkdown file to Kaggle?,"I finally succeed knit the Rmarkdown file to rmd.html/pdf/docx with graphs (ggplot)fine.  But still I HAVE NOT YET MANAGED TO LAUNCH THE GRAPHICS ON KAGGLE, THOUGH SCRIPS RUNS SUCCESSFUL.  SO I AM STILL ASKING FOR SOLUTIONS...SOS>>>>>
For succeeding to knit th eRmarkdown, .RMD to html:
I kept trial and error as the error messages look similar towards different issues as seems.  Most common are error on parse ( text = code,...), 'characters missing',  or error on chunks options.
In brief, those are related to the need to quote
sufficient libraries,
state clearly the location and source, format of data files
and remembering each time new dataset is launched, the source file has to be {r} again.

rm(list = ls())

knitr::opts_chunk$set(echo = TRUE, results='hide')



 library(readxl)
 my data 1 <- read_excel("".xlsx"", range = "":"", na = ""NA"")


library(dplyr)
library(magrittr)
library(knitr)
library(ggplot2)
library(lattice)
 


 library(readxl)
 my data 2 <- read_excel("".xlsx"", range = "":"", na = ""NA"")


library(dplyr)
library(magrittr)
library(knitr)
library(ggplot2)
library(lattice)
 


Though a bit clumsy, this is how I found work so far after trying many other solutions, if you find better code and work, please feel free revert.  Appreciate.
"
How to run kaggle datasets on vscode on jupyter notebook?,"Have you got a prompt like this in the screenshot?

If I press ESC I will get the output like you.
"
How to use GPU parameters in LGBMClassifier and GridSearchCV at Kaggle Platform?,"Does it work if you add the argument to the last part of the first line like this?
........lgb.sklearn.LGBMClassifier(device='gpu')

"
pandas select &#39;object&#39; data type using select_dtypes(),"Filter using numpy :
import numpy as np
df.loc[:, df.dtypes == np.object]

or simply try below:
df.loc[:, df.dtypes == object]

"
Working with path folder from VS Code e Kaggle getting different results,"Because the operating system of the kaggle depends on was not the windows, it's a Linux.
"
How to add directory to PATH on kaggle?,"Couldn't the PATH be updated using:
sys.path.append(os.path.abspath(""path/to/be/added/""))
"
One hot encoded output to categorical value from a ML model,"prediction output is a list like this:
prediction = [0,0,0,0,1,0,0] , [0,1,0,0,0,0,0], [0,0,0,1,0,0,0]

Then you can change them by using this code:
pred = []

for x in prediction:
    if x == [0,0,0,0,1,0,0]:
        pred.append('Sad')
    
    elif x == [0,1,0,0,0,0,0]:
        pred.append('Happy')
        
    elif x == [0,0,0,1,0,0,0]:
        pred.append('disgust')

print(pred)

output =
['Sad', 'Happy', 'disgust']

Please add more elif statement according to your need.
"
mounting google drive folder on kaggle,"I had the same situation and the only solution I have found was this stackoverflow thread:
mount google drive in kaggle notebook
"
Problem loading CSV files in Tensorflow with TPU,"I found the source of the problem. I was executing the code to activate the Google Cloud SDK before connecting to the TPU. As they state in this post, the SDK must be activated after connecting to the TPU.
"
Kaggle Titanic project output issue (i apologize ahead of time for newbie question),"I think you need to run them on kaggle. The files are stored on kaggle.
"
How should I change the way I load my dataset so I can take advantage of kaggles TPU,"There are two ways that i know
Note that the tpu argument to TPUClusterResolver is a special address just for Colab. In the case that you are running on Google Compute Engine (GCE), you should instead pass in the name of your CloudTPU.
resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')
tf.config.experimental_connect_to_cluster(resolver)
# This is the TPU initialization code that has to be at the beginning.
tf.tpu.experimental.initialize_tpu_system(resolver)
print(""All devices: "", tf.config.list_logical_devices('TPU'))



INFO:tensorflow:Initializing the TPU system: grpc://10.240.1.74:8470
INFO:tensorflow:Initializing the TPU system: grpc://10.240.1.74:8470
INFO:tensorflow:Clearing out eager caches
INFO:tensorflow:Clearing out eager caches
INFO:tensorflow:Finished initializing TPU system.
INFO:tensorflow:Finished initializing TPU system.
All devices:  [LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:7', device_type

Manual device placement
After the TPU is initialized, you can use manual device placement to place the computation on a single TPU device.
a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])
with tf.device('/TPU:0'):
  c = tf.matmul(a, b)
print(""c device: "", c.device)
print(c)

c device:  /job:worker/replica:0/task:0/device:TPU:0
tf.Tensor(
[[22. 28.]
 [49. 64.]], shape=(2, 2), dtype=float32)

Most times users want to run the model on multiple TPUs in a data parallel way. A distribution strategy is an abstraction that can be used to drive models on CPU, GPUs or TPUs. Simply swap out the distribution strategy and the model will run on the given device.
strategy = tf.distribute.TPUStrategy(resolver)

INFO:tensorflow:Found TPU system:
INFO:tensorflow:Found TPU system:
INFO:tensorflow:*** Num TPU Cores: 8
INFO:tensorflow:*** Num TPU Cores: 8
INFO:tensorflow:*** Num TPU Workers: 1
INFO:tensorflow:*** Num TPU Workers: 1
INFO:tensorflow:*** Num TPU Cores Per Worker: 8
INFO:tensorflow:*** Num TPU Cores Per Worker: 8
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)

To replicate a computation so it can run in all TPU cores, you can simply pass it to strategy.run API. Below is an example that all the cores will obtain the same inputs (a, b), and do the matmul on each core independently. The outputs will be the values from all the replicas.
@tf.function
def matmul_fn(x, y):
  z = tf.matmul(x, y)
  return z

z = strategy.run(matmul_fn, args=(a, b))
print(z)

PerReplica:{
  0: tf.Tensor(
[[22. 28.]
 [49. 64.]], shape=(2, 2), dtype=float32),
  1: tf.Tensor(
[[22. 28.]
 [49. 64.]], shape=(2, 2), dtype=float32),
  2: tf.Tensor(
[[22. 28.]
 [49. 64.]], shape=(2, 2), dtype=float32),
  3: tf.Tensor(
[[22. 28.]
 [49. 64.]], shape=(2, 2), dtype=float32),
  4: tf.Tensor(

"
ERROR: No matching distribution found for segmentation-models-pytorch,"use !pip install segmentation-models-pytorch==0.0.3
"
How can I output soft label predictions from RandomForestClassifier instead of 0 or 1&#39;s,"Use the function predict_proba(X) of the model.
This function predicts the class probabilities for an X.
The documentation says:

The predicted class probabilities of an input sample are computed as
the mean predicted class probabilities of the trees in the forest. The
class probability of a single tree is the fraction of samples of the
same class in a leaf.

So, imagine that your RandomForest is formed for 100 different trees. If 91 of these trees predict that the sample is class 0, and 9 trees predicts it's class 1, the output of predict_proba(X) will be:
[0.91, 0.09]

Whereas if you use predict(X), you will obtain directly the class prediction (in this case, class 0 since 0.91 is bigger than 0.09):
[0]  # Which refers to class 0

"
Can HSV images be used for CNN training,"It seems you have the overfitting issue, and your model only memorize the simple samples  of the training set and in contrast it can not generalize to more complex and diverse data.
In the context of Deep Learning there are various methods to avoid overfitting and I think you don't need to transform your input to HSV necessarily. First of all you can apply various data augmentation methods like random crop or rotation to create various versions of your data. If this method does not work, you can use a smaller model or applying techniques such as Drop Out or Regularization.
Here is a good tutorial from TensorFlow.
"
Can I use tf.metrics.BinaryAccuracy in a multiclass classification problem?,"tf.keras.metrics.BinaryAccuracy is used for binary labels. For more details go through Tensorflow site.
Use tf.keras.metrics.CategoricalAccuracy if labels are one-hot encoded.
"
Unable render neural network structural graph on kaggle notebook,"The first thing to do is to turn on internet support on your kaggle notebook and install the following packages via pip packet manager:
!pip install pydot
!pip install pydotplus
!pip install graphviz

"
Error when using one hot vectors as labels for training,"If your labels are one-hot - then you have to use categorical_crossentropy:
model.compile(optimizer=opt, loss=""categorical_crossentropy"", metrics=[""accuracy""])

See here: https://keras.io/api/losses/probabilistic_losses/#sparsecategoricalcrossentropy-class
"
how does image rotation work in this function,"This seems like an affine transform of an image done the simple way(without multisampling), i.e. for all of destination pixels find corresponding pixel in source array and take its value.
destination pixel coorinates are put on a list idx, which is the transformed with matrix m, then they are cast into integers, and culled to image size.
idx3 is assembled from reverse-offset components (x,y) of source coorinates and used to index original image

why aren't they starting from 0,0 instead of DIM//2

(0,0) is top left corner of an image data array. When thinking of matrix transform you usually think that (0,0) point lies at the center of an image, and thus an offset
"
tf.data.experimental.make_csv_dataset: ValueError: `label_name` provided must be one of the columns,"label_name must be included in the select_columns
Try:
train_data = tf.data.experimental.make_csv_dataset(
    ""/kaggle/input/titanic/train.csv"",
    batch_size=12,
    column_names=all_columns,
    select_columns=feature_columns + ['Survived'],
    label_name='Survived', # name of the 'label' column
    na_value=""?"",
    num_epochs=1,
    ignore_errors=False)

"
Kaggle runs code cell and then ends without error,"You need to flatten the obtained Convolutional matrix layer using Flatten layer before of feeding it into Dense layer as below:
model.add(Dropout(0.1))
model.add(Flatten())    <------add Flatten layer
model.add(Dense(10, activation='softmax'))

"
How to write TFRecords in a Kaggle notebook?,"UnimplementedError: File system scheme '[local]' not implemented (file: './cifar10.tfrecords').
This is because Cloud TPU cannot write (or access to) files on Kaggle/Colab file system, it needs files to be placed on google bucket.
https://cloud.google.com/tpu/docs/troubleshooting#cannot_use_local_filesystem
"
Factor Columns converting to numbers in R DataFrame after mutate,"as.character Did the Job. I hope this helps someone who encounters similar problem in future.
kaggle = kaggle %>%
  mutate(
      prog_experience = if_else(prog_experience == """", ""I have never written code"", as.character(prog_experience))
      )

"
&#39;numpy.float64&#39; object has no attribute &#39;predict&#39; | Unable to do Prediction on Test Data in Heat Diseases Dataset,"Welcome to Stackoverflow. In below line you overwrite the LogisticRegression() class to a numpy array. And obviously numpy arrays how no predict() methods.
lr_clf = lr_clf.score(x_test,y_test)*100
Remove this line or assign it to another variable and run it again.
"
How to extract first word from DataFrame,"A Series object doesn't have a split method. You're trying to split a string so you'll need to convert the column datatype into string first (or expand the column out into multiple columns) before applying a split.
check data type of columns with df.dtypes
assign datatype with output['Nationality'].astype(str)
edit: no parentheses on dtype call
"
Is it possible to install earlier version of tensorflow on Kaggle TPU?,"Looks like it is impossible. Refer to:
https://www.kaggle.com/questions-and-answers/195058
"
Is number of tasks same as the number of fits for GridSearchCV Logistic Regression?,"Deciphering it step by step
Fitting 3 folds for each of 1600 candidates, totaling 4800 fits

1600 candidates means you are trying out 1600 combinations
Fitting 3 folds means you specified cv=3, you are cross-validating 3 times on training data.
totaling 4800 fits = 1600 * 3. i.e we have 4800 tasks

[Parallel(n_jobs=-1)]: Done 42 tasks | elapsed: 2.9min

Parallel(n_jobs=-1), -1 means you are running on all cores of your CPU
Done 42 tasks meand out of 4800, 42 fits have been completed
elapsed: 2.9min - from the time execution started it took 2.9min for completing 42 fits/ 42 training

Let me know if you still have any doubts.
"
Pandas : Why does missing Column on titanic database spreadsheet gets dtype as an object?,"You'll want to convert the Age column to an integer data type. This can be done as follows:
df = pd.DataFrame(datas)

df['Age'] = pd.to_numeric(df['Age'])

"
Taking advantage of Kaggle&#39;s free GPUs and TPUs,"You can import the files into each other. That way, you can run it all on one separate file that imports all the files. If you can only run one file, I'd advise you run them all on one file, or perhaps look into threading to run them all at once through one file.
"
&#39;bert-base-multilingual-uncased&#39; dataloader RuntimeError : stack expects each tensor to be equal size,"data_loader = torch.utils.data.DataLoader( batch_size=batch_size, dataset=data, shuffle=shuffle, num_workers=0, collate_fn=lambda x: x )
Use of Collate_fn in dataloader should be able to solve the problem.
"
I can&#39;t understand what I am doing wrong with my training and my test data for a competition on Kaggle,"I believe your problem is happening in this line:
low_cardinality_cols = [col for col in object_cols if X_test[col].nunique() < 10]

You are referencing X_test for your unique columns. Following the Kaggle tutorial you're supposed to be referencing X_train, like so:
# Columns that will be one-hot encoded
low_cardinality_cols = [col for col in object_cols if X_train[col].nunique() < 10]

You also seem to make this same mistake further down in this line:
OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_test[low_cardinality_cols]))

You have labelled it as the one-hot encoded training columns, but you've used X_test instead of X_train. You're mixing up your training and testing set processing which is not a good idea. This line should be:
OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[low_cardinality_cols]))

I'd advise you go over the code blocks in the tutorial again and make sure all your datasets and variables match up correctly, so that you're processing the correct training and testing data.
"
Rmarkdownon Kaggle: Can you spot what&#39;s going on?,"I've been stuck with this for a week!
If i press the buttom to run code, it will simply give me this error message.
For that, i still don't know the solution. But since this is a premade Rmarkdown and i knew that it works, i simple used the Save (Commit) options and at the end, it worked.
"
How to separate row by tags in Pandas?,"To split them, we can use that:
df['genre'] = df['genre'].apply(lambda x: x.split('|')) 
df.explode('genre')

It's simpler than I thought :)
"
Why do I get the same error values when training SVMs in R?,"Turns out this was due to a combination of a very unbalanced dataset and using the wrong kernel. Once I balanced the dataset with some oversampling, the errors were different for the different cost functions. It looked even better when I then used a radial kernel instead of a linear one.
"
How to categorize files within a zip archive into a list in Python?,"OK, no one answered my question, but I could find a solution for it. Actually, I could find were my problem was! I mention it here, so others working with Kaggle zip files can use it.
My codes (which are actually stolen codes:) were all correct. The only problem was that I was looking at the wrong directory! I used the os.listdir() function to understand how the files are structured in Kaggle and where the extracted files are located. (You can use extract() function to extract files within a zip archive)
So, if you have a zip archive in Kaggle and want to use it, just use the following code. Remember that you can use the whole code, which is a categorization of files by name, or just use the part that I explore the zipped archive. You DO NOT have to do a categorization like me, since I was categorizing files to use in a Convolutional Neural Network (CNN). You may decide to do other sorts of categorizations.
# importing necessary libs and packages
from subprocess import check_output
import numpy as np
import pandas as pd 
import os
import zipfile

# opening and viewing the files
with zipfile.ZipFile(""../input/dogs-vs-cats/train.zip"",""r"") as z:
    z.extractall(""."")
print(check_output([""ls"", ""train""]).decode(""utf8""))

# categorizing files into categories of 0 and 1 to use as labels
filenames = os.listdir(""../working/train"")
categories = []
for filename in filenames: 
    category = filename.split('.')[0] #here is the part that your code can be different to mine- you can categorize files differently and with a different approach
    if category == 'dog':
        categories.append(1)
    else:
        categories.append(0)

df = pd.DataFrame({
    'filename': filenames,
    'category': categories
})
print (categories)

Just remember, if the code did not work, it is probably due to the wrong path. Use os.listdir() to find the correct path.
"
High Validation AUC - Low Test AUC,"I am just being naive here because generally cross validation score shouldn’t be that far from test score. 
I just want to make sure we are talking the same metrics.

The cross-validation scores return accuracy
Maybe the competition is on AUC(Area under curve)

Accuracy can be 98% but AUC can still be only 85% 
If you want auc in cross validation predict update the last line with
print('RF:',np.mean(cross_val_score(X=X,y=y,cv=skf,estimator=pipe_rf,scoring= ‘roc_auc')))
"
.index() use in basic python programing,"Please everytime use one example to understand how any function works.
Here we take one example,
Attendees = ['Adela', 'Fleda', 'Owen', 'May', 'Mona', 'Gilbert', 'Ford'] 
Name = Mona
We check Mona is fashionably late or not
fashionably_late(arrivals,name) --> when function called we pass name as Mona and arrivals is list of attendees
order = arrivals.index(name) --> it return index of Mona here is '4'
return order >= len(arrivals) / 2 and order != len(arrivals) - 1
Order >= len(arrivals) / 2  --> len() returns length of list and /2 is devide by 2 as fashionably late is who come after half of attendees here 7/2
So it returns true as mona's index is greater than 7/2
order != len(arrivals) - 1 --> this condition check Mona is very last guest or not here it return true as Mona is not last guest
and --> 'true and true' return true so finally function returns true means Mona is fashionably late.
"
Why librosa.load() Runs Out of Memory when generating spectrograms?,"I happened to have the same issue and share my findings.
librosa.load may have memory leak issue, but not in your case, check this link.
Your issue comes from specshow with matplotlib related.
I suggest to use non-interactive mode for matplotlib:
import matplotlib
matplotlib.use('Agg')

And close all the resources, something like:
y, sr = librosa.load(audio_path)
fig, ax = plt.subplots(nrows=1, ncols=1)
spec = np.abs(librosa.stft(y, hop_length=512))
spec = librosa.amplitude_to_db(spec, ref=np.max)
color_mesh = librosa.display.specshow(spec, sr=sr, x_axis='time', y_axis='log', ax=ax)
ax.set(title='Spectrogram')
fig.colorbar(color_mesh, format='%+2.0f dB', ax=ax)
fig.savefig('Spectrogram.jpg')

fig.clear()
plt.cla()
plt.clf()
plt.close('all')
del color_mesh
del ax
del fig

referred link
"
Sqlalchemy won&#39;t load for Kaggle notebooks,"At this point, there appears to be an environemental problem on Kaggle when trying to connect to a db2 database using sqlalchemy. The problem doesn't exist when using notebooks locally in Jupyterlab, on IBM Watson, Microsoft Azure, and Cognitive Class. So, at this point I consider this question resolved. Thanks @mao for all the effort.
"
Cannot Decrease Loss via Gradient Descent in Logistic Regression,"I think the problem is that you're trying to implement the code for +1,-1 output label whereas titanic dataset has output 0/1 not +/-1, so you gotta change your algorithm and calculate derivative and logloss properly as this is not the log loss formula you use for 0/1 label.
"
How to fix problem installing tensorflow 1.5?,"Check Settings in your Kaggle notebook. Internet must be set to - On.
"
How to combine mean and count value frequency in pandas data frame?,"You can do reindex and assign it back 
#df1=train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean().sort_values(by='Survived',ascending=False)
#s=train.Title.value_counts(normalize=True)

df1['Title Freq']=s.reindex(df1.Title).tolist()

"
No detection on googlecolab or Kaggle kernel,"I somehow found the answer by myself. I modified the configuration file for all models.
# Testing
batch=1
subdivisions=1
# Training
#batch=256
#subdivisions=64

For testing change batch and subdivision to 1 and it will work.
"
Kaggle TPU NotFoundError for the GCS path,"I was using an incorrect path to access my data. My images were in ""my-first-data/plant-pathology-2020-cropped-images/images""
GCS_DS_PATH = KaggleDatasets().get_gcs_path('my-first-data')
!gsutil ls $GCS_DS_PATH

shows that the bucket contains this ""plant-pathology-2020-cropped-images"" folder.
This is how you define the paths for the images to pass them to the model.
def format_path(st):
    return GCS_DS_PATH + '/plant-pathology-2020-cropped-images/images/' + st + '.jpg'
train_paths = df_train.image_id.apply(format_path).values

"
Error when plotting on Kaggle using Seaborn. Error says I have categorical data even my data is int64,"In the second screenshot you are actually trying a distplot. This function needs numerical data and not categorical data.
Edit:
Quite a few entries in that column have value -1. Although numerical, it is actually a hack to use a NaN in an integer column if I understand correctly.
If I replace them and drop them I do get a proper output:
sns.kdeplot(df[df['deposit']=='no']['pdays'].replace(-1, np.nan).dropna())

Note that since pandas 0.24.0 there is support for <NA> entries in an integer column through the use of the new Int64: ""Nullable integer data type"".
"
Cannot connect Google AutoML to kaggle kernel,"I tried your code but I did not get errors. I used the same code as you:
from google.cloud import automl_v1beta1 as automl


# Create an AutoML client
client = automl.AutoMlClient()

Have you installed the library?
pip install google-cloud-automl

"
Kaggle in Jupyter,"Seems that the GOOGLE_APPLICATION_CREDENTIALS environment variable is not set correctly.
I suggest you to verify that the 'kagglebqtest-2999bd391350.json'  file is in the  path 'C:/Users/Jirah Marie Navarro/'.
I recommend you also to use a path without spaces such as 'C:/' or 'C:/credentials/' maybe the JSON credential is not recognized for the spaces in your path, so you can try with something like:
$env:GOOGLE_APPLICATION_CREDENTIALS=""C:\credentials/kagglebqtest-2999bd391350.json""

"
Accessing Kaggle tools in VM by mounting key,"Alright, I misread the instructions: ""You can define a shell environment variable KAGGLE_CONFIG_DIR to change this location to $KAGGLE_CONFIG_DIR/kaggle.json""
So the env variable should be /home/ubuntu/.kaggle/ instead of /home/ubuntu/.kaggle/kaggle.json.
"
Is there a way to open and run a complete ML project in kaggle notebooks just like in google colab?,"Yeah you can simply compress your project directory into a zip file ... and upload the zip file as a dataset, then launch a kernel from there.  Just keep in mind everything is read only unless you are in kaggle/working/
"
How to unzip cats-vs-dogs data in kaggle?,"You can load the zip file into pandas,
df = pd.read_csv('train.zip')
df

"
Loading pretrained FastAI models in Kaggle kernels without using internet,"so input path ""../input/"" in kaggle kernel is read only. create a folder in ""kaggle/working"" rather and copy the model weights there. Example below
if not os.path.exists('/root/.cache/torch/hub/checkpoints/'):
        os.makedirs('/root/.cache/torch/hub/checkpoints/')

!mkdir '/kaggle/working/resnet34'
!cp '/root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth' '/kaggle/working/resnet34/resnet34.pth' 

"
Python Macro to convert file names or read JPG and jpg as the same,"There are several possible solutions, this one is probably simplest - check for both .jpg and .JPG:
image_count = len( list(data_dir.glob('*/*.jpg')) + list(data_dir.glob('*/*.JPG')) )

"
"RandomForestRegressor: Input contains NaN, infinity or a value too large for dtype(&#39;float32&#39;) on kaggle learn","The problem is caused, as stated by the error message, from NaN values in the OH_X_test. Those values are introduced in the concat statement since the indices of the dataframes are mixed up.
I've therefore added 3 fixes in the code below: look at the ###FIX tag.
#### DATASETS LOAD ####
import pandas as pd
from sklearn.model_selection import train_test_split

# Read the data
X = pd.read_csv('../input/train.csv', index_col='Id') 
X_test = pd.read_csv('../input/test.csv', index_col='Id')

# Remove rows with missing target, separate target from predictors
X.dropna(axis=0, subset=['SalePrice'], inplace=True)
y = X.SalePrice
X.drop(['SalePrice'], axis=1, inplace=True)

# To keep things simple, we'll drop columns with missing values
cols_with_missing = [col for col in X.columns if X[col].isnull().any()] 
X.drop(cols_with_missing, axis=1, inplace=True)
X_test.drop(cols_with_missing, axis=1, inplace=True)

# Break off validation set from training data
X_train, X_valid, y_train, y_valid = train_test_split(X, y,
                                                      train_size=0.8, test_size=0.2,
                                                      random_state=0)

#### IMPUTATION OF MISSING VALUES FOR X_TEST ####
from sklearn.impute import SimpleImputer

# All categorical columns
object_cols = [col for col in X_train.columns if X_train[col].dtype == ""object""]

# Columns that will be one-hot encoded
low_cardinality_cols = [col for col in object_cols if X_train[col].nunique() < 10]

# Fill in the lines below: imputation
my_imputer = SimpleImputer(strategy='most_frequent')
imputed_X_test = pd.DataFrame(my_imputer.fit_transform(X_test))

# Fill in the lines below: imputation removed column names; put them back
imputed_X_test.columns = X_test.columns 
imputed_X_test.index = X_test.index ###FIX

#### ONEHOT ENCODING FOR DATA #####
from sklearn.preprocessing import OneHotEncoder

# Apply one-hot encoder to each column with categorical data
OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)
OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[low_cardinality_cols]))
OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[low_cardinality_cols]))
OH_cols_test = pd.DataFrame(OH_encoder.transform(imputed_X_test[low_cardinality_cols]))

# One-hot encoding removed index; put it back
OH_cols_train.index = X_train.index
OH_cols_valid.index = X_valid.index
OH_cols_test.index = imputed_X_test.index ####FIX

# Remove categorical columns (will replace with one-hot encoding)
num_X_train = X_train.drop(object_cols, axis=1)
num_X_valid = X_valid.drop(object_cols, axis=1)
num_X_test = imputed_X_test.drop(object_cols, axis=1) ####FIX

# Add one-hot encoded columns to numerical features
OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)
OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)
OH_X_test = pd.concat([num_X_test, OH_cols_test], axis=1)

##### BUILD MODEL AND CREATE SUBMISSION ####
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error

# normalize datatypes columns
#for colName in  ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'GarageCars', 'GarageArea']:
#    OH_X_train[colName] = OH_X_train[colName].astype('float64')
#    OH_X_valid[colName] = OH_X_train[colName].astype('float64')

# Build model
model = RandomForestRegressor(n_estimators=100, random_state=0)
model.fit(OH_X_train, y_train)
preds_test = model.predict(OH_X_test)

# Save test predictions to file
output = pd.DataFrame({'Id': OH_X_test.index,
                       'SalePrice': preds_test})
output.to_csv('submission.csv', index=False)

"
Python function not producing desired result,"You must write:
df = clean_data(df)

"
Submitting files from google colab to kaggle,"You have to provide an actual message to go with your submission. Can be empty.
kaggle competitions submit favorita-grocery-sales-forecasting -f sample_submission_favorita.csv.7z -m ""My submission message""
"
How to download dataset of Kaggle to Colab,"You can specify where to download to by using the -p (or --path) flag:
!kaggle competitions download pku-autonomous-driving -p data_folder/
"
Column Transformer with fit_transform error,"Remove the LabelEncoder() and use the OneHotEncoder(). You no longer need to LabelEncode before performing a OneHot with Scikit-learn.
Also, indeed, LabelEncoder() does have some issues when used with a Pipeline. For this, if you need to encode a particular column, you may devise a way to run this transformation independently without passing through a Pipeline transform.
Edit:
To label encode DataFrame columns:
df.apply(LabelEncoder().fit_transform)

This answer on SO will be helpful: https://stackoverflow.com/a/31939145/3353760
"
I am facing this error while submitting the solution on Kaggle Competition-,"It might be a problem with the file you created in the program. The file might contains unwanted quotes ("",'), which will cause the problem . Please check your file generated before submission function.
"
unable to view jpeg file,"I usually name my files '.jpg' when using the PIL module and will then reference the appropriate file path.
I tried your code with a renamed image.jpg file and it worked fine, when I use '.jpeg' I get a similar error.
from PIL import Image
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

image = Image.open('/Users/GameComputerC/ocr3.jpg')
image.show()

Let me know if it works for you. That might only be part of the error.
"
What path does mean in fastai,"Path here refers to your training dataset directory where your data is located. 
For eg in your example above:-
kaggle/input/dogs-vs-cats-redux-kernels-edition/train is your path.  The api will go through all the files in this path and then label them according to your list that you've provided.
"
How to setup Tensorboard in Kaggle Kernels (or in Colab with TF2) ? (in order to tune hyperparameters),"Here's how you use Tensorboard in Colab with TF2
get_start.ipynb
The key part is
%tensorflow_version 2.x
%load_ext tensorboard
# train and collect logs then call tensorboard
%tensorboard --logdir logs/fit

"
solutions for running &amp; sharing jupyter notebooks in Azure,"One of the almost custom solution could be the usage of Azure Data Science Virtual Machines. 
You can create the VM connect to it with RDP or SSH and share the access to VM with you collaborators, create separated VMs for them.
Not really a SaaS but could be a solution if you need to access or keep the data in Azure. 
"
%s function Python 3,"
calling function an object is really really bad idea. It's a reserved word in Python, so it might break everything in your code.
what do you want to achieve? your problem is not clear

"
Is it feasible to do random cropping as a data augmentation technique in the context of multi-label image classification?,"I haven't visited the links yet, but random crop helps as long as you can keep the presence of classes, even only a small part of the actual object.
"
Using CudnnLSTM gives this error: No module named &#39;tensorflow.contrib&#39; in kaggle kernel,"I put together this quick example in Kaggle based on your structure.  Note how the CuDNNGRU is called.  I hope this helps.
import tensorflow as tf

inp = tf.keras.layers.Input(shape=(10,))
emb = tf.keras.layers.Embedding(20, 4)(inp)
x2 = tf.keras.layers.Bidirectional(tf.compat.v1.keras.layers.CuDNNGRU(64, return_sequences=True))(emb)
max_pl = tf.keras.layers.GlobalMaxPooling1D()(x2)
x = tf.keras.layers.Dense(16, activation=""relu"")(max_pl)
x = tf.keras.layers.Dropout(0.1)(x)
output = tf.keras.layers.Dense(1, activation=""sigmoid"")(x)

model = tf.keras.models.Model(inputs=inp, outputs=output)
model.summary()

"
Training data converted to feature vectors but test data is string or text,"Test data should also be converted into feature vectors using the same vectorizer (used for creating training vectors).
Only then it can serve as an input to the model.
"
&quot;AttributeError: &#39;NoneType&#39; object has no attribute &#39;thread&#39;&quot; Error while committing in Kaggle,"You wrote something like y = x.thread or x.thread() The problem is, x was None. None doesn't have a member variable named ""thread""
"
How to calculate score for news category prediction?,"I'm not used to Pytorch, but in scikit learn it's possible to obtain the probability that an instance belongs to every class when you are making a prediction on it... maybe you could achieve the same on Pytorch
"
Trouble turning comorbidity data into a table using Python and Pandas,"For those who come across my question, after some brooding I came up with a solution;
df_findings = pd.DataFrame(columns=findings, index=findings).fillna(0)

for finding in df_string:
    finding = finding.split('|')
    finding.sort()
    if len(finding) > 1:
        col_label = finding[0]
        for row_label in finding[1::]:
            df_findings.loc[row_label,col_label] += 1

"
Error while exporting FastAI text classifier model,"You have to replace path/file with the name of the file encased in strings. 
target = open(path/file, 'wb') if is_pathlike(file) else file

In your code, python is treating path and file as variables instead of strings literals and trying to divide them. So you need some like this:
target = open('mydirectory/modelname.ext', 'wb') if is_pathlike(file) else file

or you can try 
 target = open(path+'/'+file, 'wb') if is_pathlike(file) else file

"
How to quickly get code from Kaggle notebook without requiring registration?,"In case it's useful to others, put the notebook url here to extract raw code
"
Why my competition results are disappearing from my profile page at Kaggle?,"The leaderboards for the getting started competitions (Titanic, Housing Prices Regression and Digit Recognizer) are cleared on a rolling basis. That way if you're just entering to try it out it doesn't clutter your profile forever. The leaderboards from other competitions aren't cleared.
"
How do I load zipfile on Kaggle?,"I believe by default the top level zip file in a Kaggle dataset is unzipped. You should be able to treat it as a directory.
"
"Tensorflow doesn&#39;t train: &#39;DataFrame&#39; objects are mutable, thus they cannot be hashed","The problem derives from your seventh(Test) step.
#Set X to the test data
X = test_normalized.astype(np.float32)
print(type(X)) # **<class 'pandas.core.frame.DataFrame'>**
Y1 = tf.nn.sigmoid(tf.matmul(X, W1))
Y2 = tf.nn.sigmoid(tf.matmul(Y1, W2))
Y3 = tf.matmul(Y2, W3)

You are setting X to a DataFrame. On the first run this does not affect anything. But, when you run sixth step after seventh you run into this problem because you have overwritten contents of X.
Try changing X to X_:
X_ = test_normalized.astype(np.float32)
Y1 = tf.nn.sigmoid(tf.matmul(X_, W1))

Also, your final eval does not work. Get it into a tf.Session.
"
Installed kaggle in the wrong directory,"If you do not want to play around with your system environment variables, then there is a simple workaround for that. 
Open your python3 command line, then Use pip from your code to install the required package!
import pip

def install(package):
    if hasattr(pip, 'main'):
        pip.main(['install', package])
    else:
        pip._internal.main(['install', package])

# Example
if __name__ == '__main__':
    install('kaggle')

"
Push and run python script on Google Cloud for ML,"I found nice functionality described there:
https://towardsdatascience.com/how-to-use-jupyter-on-a-google-cloud-vm-5ba1b473f4c2
It's a bit different comparing to Google Colab, but this quick tutorial shows where you can put notebook file from Google Colab.
You need to change only the path from /content/ in Colab to /home/jupyter in Google Cloud
"
Why does NLP feature matrix have two columns?,"There are a couple of questions there, so I'll try to answer them one by one.
x_val_count.shape - (1250, 8411) indicates that there are 1250 samples and 8411 features (where 8411 is the size of your vocabulary). However, scikit-learn's vectorizers store the data in the form of a sparse matrix (indices of non-zero features) for efficiency reasons. This is because there are lots of 0's in the features column (a document - in your case a Quora question - will have hardly 1% of the words from the vocabulary). If you want to convert it to a regular matrix, you can simply call x_val_count.toarray(), however, you may run out of memory because that would be a huge matrix. The output
(0, 1057) 1
(0, 4920) 1
(0, 5563) 1

can be read as ""Document 0 has 3 words in it, each occurring once."" If you are curious to know what these words are, you can look for them in the count.vocabulary_ dictionary, where words are keys and the indices (1057, 4920, ...), are values.
For your second question regarding predictions.shape - (1250, 2) you are getting 2 columns because you called the predict_proba() of LogisticRegression which returns probabilities of each class (in your case - 2 classes). If you just want the prediction labels, you should call predict().
"
Losing output trained model in Kaggle after commit,"Since you've already committed the model, you can get the output files in your current notebook.
Go to :
Files --> Add or upload dataset
Select the Kernel output files tab and filter by Your work. 
Your previously committed output files (say, saved models) will be imported to current notebook.
Hope this helps!
"
ValueError The minimum number of groups for any class cannot be less than 2,"is your y picking up indexes values ..not sure though..
you can try StratifiedKFold instead..the below worked for me 
kfold = StratifiedKFold(n_splits=10, random_state=7)
results = cross_val_score(model, X_train, y_train, cv=kfold)
"
how to access kaggle data after downloading with terminal?,"This command downloads data to the localization you are currently in within your terminal. It display what's been downloaded + you can check it using linux's ls command. You should see those three files if properly configured:
train_V2.csv
test_V2.csv 
sample_submission_V2.csv 

(you can find description of each on the competition's site here.
Data can be loaded using pandas library, this code will display the contents of one of the .csv files:
df = pd.read_csv(""train_V2.csv"")
print(df.head())

BTW. this competition is kernel only so you cannot make a submission using this data from your localhost.
"
"Dimensions must be equal, but are 64 and 1 for &#39;Conv2D_13&#39; (op: &#39;Conv2D&#39;) with input shapes: [?,28,28,64], [3,3,1,64]","I know the reason.
because padding=""SAME"" so the shape unchange. the shape of filter I set is missed.
for example:
filter1s shape is (3,3,1,64). filter2s shape is (3,3,64,64)
"
Gaierror while importing pretrained vgg model on kaggle,"It looks like you might not have enabled internet access in your kernel. You can do that in the panel on the right hand side. Once you add an internet connection you'll be able to download the files. 
We also actually already have the VGG-19 weights uploaded to Kaggle. If you like, you can add this existing dataset to your kernel rather than downloading it, which will probably be a little bit faster for you. 
Hope that helps! :)
"
Loading Amazon fine food reviews dataset from kaggle into colab notebook,"from google.colab import drive
drive.mount('/content/drive')

# Reading our CSV File
df=pd.read_csv(""/content/drive/...""Your Path""../Reviews.csv"")
df.head()

"
Trouble specifying column names when making dataframe from aggregate function,"You don't need to pass your groupby object into a new dataframe constructor (like @Vaishali mentioned already)
If you want to rename your columns after groupby, you can simply do something like:
combined.groupby(['Latitude', 'Longitude','from_station_name']).trip_id.agg('count').rename(columns={'Latitude': 'lat', 'Longitude': 'long', 'from_station_name':'station', 'count': 'trips})

"
How can I unzip the csv files downloaded from kaggle on google colab?,"In google colab they might erase all your data if you are reconnecting or changing the Runtime type. So be careful with your data. 
Before you run the file you should check if the file exists. Try listing files using ls command. If the file does not exist try to place it again. If you are downloading it from the web you can use wget command.
"
I cannot install tar.gz packages,"Use pip install --user kaggle and read this
And make sure you using python
Offical docs says: We do not offer Python 2 support. Please ensure that you are using Python 3 before reporting any issues.
"
Getting nan in tensorflow,"Note: Since no one has answered my question and I figured it out myself, I decided to answer my question in case someone encounters the same issues.
1) the nan values I was receiving when running the program was because the orginal data set had nan values. To solve this use
train.dropna(inplace=True)

This should clear up the nan values.
2)The size of w should match the size of x. For a single feature let the size of x be (m,1). Then the size of w will be (1,m). This is essentially matrix multiplication.
"
Error: &#39;too many values to unpack (expected 2) when applying a function,"This kind of error is caused by the mismatch between the number of returned values and the number of variables you are trying to store them into. To be more exact, in your case, the returned values are more than 2 but you are trying to store them in only two variables sib and parch.
I cannot give you an exact answer of how to solve it since you didn't show the exact format of passenger, but here's a short example to show you why the error happens.
Imagine you have the following case:
a = (1,2,3)
c,b = a

This will throw the same error you faced since a has 3 values but we are using only 2 variables to store them. To solve the issue we can do:
a = (1,2,3)
c,b,e = a

Now each value is respectively stored in 1 variable respectively and no error.
"
Stanford Cars Dataset Annotations missed,"You might find the paper that discusses how the dataset was put together helpful. It was intended for 3D rather than 2D object detection, which could be why the ground truth looks slightly different than you'd expect for 2D modelling.
Link to paper: http://vision.stanford.edu/pdf/3drr13.pdf :)
"
Creating Kernel using Julia,"We don't currently support Julia in Kaggle Kernels.
"
not able to login using request.post in kaggle website,"Hmm, have you tried the Kaggle API? You might find it easier to use. 
"
Using matplotlib to obtain an overlaid histogram,"Welcome to Stack Overflow! From next time, please post your data like in below format (not a link or an image) to make us easier to work on the problem. Also, if you ask about a graph output, showing the contents of desired graph (even with hand drawing) would be very helpful :)

df
    state   Year    n_killed    n_injured
0   Alabama 2013    9           3
1   Alabama 2014    591         325
2   Alabama 2015    562         385
3   Alabama 2016    761         488
4   Alabama 2017    856         544
5   Alabama 2018    219         135
6   Alaska  2014    49          29
7   Alaska  2015    84          70
8   Alaska  2016    103         88
9   Alaska  2017    70          69

As I commented in your original post, a bar plot would be more appropriate than histogram in this case since your purpose appears to be visualizing the summary statistics (sum) of each year with state-wise comparison. As far as I know, the easiest option is to use Seaborn. It depends on how you want to show the data, but below is one example. The code is as simple as below.
import seaborn as sns    
sns.barplot(x='Year', y='n_killed', hue='state', data=df)

Output:

Hope this helps.
"
Fetch argument None has invalid type &lt;class &#39;NoneType&#39;&gt;,"Looks like one of the train_op, loss or summary_op is None.
You didn't provide the complete code, so I can't say more. Check corresponding functions of your Model class.
Also if working in a Notebook environment make sure you executed all dependant cells after making some changes. For example you can try executing all cells from the beginning.
"
"When I add Anotation to the ggplot in a &#39;kaggle&#39;s notebook&#39; I get a huge whitespace above graph, how can I fix that","Your ""empty space"" is actually your grid.text being drawn: by default, when you create a grid.text, even if you assign it to a variable like you are, it is still drawn. This is why you have just the text ""Concern Area"" in the middle of nowhere. 
You can get rid of it by adding the argument draw = F to your initial grid.text call. 
This should give you what you're looking for: 
# City pickup point subset
city_p <- filter(uber,Pickup.point=='City')

# Create annotations object
my_text_city <- ""Concern Area""
my_grob_city <- grid.text(my_text_city, x=.32,  y=.91,
    gp=gpar(col=""black"", fontsize=7, fontface=""bold"",alpha=0.7), 
    draw = F)

# Plot
ggplot(city_p,aes(x=factor(request_hour), fill=Status))+
    geom_bar(position = 'dodge')+
    facet_wrap(~request_date, nrow =5)+
    annotation_custom(my_grob_city)

I've created a fork of your notebook to show that it's not just something funky up with Jupyter.
Hope that helps! :)
"
how to use GPU in kaggle_python docker image,"Nvidia has released a docker runtime that allows docker containers to access their host GPU. Assuming the image you're running has the CUDA libraries built in, you ought to be able to install nvidia-docker as per their instructions, then just launch a container using docker run --runtime=nvidia ...
There's an FAQ for using nvidia-dockers if you run into other roadblocks. I haven't done this myself, but lots of issues are probably going to be specific to how you installed the drivers and cuda libraries on your particular machine. You may also have to modify the image to include any necessary CUDA libraries if they aren't already installed.
"
Error downloading YouTube-8M dataset with curl in Windows 8.1,"I'd try using the Kaggle API instead. You can install the API using:
pip install Kaggle

Then download your credentials (step-by-step guide here). Finally, you can download the dataset like so:
kaggle competitions download -c youtube8m

If you only want part of the dataset, you can first list all the downloadable files:
kaggle competitions files -c youtube8m

And then only download the file(s) you want:
kaggle competitions download -c youtube8m -f name_of_your_file.extension

Hope that helps! :)
"
Can&#39;t convert float to int in python DataFrame/Array,"So first of all, try and keep in mind that you want to use as many vectorized operations as possible because this will speed up your code! Always important. So instead of looping through, pandas has an amazing way of doing this.
submission['Survived'] = submission['Survived'].astype(int)

Do note that this will truncate values so in your case you might want to say:
submission['Survived][:] += 0.5 before performing the above which will ensure values of 0.5 to be 1 when you convert to int and values below that to truncate to 0.
Changing of the dtype (types of columns can be found with df.dtypes) is thus done with the function pd.astype()
Might be another way of stating literally that it should be rounded up/down but with this simple data manipulation it should work ;) 
"
Link objects in nested list by dictionary key,"You want to do a merge on two dataframes on the region field. pandas library makes this really easy (also performant). The code looks like this (your CSV files are behind the Kaggle registration-wall):
import pandas as pd

loans = pd.read_csv('kiva_loans.csv')
mpi_regions = pd.read_csv('kiva_mpi_region_locations.csv')

df = loans.merge(mpi_regions, on='region')

You really don't want to reinvent the wheel by writing your own join code in base Python, use pandas package already.
(Note you're assuming region is unique across countries. It might be safer to merge both on=['country','region'])
"
How to extract specific values in a column in python /kaggle dataset,"If the text of the jobpost column is structured consistently (I've only looked at the online preview of the data) you can to use the subsequent stop word as anchor, e.g. all text from one stop word till you reach the next one:
(?s)JOB TITLE:.*(?=POSITION LOCATION)

If the stop words are in variable order, you can use a negative assert with alternations, e.g.
(?s)JOB TITLE:((?!POSITION LOCATION|JOB RESPONSIBILITIES|REQUIRED QUALIFICATIONS).)*

Demo
"
How to extract text at newline using regex in python?,"You can use : Positive Lookbehind (?<=REQUIRED QUALIFICATIONS:)
code:
import re

text = """"""
JOB RESPONSIBILITIES:
- Working with the Country Director to provide environmental information

to the general public via regular electronic communications and serving

as the primary local contact to Armenian NGOs and businesses and the

Armenian offices of international organizations and agencies;

- Helping to organize and prepare CENN seminars/ workshops;

- Participating in defining the strategy and policy of CENN in Armenia,

the Caucasus region and abroad.
REQUIRED QUALIFICATIONS:

- Degree in environmentally related field, or 5 years relevant

experience;

- Oral and written fluency in Armenian, Russian and English;

- Knowledge/ experience of working with environmental issues specific to

Armenia is a plus.

REMUNERATION:
""""""





pattern =r'(?<=REQUIRED QUALIFICATIONS:)(\s.+)?REMUNERATION'

print(re.findall(pattern,text,re.DOTALL))

output:
['\n\n- Degree in environmentally related field, or 5 years relevant\n\nexperience;\n\n- Oral and written fluency in Armenian, Russian and English;\n\n- Knowledge/ experience of working with environmental issues specific to\n\nArmenia is a plus.\n\n']

regex information:
Positive Lookbehind (?<=REQUIRED QUALIFICATIONS:)
Assert that the Regex below matches


*REQUIRED QUALIFICATIONS*:   matches the characters REQUIRED *QUALIFICATIONS*:                literally (case sensitive)
*1st Capturing Group*        (\s.+)?
*? Quantifier* —             Matches between zero and one times, as 
                             many times as possible, giving back as 
                             needed (greedy)
*\s*                         matches any whitespace character (equal to 
                             [\r\n\t\f\v ])
*.+*                         matches any character 
*+* Quantifier —             Matches between one and unlimited times, 
                             as many times as possible, giving back as 
                             needed 

"
Google Cloud DataLab + BigQuery: how to set region/zone/location,"Have you tried bq.Dataset('[your_dataset]').create(location='EU')?
"
Use python 2.7 on Kaggle kernel,"Unfortunately, kernels only support Python 3.
ETA: If you're looking for a hosted notebook environment, I just checked and Google Colab does support Python 2, so I might check that out instead if you really need 2.7.
"
Titanic Kaggle dataset Naive Bayes classifier error R programming,"Here is the answer: original question deleted, so web-cache link.  
The reason is that the model doesn't REALLY know how to deal with character columns, as you can see if you run data.matrix(test_data).  
The solution is to first convert your character columns into factors, ensuring that the factor levels in both train and test are consistent.  
On a side note, I suggest starting with Random Forest, as it generally performs well without any parameter tuning, and doesn't care about the distribution of your variables (as opposed to NB which assumes Gaussian distributions).  
"
Shiny R randomForest Error,"tit_glm <- randomForest(Survived ~ ., tit)

will give error, because this means that you are using all the rest of the features for model training. However, this is not possible, as there are many features which are mostly NA's and there are a few of them which have near zero variance.
First try to remove the features, that are mostly NA's, and then remove the near zero variance features.
"
radar chart with plotly (python 3x),"Finally, concerning the bad points, this is a bug in plotly and for the labels, this is a missing feature.
"
How to reshape csv file into matrix form for Digit Recognition Machine Learning,"Here is working example for you with generated random images:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

arr = np.random.randint(2,size=(1000,784))
train_df = pd.DataFrame(arr)

mat28x28 = train_df.values.reshape((train_df.shape[0],28,28))

for index in range(0,8):
    plt.subplot(2, 4, index + 1)
    plt.axis('off')
    plt.imshow(mat28x28[index], cmap=plt.cm.gray_r, interpolation='nearest')
    plt.title('Image: %i' % index)

"
Docker on Ubuntu 14.04 my browser is not starting after run command for Kaggle/python,"You can use datmo/kaggle:python to be able to run kaggle projects with jupyter notebook. After pulling the image, you can use code like this in the shell:
docker run --rm -it -p 8888:8888  -v ~/.:/home/  datmo/kaggle:python 'jupyter notebook'

This mounts the local directory onto the container having access to it. 
Then, go to the browser and hit https://localhost:8888, and when I open a new kernel it's with Python 3.5. I don't recall doing anything special when pulling the image or setting up Docker.
You can find more information from here.
You can also try using datmo in order to easily setup environment and track machine learning projects to make experiments reproducible. You can run datmo task command as follows for setting up jupyter notebook,
datmo task run 'jupyter notebook' --port 8888

It sets up your project and files inside the environment to keep track of your progress.
"
Not getting expected output - Plotting Histograms in Pandas,"Turns out running %matplotlib inline in an empty cell before writing any code solves it. Thanks @MattR for your help!
"
ValueError in creating submission csv,"It looks like you didn't run cell 16 in the notebook link you provided, in which Embarked values are converted to integers (including the string value Q, which is throwing the error you're seeing):
Cell 16
# fill the missing values of Embarked feature with the most common occurance
freq_port = train_df.Embarked.dropna().mode()[0]
for dataset in combine:
    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)
train_df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)

for dataset in combine:
    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)

train_df.head()

I just ran all the cells in order and the LogisticRegression section worked fine for me.  Try shutting down your notebook and re-running all the cells in the order they appear.
A general data science tip:
When you've already trained a model but your predict() function is throwing an error, it's helpful to look at the test data you're inputting and try and figure out what's wrong there.
In this case, searching the values in X_test_kaggle for the string Q might have revealed the problem was with the Embarked field, and that could have served as a first breadcrumb in tracking the problem back to its source.  
"
Python pandas read data of the form Label &lt;index_1&gt;:&lt;val_1&gt; &lt;index_2&gt;:&lt;val_2&gt;,"Based on your data description and the error message my guess is that the rows in your csv file do not have the same amount of fields per row. Try specifying the field columns:
my_cols = range(0,4125)
train = pd.read_csv('train.csv', header=None, delim_whitespace=True, names=my_cols).values

Find more help here: import csv with different number of columns per row using Pandas and here: Handling Variable Number of Columns with Pandas - Python
"
How to test a logistic regression model in R?,"This is happening because you are including the name of the data frame for each variable in the model formula. Instead, your formula should be:
glm(clicks ~ C1 + site_category + device_type, family = binomial(link = ""logit""), data = ad_train)

As described in second link in the duplicate notification:

This is a problem of using different names between your data and your
  newdata and not a problem between using vectors or dataframes.
When you fit a model with the lm function and then use predict to make
  predictions, predict tries to find the same names on your newdata. In
  your first case name x conflicts with mtcars$wt and hence you get the
  warning.

"
Poor results with Keras inbuilt VGG16 model on cats vs. dogs dataset,"The problem is that I was using a very high learning rate (0.01).
I have since amended the learning rate to lr=0.0001 and the accuracy after one epoch is closer to 95%, which is the expected result.
"
Jupyter IPYNB Problems,"For others gratification:
Glad it's solved.
Make sure python, ipython, jupyter from anaconda (using conda env's possibly) is in your path before others.  Order is important.
"
Python 3 Comparison String from Kaggle Dataset CSV-data: Error &#39;string index out of range python&#39;,"I believe you're mistaken by doing string comparison, though you intend to do age (number) and income (number) comparison.
if (ages[x] > 40 and ages[x] < 66) and income[x] < 50000:

make sure those (age and income) python lists are numeric. Use the conversion method. Let me know if this works.
"
OutOfRangeError while doing logistic regression,"I assume you are trying to execute the logistic regression example found in the Tensorflow for Machine Intelligence book.
When the csv file is loaded, the path is incorrect:
Instead of using 
filename_queue = tf.train.string_input_producer([os.path.dirname(__file__) + ""/"" + file_name])
use 
filename_queue = tf.train.string_input_producer([os.path.join(os.getcwd(), file_name)])

The first line will load the file ""/train.csv"" while the second one will load ""yourPath/train.csv"". Due to the fact that the first file doesn't exists, no data to train can be loaded.
The source code of all of the book's examples can be found in the following repo:
https://github.com/backstopmedia/tensorflowbook
"
R: Create a list of functions calls,"funs doesn't need 3 arguments. In the documentation, where it says

A list of functions specified by: Their name, ""mean"" The function itself, mean A call to the function with . as a dummy parameter, mean(., na.rm = TRUE)

This is 3 different ways to format your arguments to funs. Each argument to funs is interpreted as a function.
unique_vars = length(unique(.)) labels the output of summarizing by length(unique(.)) as a column called ""unique_vars"".
"
Python 3.x - Merge pandas data frames,"You want df.groupby(['gender','class'])['age'].median()  (per JohnE)
"
run r script using docker kaggle image,"You can load already builded image rstat from dockerhub:
docker run kaggle/rstats

For using your local data you should create volume:
docker run -v /you/local/data/path:path/in/docker/container kaggle/rstat

Volume binds your local storage with container storage. Also you can create additional volume for output data.
The last line in rstate dockerfile is 
CMD [""R""]

It means that R console will be called after container start. Just past your script in terminal (script should use data from mounted volume in container and write result to mounted output volume). After script execution you can stop container. Your output data will be saved on your local machine.
P.S. image is giant (6Gb). I never seen before such large docker image.
"
error: cannot convert argument to integer in Python,"Per my experience, the issue code is titles == k in the code titles[titles == k] = title_mapping[k]. The value type of expression titles == kis boolean type. 
In Python, the boolean type is a kind of integer value type. The False value is equal with 0, and all non-zero integer is True value.
But the value type of the key of map 'titles' should be the string type so that the error message is ""cannot convert argument to integer"".
Best Regards.
"
RuntimeError on windows trying python multiprocessing,"Python documentation is quite clear in this case.
The important part is Safe importing of main module.  
Your try_convnet_cc_multirotflip_3x69r45_maxout2048_extradense.py script is doing lots of things on module level. Without reading it in details I can already say that you should wrap the workflow with a function and use it like this:
if __name__ == '__main__':
    freeze_support() # Optional under circumstances described in docs
    your_workflow_function()

Besides the problem you have, it's a good habit not to surprise possible user of your script with side effects, if the user just wants to import it and reuse some of it's functionality.
So don't put your code on module level. It's ok to have constants on module level but the workflow should be in functions and classes.
If Python module is intended to be used as a script (like in your case), you simply put the if __name__ == '__main__' in the very end of this module, calling your_workflow_function() only if the module is the entry point for the interpreter - so called main module.
"
RStudio Shiny Error - number of items to replace is not a multiple of replacement length,"As the error suggest, you are trying to insert a new vector of length that is shorter than the column that you are placed and the number of items is not a multiple of replacement length 
using the following as an example :
## create a matrix that is 6 rows and 2 columns
mat <- matrix(1:12, 6,2) 

## try to replace first column with a vector of length 3
## works fine because 6 is a multiple of 3

mat[,1] <- 1:3 

## try to replace first column with a vector of length 5
## Fails because 6 is not a multiple of 5, try the same exception.

mat[,1] <- 1:5 

##   number of items to replace is not a multiple of replacement length

Hence, you may want to reconsider your apply statement. I will consider creating list to store the results instead of trying  to store it in a matrix. 
"
Accessing a list of lists,"In a list of lists, you can do:
values = [l[3] for l in data[:100]]

to achieve the same thing. 
"
Read images from a csv file with Octave,"This post contributed greatly to figure this out.
The key is to:

Remove the header row
Replace the "",,""(double commas) with ""0"" 
Replace "",""(single comma) with "" "" (space)
The code to read the file
fn = 'training_space.txt';
M = dlmread(fn);

"
Evaluation of multilabel and multiclass data labels,"There's this library from Kaggle's Director of Engineering:
https://github.com/benhamner/Metrics/tree/master/Python
"
"Predicted mask image has wrong dimension unet- TypeError: Invalid shape (2023, 2023, 256) for image data","first if you use vscode, I advice you to use this to debug :
https://marketplace.visualstudio.com/items?itemName=elazarcoh.simply-view-image-for-python-debugging
Out of my head, I would say you should sum the values over one axis, (As I image them beoing one hot encoded) :
np.sum(A,axis = 1)
Taken from this :
sum numpy ndarray with 3d array along a given axis 1
"
Kaggle Beginner problems,"Kaggle has tons of linear regression notebooks and datasets to learn from, most popular ones are probably about house pricing (given certain house features predict it's price).
Here's a new one I'm looking forward to solve:
Ben & Jerry's flavours and ratings ---> products.csv
The main goal would be predict wich ice cream flavours are better accepted based on it's ingredients.
"
Kaggle Python course Exercise: Strings and Dictionaries Q. no. 2,"A better approach to solve this would be to use the method contains(). An example of its usage can be found here.
So the algorithm would become:
list_to_return = []
counter = 0
for item in doc_list:
  if item.contains(word):
    list_to_return.append(counter)
  counter += 1
return list_to_return

"
"ERROR: Expected &#39;Id&#39; column to be of type &#39;Int32&#39;, but was &#39;String&#39;","You can try with a dictionary, where the index column has been generatend using range():
keys = range(1461, 1461 + len(predictions))
df_subm = pd.DataFrame({'Id': keys, 'SalePrice': predictions})
df_subm.to_csv('test_results.csv', index=False)

"
Reading large csv files using pandas with specifying dtypes giving memory error?,"Unfortunately, reading a csv file requires more memory than its size on the disk (I do not know how much more though).
You can find an alternative way to process your file here
"
What differs in while and for loops?,"You can simply use the built-in function any()
def menu_is_boring(meals):
    return any(meals[i] == meals[i+1] for i in range(len(meals)-1))

Difference between While and For Loop is simple:

While loops run until you break them or pass in a specific condition
For loops run until the range has ended or when you break them.

A for loop can't run forever, but a while loop can.
"
Applying functions on each row. is it checking the condition for every column in the row and applying on all of them,"There are multiple conditions. One applies on every row of the ""country"" column, while the other two are on the ""points"" column. The ""alternative result"" with else is if no conditions are met. With that said, it is better practice for pandas to use np.select, so that your solution is highly vectorized (faster run time):
import numpy as np
star_ratings = np.select([(row.country == 'Canada') | (row.points >= 95), (row.points >= 85)], #condiitons
                         [3,3], #results
                          1)    #alternative result (like your else)

The three parameter arguments are conditions (list of all conditions), Results (list of results in order of conditions), and alternative result. More here on numpy.select.
"
Making a US Heat Map?,"Based on the error, it is most likely that you have redefined the name list to a variable name, so that when you call list() to cast a variable to a type of list, the Python interpreter is actually trying to call your redefined variable as a function.
Example:
print(list(""123"")) # prints [""1"", ""2"", ""3""]

Now, if you define a variable called list, this happens:
list = [""redefining"", ""list""]
print(list(""123"")) # TypeError: 'list' object is not callable

Python thinks you've tried to call a variable as a function!
To resolve this issue, simply rename list to another name. The full code hasn't been posted, but you've likely redefined list at least somewhere.
"
How to use the test data against the trained model?,"train_test_split() is intended to take your dataset and split it into two chunks, the training and testing sets. In your case, you already have the data split into two chunks, in separate csv files. You are then taking the train data and splitting it again into train and val, which is short for validation (essentially the test or verification data).
You probably want to do the model.fit against your full training data set, and then call model.predict again the test set. There shouldn't be a need to do the call to train_test_split().

Edit:
I may be wrong here. In looking at the competition page, I'm realizing that the test set does not include the ground truth values. You can't use that data to validate your model accuracy. In that case, I think splitting the original training dataset into training and validation makes sense. Since you're fitting the model only on the train portion, the validation set is still unseen for the model. Then you are using the known values from the validation set to verify the predictions of your model.
The test set would be just used to generate 'new' predictions, since you don't have the ground truth value to verify.

Edit (in response to comment):
I don't have these data sets, and haven't actually run this code, but I'd suggest something like the following. Essentially you want to do the same preparation of your test data as what you are doing with the training data, and then feed it into your model the same way that the validation set was fed in.
import ...

def get_dataset(path):
    data = pd.read_csv(path)

    data['Sex'] = pd.factorize(data.Sex)[0]

    filtered_titanic_data = data.dropna(axis=0)

    return filtered_titanic_data

train_path = ""C:\\Users\\Omar\\Downloads\\Titanic Data\\train.csv""
test_path = ""C:\\Users\\Omar\\Downloads\\Titanic Data\\test.csv""

train_data = get_dataset(train_path)
test_data = get_dataset(test_path)

columns_of_interest = ['Pclass', 'Sex', 'Age']

x = train_data[columns_of_interest]
y = train_data.Survived

train_x, val_x, train_y, val_y = train_test_split(x, y, random_state=0)

titanic_model = DecisionTreeRegressor()
titanic_model.fit(train_x, train_y)

val_predictions = titanic_model.predict(val_x)

print(val_predictions)
print(accuracy_score(val_y, val_predictions))

text_x = test_data[columns_of_interest]
test_predictions = titanic_model.predict(test_x)

(Also, note that I removed the Survived column from the columns_of_interest. I believe by including that column in your x data, you were giving the model the value that it was attempting to predict, which is likely why you were getting 1.0 for the validation as well. You're giving it the answers to the test.)
"
pandas str.extractall on complete words,"Apparently pandas looks to separate groups to columns so the solution is to wrap all the regex also as a group.
df.Tweets.str.extractall('(\@(\w+))')
difference being a wrapping parenthesis inside the string.
"
download kaggle notebook along with the printed logs,"The Download Code menu option does just that - downloads the code, and nothing else.
To export the notebook, including the code cells and their associated outputs, open the notebook in question and select File → Download Notebook.
"
My validation accuracy is higher than my training accuracy and also there is no change both my accuracies,"as shown in the provided confusion matrix, the model is underfitting the data by predicting label 2 for all instances in the training set.

Probably this is also happening to the validation set and the reason why the validation accuracy is higher in this case is because the proportion of label-2 instances on validation set is higher.
It turns out that the chosen set of hyperparameters stink the model in some local minima over the cost function surface. Below is an illustration of the cost function surface in case your are not familiar with this concept already:

Since an underfitted model is useless, you need to explore some options for fix your model training.
Unfortunately, there is no deterministic path here. However, there are some common recommendations, especially for the case of underfitted models:

check the data: if the data cannot express a relation between the inputs and outputs you need to fix it (by adding features, cleaning or adding data, etc). Although I don't think that this is the cause of your particular problem, in general we also check the data when we get an underfitting.

Check the model: underfitting sometimes occurs because of the model capacity is low. Trying a bigger model (more trainable parameters) can solve the issue. Be careful and add parameters slowly. A really bigger model can cause your model to overfit. The best approach is starting with a small model and increasing it until you don't achieve the desired performance.

Try different optimizers: if you are using a ML library like Pytorch or Tensorflow, it is easy to change the training optimizer such as Adam, RMSProp, SGD, etc... Sometimes, Adam provides the fastest result, sometimes doesn't.

Tune the hyperparameters: Here we can play with several parameters. Learning rate and batch size are very tight correlated. A small Batch size usually requires a small learning rate. One very important and often forgot hyperparameter is the weight initialization. Frequently, you can avoid the local minima just by choosing different weight initializers.


Dealing with under and overfitting is a common scenario in model training. Sometimes, we can take a lot of time before realize where the problem is (initialization, learning rate, model size, etc). I think that a deep discussion about the choice of hyperparameters is beyond the scope of this answer. If you have an specific question about some of them, does not hesitate to ask.
A last word: using only accuracy for evaluate models performance is dangerous. As your example shows, the first model achieved a moderate performance (54%) despite it is totally unusable. To get more insightful feedback, use other more suitable metrics. Tow I would like to recommend for your problem are precision and recall:

Check here if you don't know what are local/global minima.
Check here if you don't know what is underfitting.
"
How do I count different values in the same column with in a specific date range?,"To count how many of each types happen for every year you could use something like this.
SELECT
    primary_type,
    year,
    count(*)
FROM
    `bigquery-public-data.chicago_crime.crime`
GROUP BY
    primary_type,
    year
ORDER BY 
    primary_type,
    year

It will show you number of each type for each year present in your table.
Take a note, that if crime of some type didn't happen in any year you wouldn't see 0, but rather row for this crime in that year will be lacking.
"
File not found error even I didn&#39;t mentioned or used the error file name or directory name in code,"It is easier if you debug the issue, as the error is coming on line Image.open(image_path)
Use the following lines to debug:
print(""Image paths for training set:"")
print(image_paths_train)
print(""\nImage paths for testing set:"")
print(image_paths_test)

features_train = extract_features(image_paths_train)
features_test = extract_features(image_paths_test)

If the printed file paths are incorrect or don't exist, you will need to investigate how these variables were created and ensure that they contain the correct file paths.
"
numpy.core._exceptions.MemoryError: Unable to allocate space for array,"Your 3 million by 20000 matrix better be sparse or you will need a computer with a very large amount of memory. One copy of a full real matrix that size will require a few hundreds GB or even a few TB of contiguous space.

Exploit more efficient matrix representation, like sparse one scipy.sparse.csc_matrix. The question is if the matrix has most of 0 scores.
Modify your algorithm to work on submatrices.

"
The productivity of Colab: to load directly from the Net(e.g. Kaggle) databases or to upload them on the colab directory and then extract them?,"Try getting API token from Kaggle's account tab. Then upload it in the google colab and try the following code to Initialize the Kaggle library,
! pip install kaggle
! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json

after the setup use the syntax below to download the dataset
! kaggle datasets download <name-of-dataset>

for more reference of the detailed work click here
"
&quot;Which one to choose? Lemmatization or Stemming?&quot;,"That depends on what you want to do.
Lemmatisation is linguistically motivated, and generally more reliable to give a correct result when reducing an inflected word to its base form. However, it is more resource intensive.
Stemming is (usually) a short procedure which uses string matching to remove parts of a string. This is much faster, doesn't need a lexicon, but the results aren't as accurate.
There is also a difference in output: Lemmatisation preserves the base class, so revolved is changed into revolve, and revolution remains unchanged (it is already the base form). In some stemming algorithms, derivational suffixes (-tion) are also removed, so all of the above might end up as revol. This might be what you want, as it returns something like a 'stem', or base morpheme.
"
Somebody please help me with this exercise/lesson on Kaggle for SQL,"In earlier tutorials in the course they show you how to list the tables for a given dataset. After the initial setup cell, you should have this cell to import bigquery and load the Chicago taxi trips dataset:
from google.cloud import bigquery

# Create a ""Client"" object
client = bigquery.Client()

# Construct a reference to the ""chicago_taxi_trips"" dataset
dataset_ref = client.dataset(""chicago_taxi_trips"", project=""bigquery-public-data"")

# API request - fetch the dataset
dataset = client.get_dataset(dataset_ref)

You can print all of the tables included in the dataset in a loop.
# Find the table name
tables = list(client.list_tables(dataset))
for table in tables:
    print(table.table_id)

In this case, there's only one table, so that's the value you put in the next cell to answer the exercise.
# Write the table name as a string below
table_name = _____  # replace the blank with the table name from above.

# Check your answer
q_1.check()

For future reference, there is a forum set aside on Kaggle for each of the Kaggle Learn Courses where you can ask questions about the tutorials and exercises. I often see feedback there from the course instructors themselves, as well as other Kaggle employees and community members.
"
Why am I getting error while importing a module in kaggle?,"As mentioned in the comment, you should try to install the module first.
Create a cell at the beginning of the notebook and type:
!pip install bs4

Then you should be able to import the module.
"
Kaggle BigQuery integration,"The error says it: 

Dataset IDs must be alphanumeric (plus underscores and dashes) and must be at most 1024 characters long.

So, if you are not  certain about your dataset id, then maybe try the alphanumeric underscore/dash separated options (like dc-taxi-trips or dc_taxi_trips). 
"
Colab running out RAM,"I wouldn't share a notebook with your kaggle ID and api key in it. Other people can access your kaggle account with that. 
Also, which runtime are you using? And what are your results from running the code in this thread? 
"
kaggle cli installation error,"I had the same problem although I used pipenv. To solve it I just followed the API credentials section of kaggle-api readme. Extract from the readme:

To use the Kaggle API, sign up for a Kaggle account at
  https://www.kaggle.com. Then go to the 'Account' tab of your user
  profile (https://www.kaggle.com//account) and select 'Create
  API Token'. This will trigger the download of kaggle.json, a file
  containing your API credentials. Place this file in the location
  ~/.kaggle/kaggle.json (on Windows in the location C:\Users\.kaggle\kaggle.json - you can check the exact location, sans
  drive, with echo %HOMEPATH%). You can define a shell environment
  variable KAGGLE_CONFIG_DIR to change this location to
  $KAGGLE_CONFIG_DIR/kaggle.json (on Windows it will be
  %KAGGLE_CONFIG_DIR%\kaggle.json

"
How to extract and format the string in a python data-frame,"Assuming the letter to extract is always the first character in the string, the following should help.
df['CabinCapital'] = df['Cabin'].str[0]

This also assumes your DataFrame is called df.
"
unexpected line continuation character,"Your code (and the code on the website you copied from) has backslashes followed by comments. E.g.
\ #is a female

The backslash is the ""line continuation character"". The error is telling you you shouldn't have a line continuation character followed by more text (in this case a comment).
Take out the backslashes.
"
Why use outlier data to train random forest regression for the use of filling other missing data,"The code works as the following:
remove_outlier = dataAgeNotNull[(np.abs(dataAgeNotNull[""Fare""]-dataAgeNotNull[""Fare""].mean())>(4*dataAgeNotNull[""Fare""].std()))|
                      (np.abs(dataAgeNotNull[""Family_Size""]-dataAgeNotNull[""Family_Size""].mean())>(4*dataAgeNotNull[""Family_Size""].std()))                     
                     ]

Here you define a dataframe based on the dataAgeNotNulldataframe, where the condition is that A or B is valid:
A. (np.abs(dataAgeNotNull[""Fare""]-dataAgeNotNull[""Fare""].mean())>(4*dataAgeNotNull[""Fare""].std())
B. (np.abs(dataAgeNotNull[""Family_Size""]-dataAgeNotNull[""Family_Size""].mean())>(4*dataAgeNotNull[""Family_Size""].std())

so you only keep the rows where A or B is True.
And then in:
rfModel_age.fit(remove_outlier[ageColumns], remove_outlier[""Age""])

ageNullValues = rfModel_age.predict(X= dataAgeNull[ageColumns]

You are predicting the Age based on the features in the ageColumns. The remove_outlier is only a new dataframe, where some outliers are excluded.
"
why don&#39;t the first cell run in the algorithm named Flow forecast,"Environment Restrictions: Some platforms or environments (like certain work or educational environments) might restrict access to external websites or version control systems. Check if there are any restrictions in the environment you're using.
Alternative Cloning Methods: Instead of using git clone within the code, you can download the repository as a ZIP file from GitHub directly and then manually upload and extract it into your environment.
Try Different Environment: If possible, try running the code in a different environment or platform to see if the issue persists.
But You Can Facing Same Problem in Kaggle You try to
using my shared google colab it can properly work
https://colab.research.google.com/drive/1tGZ404QjdvLYNm-8NvxUUGlWXIoQkLYL?usp=sharing
"
Is there a way to install a package in the Docker image file without rebuilding it again?,"A container is a running image.  You can add anything to a container but these changes are not persistent.  In this case you want to persist your correction so you have to rebuild the image; in other words you have to modify the image, not the container.  This is how Docker works.  Once you have a correct image you can create containers from it in any environment.
Note, however, that the Docker build cache will help you to speed up the build process: the lines of the Dockerfile that precede your correction will be executed quickly after the first build.
"
How to get data from the kaggle datasets site by entering the command on the Google Colab?,"You can use the Kaggle API. Refer here
kaggle competitions download favorita-grocery-sales-forecasting

or
kaggle competitions download favorita-grocery-sales-forecasting -f test.csv.7z

NOTE: you will need to accept competition rules at https://www.kaggle.com/c/<competition-name>/rules.
"
how to use custom dataset from kaggle to keras vision transformer,"If you are using google colab for your work, you can follow the steps below to use dataset:

Go to your kaggle profile.
Go to ""Account"" tab in your profile.
In the ""API"" section click on ""Create New Token"". It download ""kaggle.json"" file.
Use the following code for using the dataset. You can paste it in a blank cell:

!pip install -q kaggle
from google.colab import files
files.upload()
!mkdir ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets download {link_of_dataset}
!unzip {name_of_dataset}.zip -d {desired_directory}

When you run this cell, it asks you to upload a file. You need to upload the kaggle.json file that you downloaded in step 3.
For example if you want to download the ""ASL Alphabet"" dataset, use the following code:
!pip install -q kaggle
from google.colab import files
files.upload()
!mkdir ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets download 'grassknoted/asl-alphabet'
!unzip asl-alphabet.zip -d /content/

"
How to read/load images from a dataset from Kaggle directly into Python without downloading locally?,"When you request data from a webpage, the response is loaded into your local or virtual machine's memory. To do this, you need the URL of each image. Then you can run this code for each URL:
resp = requests.get(
    url
    stream=True,
)
for chunk in resp.raw:
    print(""Do something..."")

This is basically webscraping and doesn't suit your use case I imagine.
The Kaggle Python API allows you to download the entire dataset locally which is probably a better option.
"
How to recursively save GAN generated images to a folder in Kaggle,"It is just a simple task of saving it into kaggle output directory by replacing the output directory with ""/kaggle/working"" and then download afterwards into local directly.
Change from
vutils.save_image(img_list2[-1][i],
""/content/drive/MyDrive/DAugmentn/genPyImage
/gpi%d.jpg"" % i, normalize=True)
To
vutils.save_image(img_list2[-1][i],""/kaggle/working/gpi%d.jpg"" % i, normalize=True)

"
Multi class neural network for MNIST data set not working,"You should use a different bias for the second layer
self.z2 = self.w2.dot(self.a1) + self.b1 # not b1
self.z2 = self.w2.dot(self.a1) + self.b2 # but b2

When doing something like this
derB2 =(1/m) * np.sum(e2,axis=1)

you would like to use (keepdims = True) to make sure that derB2.shape is (something,1) but not (something, ). It makes your code more rigorous.
"
Kaggle competition: categorical variables,"You have different amount of features in training and test sets. So, there may be features present in the training that the model cannot find in the test, or features in the test for which the model was not trained.
A possible reason for this error is the one-hot-encoding when it's performed separately in each data set: there may be values for categorical variables that are only present in one of the tests.
One solution is to perform the OHE before splitting the data or, alternatively, you can use fit_transform with your training set and then only transform with your test set. Remember that you should always use transform when processing novel data, and this is a general rule for all scikit transformers.
Of course you should also make sure that all other trasformations, like droping columns were performed the same in both training and test sets. Pipelines are here your best friends.
"
Get dataset cover image from kaggle using python selenium web scraping,"This code uses selenium to get the source of the cover image element and then download it using requests:
import requests
from selenium import webdriver

driver= webdriver.Chrome()
driver.get(kaggle_website)

cover_src = driver.find_element_by_xpath('xpath/to/image').get_attribute(""src"")
response = requests.get(cover_src)

"
How do I use YOLOv5 without the internet for kaggle competition,"Yolov5 is a follow up version of yolo which is a neural network library in c language, also known as Darknet created by pjreddie.
It is an object detector model which can be trained to recognise objects in images or videos.
If you just want to detect some daily life object then you can just run inference on images/videos using python and trained weights and config file. You will find these files under the pretrained checkpoints section at the following link.
https://github.com/ultralytics/yolov5
"
Code and dataset(small size) for image clustering,"If you are looking for some tutorial with dataset and python code examples, here you will find some examples.

Keras & Sklearn for binary (cat or dog) clustering.

https://towardsdatascience.com/image-clustering-using-k-means-4a78478d2b83

Combining CNN and K-Means for multilabel clustering. (Data from Kaggle). At the end you can find all the code.

https://towardsdatascience.com/how-to-cluster-images-based-on-visual-similarity-cd6e7209fe34
"
I Tried to upload my csv file datasets but weka is not recognizing it,"Weka's CSVLoader only works with a small subset of CSV dialects. You can install and use common-csv-weka-package, which loads your CSV just fine.
Be aware, that you need Weka 3.9.5 or later for this package.
"
Distributions all numeric values in python,"It seems that you just need to reset the plot with plt.figure() in each iteration:
num_variable=train_data[['Age','SibSp','Parch','Fare']]
for i in num_variable.columns:
    plt.figure()
    plt.hist(num_variable[i])
    plt.title(i)
    plt.show

"
Kaggle exercise:Lists Question no. 5. Syntax error using &quot;print&quot;,"You forged bracket in line 6.
Instead:
final_index_of_list=arrivals.index((len(arrivals)-1)

should be:
final_index_of_list=arrivals.index((len(arrivals)-1))

UPD:
I find next some mistakes:

Incorrect function indentation. See more here
Instead else final_index_of_list%2>0: should be elif final_index_of_list%2>0:. You can read about python conditional statements here

So, correct function is:
def fashionably_late(arrivals, name):
    """"""Given an ordered list of arrivals to the party and a name, return whether the guest with that
    name was fashionably late.(This is a hint by the website.)
    """"""
    name_index=arrivals.index(name) #This line of code is working fine.
    final_index_of_list=arrivals.index((len(arrivals)-1))
    print(final_index_of_list)
    if final_index_of_list%2==0:
        return False if name_index<=(final_index_of_list/2) or name_index==final_index_of_list else True
    elif final_index_of_list%2>0:
        return False if name_index<=(final_index_of_list/2+1) or name_index==final_index_of_list else True

"
How to activate GPU in my kaggle notebook? It uses CPU 100% and GPU 0%. I need GPU accelerator for my deep learning project,"I was in a similar situation and which has been resolved. You can try doing the following since it worked for me.

Login to your kaggle account
2.Go to your profile
Find the Account tab
Now verify your phone number.
Refresh, and boom!!!! accelerator option is available

"
How to import zip image folder as data in a cnn model?,"Try unzipping it :
!unzip ""filePath""

"
Kernel Stuck in GeosSpatial Analysis Kaggle micro-course,"I am experiencing the very same problem with the very same outcome. From the traceback it seems clear that the error reported is not related to the student code itself but to the check() method utilized for code verification. I submitted a formal request for help from Kaggle through the 'More' option on the main menu: ""More > Support/Contact > I found an issue with the website"". I commit to share with you any reply I may get in the future. Please do the same if you succeed on your attempts. Thanks!
"
ggplot2 error: missing value where TRUE/FALSE needed. I have no idea,"If you want to add different colors for different hours you can do
library(dplyr)
library(ggplot2)

ggplot(filter(bike,workingday==1),aes(hour,count, color = hour)) + geom_point()


"
Could not find kaggle.json in colab?,"You need to create the directory. Also need to chmod.
!mkdir ~/.kaggle
!echo '{""username"":""korakot"",""key"":""xxxxxxxx""}' > ~/.kaggle/kaggle.json
!chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets download -d zillow/zecon -f State_time_series.csv

"
"How to output the number of restaurants in each category (such as: Italian, Japanese, Chinese)","If you want to count the values in the category column:
 restaurants.categories.value_counts()
 # or
 restaurants.groupby('categories').count()

You will get a table of the restaurant type and the number of times it is in the column.
"
How do I install basemap on Kaggle? Can someone provide the code?,"In the kernel:
1) Setting (at the right bottom of the kernel): 
Turn the internet ""ON""
2) In the kernel:
!pip install basemap-data-hires

"
Generate a validation set from a Kaggle&#39;s training set,"The simplest way to split the training data into train and validation set is to use train_test_split method from sklearn.
from sklearn.model_selection import train_test_split
train_set, val_set = train_test_split(train_data, test_size=0.2)

However, the split of data will happen at random and not ensure equal distribution of classes or some other feature. Let's assume that we want to split based on distribution of target label (eg - train_data['y']). Then we should use StratifiedShuffleSplit instead.
from sklearn.model_selection import StratifiedShuffleSplit
split = StratifiedShuffleSplit(n_splits=1, test_size=0.2)
for train_index, val_index in split.split(train_data, train_data['y']):
    strat_train_data = train_data.loc[train_index]
    strat_val_data = train_data.loc[val_index]

In this case, the training data will be split into train and val set, and the distribution of 'y' will be similar in both strat_train_data and strat_val_data. If we want the split to happen based on a particular feature (eg - gender, income group etc), replace the y-parameter of split() function accordingly.
"
Random Forest Regression not give 0 or 1,"Regression is a machine learning problem of predicting quantity/amount/price (such as market stock prediction, home price prediction, e.t.c). As far, as I remember, the goal of titanic competition is to predict whether a passenger survive. It's sounds like a binary classification problem. If it's a classification problem you should use RandomForestClassifier (docs). 
So your code would look like: 
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(
    #some parameters
)

model.fit(X_train, y_train)
y_pred = model.predict(X_test)


submit_df = pd.DataFrame({'PassengerId': passengerId, 'Survived': y_pred})
submit_df.to_csv('./csvToday/submission.csv', index=False)

This kernel can provide you with some more insights.
"
Solving Kaggle&#39;s Titanic Machine Learning,"Before you fit the model with features and target, the best practice is to check whether the null value is present in all the features which you want to use in building the model. You can know the below to check it



dataframe_name.isnull().any() this will give the column names and True if atleast one Nan value is present
dataframe_name.isnull().sum() this will give the column names and value of how many NaN values are present



By knowing the column names then you perform cleaning of data. 
This will not create the problem of NaN.
"
Kaggle airbus ship detection challenge.How to deal with class imbalance?,"One option that Keras provides is class_weight parameter in fit from documentation:

class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to ""pay more attention"" to samples from an under-represented class.

This will allow you to counter the imbalance to some extent.
"
download error Diabetic Retinopathy dataset from Kaggle,"Yeah ! I found its solution. Unzipping dataset using winRAR resulted aforementioned problem. Using 7zip for extracting is the solution. My friend found this solution. 
Thanks  
"
Trying to work out a python code,"creates an array called actualScore using the column Score from filtered_data
actualScore = filtered_data['Score']
creates array positiveNegative coding negative for values <3 and positive for >3
positiveNegative = actualScore.map(partition) 
overwrites old column score with new coded values
filtered_data['Score'] = positiveNegative
"
Text preprocessing in Python,"No package necessary in R (nor in Python if I'm not mistaken). First get everything split up (and remove that initial 5). I'm guessing you want the result as numbers, not strings:
x<-""5 0:10 8:1 18:2 54:1 442:2 3784:1 5640:1 43501:1""
y<-as.integer(unlist(strsplit(x,split="" |:""))[-1])
feature<-y[seq(1,length(y),by=2)]
[1]     0     8    18    54   442  3784  5640 43501
value<-y[seq(2,length(y),by=2)]
[1] 10  1  2  1  2  1  1  1

If you want them side-by-side:
cbind(feature,value)
     feature value
[1,]       0    10
[2,]       8     1
[3,]      18     2
[4,]      54     1
[5,]     442     2
[6,]    3784     1
[7,]    5640     1
[8,]   43501     1

If you want to assign them to a data.table for analysis:
library(data.table)
   dt<-data.table(feature=feature,value=value)
> dt
   feature value
1:       0    10
2:       8     1
3:      18     2
4:      54     1
5:     442     2
6:    3784     1
7:    5640     1
8:   43501     1

Etc.
"
How to load a .csv and Image dataset in kaggle?,"The error suggests that filename is missing the "".jpg"" extension. A simple fix is to add the extension manually if it's always the same format:
filename = self.df.loc[index, 'image_name'] + "".jpg""
"
What site can I find a dataset containing parameters related to the heart condition that I could use to train a machine learning model?,"Kaggle has some good datasets for heart condition prediction. Have a look here. 
These are some of the other ones I found: UCI, IEEE DataPort, US Data.GOV
"
"Is there any dataset available for similar posts recommendation like Facebook, Instagram and Twitter?","Hello and welcome to the forum! I recommend against building a dataset of your own as real world datasets have certain properties (distribution, missing data, noise) which are quite hard to replicate artificially.
You included Kaggle in your tags. Why don't you use it? It looks perfect for the job. Here's a list of options with the #recommender-systems tag.
This one and this one are about films. This other one is about user interactions with a news platform.
If this answered your question, please don't forget to accept it by clicking on the Tick button at the left of the answer. Have fun!
"
How to upload large image datasets from kaggle to google colab?,"You can use the Kaggle command-line client directly to download data sets in Colab.
Here's a step-by-step walk-through:
https://stackoverflow.com/a/50650918/8841057
"
Kaggle API Token Not Downloading,"Disable the pop-up blocker. Or try another browser
"
Why is this sapply not working on my data-frame? (titanic kaggle),"It doesn't work because sapply applies a function to all columns of a data frame, and you are trying to apply to rows. To implement what you are suggesting, you need apply(margin = 1).
But the main problem is that you don't need a loop for this, because most functions are vectorized in R (see chap. 3 of The R Inferno). The following code should work:
df.train$returnedage <- df.train$Age
df.train$returnedage[is.na(df.train$Age)] <- 24
df.train$returnedage[is.na(df.train$Age) & passenger$Pclasse==1] <- 37
df.train$returnedage[is.na(df.train$Age) & passenger$Pclasse==2] <- 29

"
Does Colab stops runtime if it reaches to RAM limit?,"The answer is yes, if you reach the RAM limit, your process will be killed.
check this link https://www.kaggle.com/code/yuliagm/how-to-work-with-big-datasets-on-16g-ram-dask
Another simplest way is to use Colab Pro+ (The Colab pro not work in this case (13 GB RAM limit).).
Also with this link
https://www.kaggle.com/discussions/general/74235
"
Why my neural network isn`t learning and why it prediction is equal on all test dataset?,"I see that you are using Cross Entropy Loss. If this is a classification task you should probably apply Softmax on the final output. Softmax will bound the values to be within the range of 0-1 by relatively scaling them.
"
How should I reduce the computing time in pandas on Kaggle?,"Pandas read_csv method has a chunksize argument yields a certain number of rows as an iterator. This is useful for very large data sets where you can train on a smaller subset of the data iteratively. 
More information on iterating through files is described in the documentation here.
"
find open source consumer dataset,"Hmm, I don't know what forum you heard about, but I may be able to help. :) I couldn't find exactly what you were looking for, but maybe one of these might work?

This dataset has information on online auctions, including: 


auctionid: unique identifier of an auction
bid: the proxy bid placed by a bidder
bidtime: the time in days that the bid was placed, from the start 
of the auction
bidderrate: eBay feedback rating of the bidder
openbid: the opening bid set by the seller
price: the closing price that the item sold for (equivalent to the 
second highest bid + an increment)

This dataset has store-wide sales data from 45 different department stores including information on the different sales they all ran.
This dataset has 12 million sales records of liquor in Iowa, but not a lot of customer information. 

"
"Error in eval(expr, envir, enclos) : object &#39;PAY_0.1&#39; not found (Boosting)","I guess PAY_0 is a binary/factor variable in your dataset. PAY_0.1 is dummy variable created from expansion of this variable which is PAY_0.0 and PAY_0.1 . More than likely PAY_0 have very few 1s so PAY_0.1 will have very few 1's. When doing training along with CV (repeatedcv) it is possible that one of the fold might not have got any 1's and hence this error. Or probably when you are building tree of depth 1 then no tree have breakage based on PAY_0.1 variable.
To correct the problem please do:

start your depth range a bit high. So instead of maxdepth=seq(1,10,1) if you use maxdepth=seq(4,10,1) you might not get this problem. Depth 1 tree is too shallow.
Check the distribution of PAY_0. If there are too less 1's then you can do upsampling of 1's or downsampling of 0's.

"
Change char name containing colon (&quot;:&quot;) with dplyr,"select only selects columns. In case the column name has a colon, you may have to use select_ and quote the column name: select_(""en:france"")
If you need cases, you have to use filter - but note, that comparison need ==, not a sinlge = (as in your select-call above): filter(France == ""en:france"")
If you need to rename columns, use colnames(dat)[which(colnames(dat) == ""en:france"")] <- ""France""
If you need to rename values, you can also use dat$x[which(dat$x == ""en:france"")] <- ""France"".
"
R data.table - Set Value in new column where value in other columns = 1,"Thanks to David Arenburg for answering.
train_raw[, Soil_Type := """" ]
indx <- which(names(train_raw) == ""Soil_Type"")
cols <- paste0(""Soil_Type"", 1:4)
for(j in 1:length(cols))
   set(train_raw,which(train_raw[[cols[j]]] == 1L), 
       j = indx, value = paste0(""S"", j))

"
InvalidArgumentError: `predictions` contains negative values,"I forgot to execute following cell in the code,
y_pred = tf.argmax(y_pred, axis=1)
y_true = tf.concat(list(test_spectrogram_ds.map(lambda s,lab: lab)), axis=0)

Before running the code:
confusion_mtx = tf.math.confusion_matrix(y_true, y_pred)
plt.figure(figsize=(10, 8))
sns.heatmap(confusion_mtx,
            xticklabels=label_names,
            yticklabels=label_names,
            annot=True, fmt='g')
plt.xlabel('Prediction')
plt.ylabel('Label')
plt.show()

So adding the cell solved the issue.
"
"Loop is slow when adding a @tf.function inside, why?","ok guys, i just forget to activate the GPU on Collab/Kaggle, however i still don't understand why the transition between the two function is slow but anyway, it's working well now !
"
Understanding the functioning of &#39;and&#39; and &#39;or&#39;,"To win a blackjack hand, you need to be closer to 21 than the dealer but not above 21. If you're above 21 (""bust""), you lose, no matter which hand the dealer has. If you're at 21 or below, you win if the dealer busts, or if the dealer does not bust but has fewer points than you do. If we denote the score of your hand as total_1 and the total of the dealer's hand as total_2, then this is logically equivalent to if (total_1 <= 21) and (total_1>total_2 or total_2 > 21): If you bust, the first condition will be False, and since we're in a conjunction, the overall expression will evaluate to False. If you're at 21 or below, you still haven't won for sure - you need to either have more than the dealer, or the delaer must bust. That's the disjunction that forms the second term in the conjunction: total_1>total_2 or total_2 > 21.
In other words, the issue is not one of coding or of logic, but of blackjack rules.
"
Can Someone help in visualizing if data is normally distributed or not,"You should provide data using dput() not a web link. As noted, your web link does not work. We can use data that comes with R to see if the sepal length in irises is normally distributed. We are combining all three species in this example:
data(iris)
x <- iris$Sepal.Length

I'll use the qqPlot() function in the car package:
library(car)
qqPlot(x)
shapiro.test(x)
# 
#   Shapiro-Wilk normality test
# 
# data:  x
# W = 0.97609, p-value = 0.01018

The plot shows the the observed values fall outside the expected range for small lengths. The Shapiro-Wilk test indicates that the values are significantly different from a normal distribution.

What happens if we use only one species:
shapiro.test(iris$Sepal.Length[iris$Species==""virginica""])
# 
#   Shapiro-Wilk normality test
# 
# data:  iris$Sepal.Length[iris$Species == ""virginica""]
# W = 0.97118, p-value = 0.2583

The lengths for species virginica are not significantly different from a normal distribution.
"
Which year saw the highest and lowest no of countries participating in olympics?,"As you only have 1 Row per subquery you can use CROSS JOIN
SELECT
    a1.games,a1.a,a1.b,b1.games,b1.B
FROM
(select games, count(DISTINCT r.region) AS a, count(DISTINCT r.region) AS b  from oly
JOIN regions r 
ON oly.noc = r.noc
group by games
order by a ASC LIMIT 1) a1
CROSS JOIN 
(select games, count(DISTINCT r.region) AS B  from oly
JOIN regions r 
ON oly.noc = r.noc
GROUP BY games
ORDER BY  B desc LIMIT 1 ) b1

"
Sample Submission on Boston Housing Dataset,"Try this snippet:
submission = pd.DataFrame({ 'ID': test_data.ID.values, 'medv': predictions })
submission.to_csv(""submission.csv"", index=False)

where predictions is a list of your predictions for the test set.
"
How to get predictions on X_test given the DNN?,"When you trained your model you asked tensorflow to evaluate your train_op. Your train_op is your optimizer, e.g.:
train_op = tf.train.AdamOptimizer(...).minimize(cost)

You ran something like this to train the model:
sess.run([train_op], feed_dict={x:data, y:labels})

The train_op depends on things like the gradients and the operations that update the weights, so all of these things happened when you ran the train_op.
At inference time you simply ask it to perform different calculations. You can have the optimizer defined, but if you don't ask it to run the optimizer it won't perform any of the actions that the optimizer is dependent on. You probably have an output of the network called logits (you could call it anything, but logits is the most common and seen in most tutorials). You might also have defined an op called accuracy which computes the accuracy of the batch. You can get the value of those with a similar request to tensorflow:
sess.run([logits, accuracy], feed_dict={x:data, y:labels})

Almost any tutorial will demonstrate this. My favorite tutorials are here: https://github.com/aymericdamien/TensorFlow-Examples
"
Why is my output dataframe shape not 1459 x 2 but 1460 x 2,"In the following line:
test = train.fillna(0)

you are assigning (overwriting) test variable with the ""train"" data ...
"
Python 3 OS windows 10 error handling lists,"The issue here stems from a different behaviour of the filter method. You can see that in this question, or read up on it in the python3 or python2 docs.
In short: In python 2 it generates a list, so ind_list[:][1] are all lists of integers.
However, in python 3 it generates a generator, which is why you get the output TypeError: int() argument must be a string, a bytes-like object or a number, not 'filter', since ind_list[:][1] all contain filter objects.
You can either convert all the outputs of the filter commands in the creation of ind_list to a list:
list(filter(lambda x: x not in range(0,l//10))

or use python2. But I am guessing there is a specific reason for you to use python3
Hint:
Since you are using anaconda, you can simply do
conda create -n py27 python=2.7 sklearn pandas numpy

followed by
activate py27

in an anaconda prompt and it will give you a virtual environment with python2 to use
"
Showing Matplotlib Animation on Kaggle Kernel,"To display the animated gif inside the notebook you may use
from IPython.display import Image, display
display(Image(url='pickup_animation.gif'))

You may also show the animation as html5 video
from IPython.display import HTML
HTML(anim.to_html5_video())

You may also show the matplotlib animation directly, using the notebook backend instead of the inline backend
%matplotlib notebook

"
Datasets &amp; Tutorials specifically targeting Business Data Analysis Issues,"The correct answer to this all depends on how comfortable you are currently with machine learning. Business data analysis and predictions are so closely aligned with machine learning that most developers consider it a subset that more general ML skills will cover. So I will suggest two things to you. If you have no experience in ML launch into the Data Science(python) career track of Data camp - It is excellent! This will help you get to grips with the overall ideas of cleaning your data and data processing, as well as supervised and unsupervised learning.
If you are already comfortable with all that I would suggest looking at pbpython.com - This site covers python for business analysis use entirely and suggests a plethora of books specialized for certain topics. As well as covering individual topics itself very well. 
"
python- [Errno 20] Not a directory,"It seems like the error is specifically Not a directory not not in the directory as your question asserts.
My assumption is your error is actually not in the code you shared. Your folder_list probably contains non-folder files. This makes listdir throw an error, as you're passing in a non-folder path.
Check your folder_list implementation.
"
How to evaluate Machine Learning Hackathon Submissions?,"You split you train data into train, val and test data.
You don't have to use this test data anywhere in the training. It will behave similarly to your actual test data. Run evaluations on this dataset.
"
What&#39;s the difference between &#39;==&#39; and &#39;in&#39; in if conditional statements?,"The first statement if keyword.lower() in normalized: is checking if keyword.lower() string is one of the elements inside the list normalized. This is True.
The other statement if keyword.lower() == normalized: is checking if keyword.lower() string has same value as normalized list. This is False.
"
